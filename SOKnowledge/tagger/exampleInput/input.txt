I want to use a track-bar to change a form 's opacity .
This is my code : @ code1 @ When I try to build it , I get this error : Can not implicitly convert type 'decimal ' to 'double ' .
I tried making trans a double , but then the control does n't work .
This code has worked fine for me in VB.NET in the past .I have an absolutely positioned div containing several children , one of which is a relatively positioned div .
When I use a percentage-based width on the child div , it collapses to 0 width on IE7 , but not on Firefox or Safari .
If I use pixel width , it works .
If the parent is relatively positioned , the percentage width on the child works .
Is there something I 'm missing here ?
Is there an easy fix for this besides the pixel-based width on the child ?
Is there an area of the CSS specification that covers this ?An explicit cast to double is n't necessary .
@ code1 @ Identifying the constant as 5000.0 ( or as 5000d ) is sufficient : @ code2 @Given a DateTime representing a person 's birthday , how do I calculate their age ?Given a specific DateTime value , how do I display relative time , like : 2 hours ago 3 days ago a month agoWell , here 's how we do it on Stack Overflow .
@ code1 @ Suggestions ?
Comments ?
Ways to improve this algorithm ?Is there a standard way for a Web Server to determine what time zone offset a user is in ?
Perhaps from a HTTP header ?
Or part of the user-agent string ?What is the difference between Math.Floor ( ) and Math.Truncate ( ) in .NET ?How do you expose a LINQ query as an ASMX web service ?
Usually , from the business tier , I can return a typed DataSet or DataTable which can be serialized for transport over ASMX .
How can I do the same for a LINQ query ?
Is there a way to populate a typed DataSet or DataTable via a LINQ query ?
: @ code1 @ How can I get the resultset of a LINQ query into a DataSet or DataTable ?
Alternatively , is the LINQ query serializeable so that I can expose it as an ASMX web service ?How do I store binary data in MySQL ?For a table like this : @ code1 @ Here is a PHP example : @ code2 @Solutions are welcome in any language .
: - ) I 'm looking for the fastest way to obtain the value of π , as a personal challenge .
More specifically I 'm using ways that do n't involve using # defined constants like M_PI , or hard-coding the number in .
The program below tests the various ways I know of .
The inline assembly version is , in theory , the fastest option , though clearly not portable ; I 've included it as a baseline to compare the other versions against .
In my tests , with built-ins , the 4 * atan ( 1 ) version is fastest on GCC 4.2 , because it auto-folds the atan ( 1 ) into a constant .
With -fno-builtin specified , the atan2 ( 0 , -1 ) version is fastest .
Here 's the main testing program ( pitimes.c ) : @ code1 @ And the inline assembly stuff ( fldpi.c ) , noting that it will only work for x86 and x64 systems : @ code2 @ And a build script that builds all the configurations I 'm testing ( build.sh ) : @ code3 @ Apart from testing between various compiler flags ( I 've compared 32-bit against 64-bit too , because the optimisations are different ) , I 've also tried switching the order of the tests around .
The atan2 ( 0 , -1 ) version still comes out top every time , though .Many years ago , to provide an age calculator gimmick on my website , I wrote a function to calculate age to a fraction .
This is a quick port of that function to C # ( from the PHP version ) .
I 'm afraid I have n't been able to test the C # version , but hope you enjoy all the same !
( Admittedly this is a bit gimmicky for the purposes of showing user profiles on Stack Overflow , but maybe readers will find some use for it .
: - ) ) double AgeDiff ( DateTime date1 , DateTime date2 ) { double years = date2.Year - date1.Year ; /* * If date2 and date1 + round ( date2 - date1 ) are on different sides * of 29 February , then our partial year is considered to have 366 * days total , otherwise it 's 365 .
Note that 59 is the day number * of 29 Feb. */ double fraction = 365 + ( DateTime.IsLeapYear ( date2.Year ) & & date2.DayOfYear > = 59 & & ( date1.DayOfYear < 59 || date1.DayOfYear > date2.DayOfYear ) ?
1 : 0 ) ; /* * The only really nontrivial case is if date1 is in a leap year , * and date2 is not .
So let 's handle the others first .
*/ if ( DateTime.IsLeapYear ( date2.Year ) == DateTime.IsLeapYear ( date1.Year ) ) return years + ( date2.DayOfYear - date1.DayOfYear ) / fraction ; /* * If date2 is in a leap year , but date1 is not and is March or * beyond , shift up by a day .
*/ if ( DateTime.IsLeapYear ( date2.Year ) ) { return years + ( date2.DayOfYear - date1.DayOfYear - ( date1.DayOfYear > = 59 ?
1 : 0 ) ) / fraction ; } /* * If date1 is not on 29 February , shift down date1 by a day if * March or later .
Proceed normally .
*/ if ( date1.DayOfYear ! = 59 ) { return years + ( date2.DayOfYear - date1.DayOfYear + ( date1.DayOfYear > 59 ?
1 : 0 ) ) / fraction ; } /* * Okay , here date1 is on 29 February , and date2 is not on a leap * year .
What to do now ?
On 28 Feb in date2 's year , the `` age '' * should be just shy of a whole number , and on 1 Mar should be * just over .
Perhaps the easiest way is to a point halfway * between those two : 58.5 .
*/ return years + ( date2.DayOfYear - 58.5 ) / fraction ; }The best way that I know of because of leap years and everything is : DateTime birthDate = new DateTime ( 2000,3,1 ) ; int age = ( int ) Math.Floor ( ( DateTime.Now - birthDate ) .TotalDays / 365.25D ) ; Hope this helps .If I have a trigger before the update on a table , how can I throw an error that prevents the update on that table ?I 've been having issues getting the C sockets API to work properly in C++ .
Specifically , although I am including sys/socket.h , I still get compile time errors telling me that AF_INET is not defined .
Am I missing something obvious , or could this be related to the fact that I 'm doing this coding on z/OS and my problems are much more complicated ?
Update : Upon further investigation , I discovered that there is an # ifdef that I 'm hitting .
Apparently z/OS is n't happy unless I define which `` type '' of sockets I 'm using with : @ code1 @ Now , I personally have no idea what this _OE_SOCKETS is actually for , so if any z/OS sockets programmers are out there ( all 3 of you ) , perhaps you could give me a rundown of how this all works ?The answer by phpguy is correct but I think there is a lot of confusing additional detail there .
The basic answer is in a BLOB column .
BLOB is short for Binary Large Object and that column type is specifically for handling binary data .
See the relevant manual page .@ jeff IMHO yours seems a little long .
However it does seem a little more robust with support for `` yesterday '' and `` years '' .
But in my experience when this is used the person is most likely to view the content in the first 30 days .
It is only the really hardcore people that come after that .
So that is why I usually elect to keep this short and simple .
This is the method I am currently using on one of my websites .
This only returns a relative day , hour , time .
And then the user has to slap on `` ago '' in the output .
public static string ToLongString ( this TimeSpan time ) { string output = String.Empty ; if ( time.Days > 0 ) output += time.Days + `` days `` ; if ( ( time.Days == 0 || time.Days == 1 ) & & time.Hours > 0 ) output += time.Hours + `` hr `` ; if ( time.Days == 0 & & time.Minutes > 0 ) output += time.Minutes + `` min `` ; if ( output.Length == 0 ) output += time.Seconds + `` sec '' ; return output.Trim ( ) ; }There are no HTTP headers that will report the clients timezone so far although it has been suggested to include it in the HTTP specification .
If it was me , I would probably try to fetch the timezone using clientside JavaScript and then submit it to the server using Ajax or something .I 've had no trouble using the BSD sockets API in C++ , in GNU/Linux .
Here 's the sample program I used : # include < sys/socket.h > intmain ( ) { return AF_INET ; } So my take on this is that z/OS is probably the complicating factor here , however , because I 've never used z/OS before , much less programmed in it , I ca n't say this definitively .
: -PDoes the parent div have a defined width either pixel or percentage ?
Not 100 % sure but I think in IE7 , the parent div needs a defined width for child percentage divs to work correctly .Math.Floor rounds down , Math.Ceiling rounds up , and Math.Truncate rounds towards zero .
Thus , Math.Truncate is like Math.Floor for positive numbers , and like Math.Ceiling for negative numbers .
Here 's the reference .
For completeness , Math.Round rounds to the nearest integer .
If the number is exactly midway between two integers , then it rounds towards the even one .
Reference .
See also : Pax Diablo 's answer .
Highly recommended !How do I forcefully unload a ByteArray from memory in ActionScript 3 ?
I have tried ( without success ) : @ code1 @ And @ code2 @How can I monitor an SQL Server database for changes to a table without using triggers or modifying the structure of the database in any way ?
My preferred programming environment is .NET and C # .
I 'd like to be able to support any SQL Server 2000 SP4 or newer .
My application is a bolt-on data visualization for another company 's product .
Our customer base is in the thousands , so I do n't want to have to put in requirements that we modify the third-party vendor 's table at every installation .
By `` changes to a table '' I mean changes to table data , not changes to table structure .
Ultimately , I would like the change to trigger an event in my application , instead of having to check for changes at an interval .
The best course of action given my requirements ( no triggers or schema modification , SQL Server 2000 and 2005 ) seems to be to use the BINARY_CHECKSUM function in T-SQL .
The way I plan to implement is this : Every X seconds run the following query : @ code1 @ and compare that against the stored value .
If the value has changed , go through the table row by row using the query @ code2 @ and compare the returned checksums against stored values .Sure I can post a test app .
# include < sys/socket.h > int main ( ) { return AF_INET ; } Compile/Link Output : cxx -Wc , xplink -Wl , xplink -o inet_test inet.C '' ./inet.C '' , line 5.16 : CCN5274 ( S ) The name lookup for `` AF_INET '' did not find a declaration .
CCN0797 ( I ) Compilation failed for file ./inet.C .
Object file not created .
A check of sys/sockets.h does include the definition I need , and as far as I can tell , it is not being blocked by any # ifdef statements .
I have however noticed it contains a the following : # ifdef __cplusplus extern `` C '' { # endif which encapsulates basically the whole file .
Not sure if it matters .I am aware that in .NET there are three timer types ( see Comparing the Timer Classes in the .NET Framework Class Library ) .
I have chosen a threaded timer as the other types can drift if the main thread is busy , and I need this to be reliable .
The way this timer works in the control of the timer is put on another thread so it can always tick along with the work begin completed on the parent thread when it is not busy .
The issue with this timer in a console application is that while the timer is ticking along on another thread the main thread is not doing anything so the application closes .
I tried adding a while true loop , but then the main thread is too busy when the timer does go off .I am starting a new web application in PHP and this time around I want to create something that people can extend by using a plugin interface .
How does one go about writing 'hooks ' into their code so that plugins can attach to specific events ?Consider using a ManualResetEvent to block the main thread at the end of its processing , and call Reset ( ) on it once the timer 's processing has finished .
If this is something that needs to run constantly , consider moving this into a service process instead of a console app .You can use something like Console.ReadLine ( ) to block the main thread , so other background threads ( like timer threads ) will still work .
You may also use an AutoResetEvent to block the execution , then ( when you need to ) you can call Set ( ) method on that AutoResetEvent object to release the main thread .
Also ensure that your reference to Timer object does n't go out of scope and garbage collected .Let 's say you create a Wizard in an HTML form .
One button goes back and one goes forward .
Since the back button appears first in the markup , when you press Enter it will use that button to submit the form .
Example : @ code1 @ What I would like to do , is get to decide which button is used to submit the form when a user presses Enter .
That way , when you press Enter the Wizard will move to the next page , not the previous .
Do you have to use tabindex to do this ?Here is one hack that may work .
It is n't clean , but it looks like it might work : Essentially , you just try to update a column that does n't exist .Why do n't you want to use triggers .
They are a good thing if you use them correctly .
If you use them as a way to enforce referential integrity that is when they go from good to bad .
But if you use them for monitoring , they are not really considered taboo .Have a look at this article http : //www.gskinner.com/blog/archives/2006/06/as3_resource_ma.html IANA actionscript programmer , however the feeling I 'm getting is that , because the garbage collector might not run when you want it to .
Hence http : //www.craftymind.com/2008/04/09/kick-starting-the-garbage-collector-in-actionscript-3-with-air/ So I 'd recommend trying out their collection code and see if it helps private var gcCount : int ; private function startGCCycle ( ) : void { gcCount = 0 ; addEventListener ( Event.ENTER_FRAME , doGC ) ; } private function doGC ( evt : Event ) : void { flash.system.System.gc ( ) ; if ( ++gcCount > 1 ) { removeEventListener ( Event.ENTER_FRAME , doGC ) ; setTimeout ( lastGC , 40 ) ; } } private function lastGC ( ) : void { flash.system.System.gc ( ) ; }( I 'm not positive about this , but ... ) AS3 uses a non-deterministic garbage collection .
Which means that unreferenced memory will be freed up whenever the runtime feels like it ( typically not unless there 's a reason to run , since it 's an expensive operation to execute ) .
This is the same approach used by most modern garbage collected languages ( like C # and Java as well ) .
Assuming there are no other references to the memory pointed to by byteArray or the items within the array itself , the memory will be freed at some point after you exit the scope where byteArray is declared .
You can force a garbage collection , though you really should n't .
If you do , do it only for testing ... if you do it in production , you 'll hurt performance much more than help it .
To force a GC , try ( yes , twice ) : @ code1 @ You can read more here .Give your submit buttons same name like this : @ code1 @ When the user presses enter and the Request goes to server , you can check the value for submitButton on your server-side code which contains a collection of form name/value pairs .
For example in classic ASP : @ code2 @ Reference : Using multiple submit buttons on a single formWould it be possible for you to change the previous button type into a button like this : @ code1 @ Now the Next button would be the default , plus you could also add the default attribute to it so that your browser will highlight it like so : @ code2 @ Hope that helps .Let 's say I have a DataTable with a Name column .
I want to have a collection of the unique names ordered alphabetically .
The following query ignores the order by clause .
@ code1 @ Why does the orderby not get enforced ?Unfortunately when it comes to memory management in Flash/actionscript there is n't a whole lot you can do .
ActionScript was designed to be easy to use ( so they did n't want people to have to worry about memory management ) The following is a workaround , instead of creating a ByteArray variable try this .
@ code1 @ Where byteArray is a dynamic property of byteObject , you can free the memory that was allocated for it .Where can I find a list of the MIME types and identifying characters for Office 2007 files ?
I have an upload form that is restricting uploads based on extension and identifying characters , but I can not seem to find the Office 2007 MIME types .The problem is that the Distinct operator does not grant that it will maintain the original order of values .
So your query will need to work like this @ code1 @Office 2007 MIME Types for IIS .docm , application/vnd.ms-word.document.macroEnabled.12 .docx , application/vnd.openxmlformats-officedocument.wordprocessingml.document .dotm , application/vnd.ms-word.template.macroEnabled.12 .dotx , application/vnd.openxmlformats-officedocument.wordprocessingml.template .potm , application/vnd.ms-powerpoint.template.macroEnabled.12 .potx , application/vnd.openxmlformats-officedocument.presentationml.template .ppam , application/vnd.ms-powerpoint.addin.macroEnabled.12 .ppsm , application/vnd.ms-powerpoint.slideshow.macroEnabled.12 .ppsx , application/vnd.openxmlformats-officedocument.presentationml.slideshow .pptm , application/vnd.ms-powerpoint.presentation.macroEnabled.12 .pptx , application/vnd.openxmlformats-officedocument.presentationml.presentation .xlam , application/vnd.ms-excel.addin.macroEnabled.12 .xlsb , application/vnd.ms-excel.sheet.binary.macroEnabled.12 .xlsm , application/vnd.ms-excel.sheet.macroEnabled.12 .xlsx , application/vnd.openxmlformats-officedocument.spreadsheetml.sheet .xltm , application/vnd.ms-excel.template.macroEnabled.12 .xltx , application/vnd.openxmlformats-officedocument.spreadsheetml.templateHow do you page through a collection in LINQ given that you have a startIndex and a count ?It is very simple with the Skip and Take extension methods .
@ code1 @Here 's a general description of a technique for calculating pi that I learnt in high school .
I only share this because I think it is simple enough that anyone can remember it , indefinitely , plus it teaches you the concept of `` Monte-Carlo '' methods -- which are statistical methods of arriving at answers that do n't immediately appear to be deducible through random processes .
Draw a square , and inscribe a quadrant ( one quarter of a semi-circle ) inside that square ( a quadrant with radius equal to the side of the square , so it fills as much of the square as possible ) Now throw a dart at the square , and record where it lands -- that is , choose a random point anywhere inside the square .
Of course , it landed inside the square , but is it inside the semi-circle ?
Record this fact .
Repeat this process many times -- and you will find there is a ratio of the number of points inside the semi-circle versus the total number thrown , call this ratio x .
Since the area of the square is r times r , you can deduce that the area of the semi circle is x times r times r ( that is , x times r squared ) .
Hence x times 4 will give you pi .
This is not a quick method to use .
But it 's a nice example of a Monte Carlo method .
And if you look around , you may find that many problems otherwise outside your computational skills can be solved by such methods .I 've got all these comments that I want to make into 'RDoc comments ' , so they can be formatted appropriately and viewed using ri .
Can anyone get me started on understanding how to use RDoc ?@ Jax : The extern `` C '' thing matters , very very much .
If a header file does n't have one , then ( unless it 's a C++-only header file ) , you would have to enclose your # include with it : extern `` C '' { # include < sys/socket.h > // include other similarly non-compliant header files } Basically , anytime where a C++ program wants to link to C-based facilities , the extern `` C '' is vital .
In practical terms , it means that the names used in external references will not be mangled , like normal C++ names would .
Reference .I believe the easiest way would be to follow Jeff 's own advice and have a look around existing code .
Try looking at Wordpress , Drupal , Joomla and other well known PHP-based CMS 's to see how their API hooks look and feel .
This way you can even get ideas you may have not thought of previously to make things a little more rubust .
A more direct answer would be to write general files that they would `` include_once '' into their file that would provide the usability they would need .
This would be broken up into categories and NOT provided in one MASSIVE `` hooks.php '' file .
Be careful though , because what ends up happening is that files that they include end up having more and more dependencies and functionality improves .
Try to keep API dependencies low .
I.E fewer files for them to include .You could use an Observer pattern .
A simple functional way to accomplish this : < ? php/** Plugin system **/ $ listeners = array ( ) ; /* Create an entry point for plugins */function hook ( ) { global $ listeners ; $ num_args = func_num_args ( ) ; $ args = func_get_args ( ) ; if ( $ num_args < 2 ) trigger_error ( `` Insufficient arguments '' , E_USER_ERROR ) ; // Hook name should always be first argument $ hook_name = array_shift ( $ args ) ; if ( ! isset ( $ listeners [ $ hook_name ] ) ) return ; // No plugins have registered this hook foreach ( $ listeners [ $ hook_name ] as $ func ) { $ args = $ func ( $ args ) ; } return $ args ; } /* Attach a function to a hook */function add_listener ( $ hook , $ function_name ) { global $ listeners ; $ listeners [ $ hook ] [ ] = $ function_name ; } //////////////////////////** Sample Plugin **/add_listener ( 'a_b ' , 'my_plugin_func1 ' ) ; add_listener ( 'str ' , 'my_plugin_func2 ' ) ; function my_plugin_func1 ( $ args ) { return array ( 4 , 5 ) ; } function my_plugin_func2 ( $ args ) { return str_replace ( 'sample ' , 'CRAZY ' , $ args [ 0 ] ) ; } //////////////////////////** Sample Application **/ $ a = 1 ; $ b = 2 ; list ( $ a , $ b ) = hook ( 'a_b ' , $ a , $ b ) ; $ str = `` This is my sample application\n '' ; $ str .= `` $ a + $ b = `` . ( $ a+ $ b ) .
`` \n '' ; $ str .= `` $ a * $ b = `` . ( $ a* $ b ) .
`` \n '' ; $ str = hook ( 'str ' , $ str ) ; echo $ str ; ? > Output : This is my CRAZY application4 + 5 = 94 * 5 = 20 Notes : For this example source code , you must declare all your plugins before the actual source code that you want to be extendable .
I 've included an example of how to handle single or multiple values being passed to the plugin .
The hardest part of this is writing the actual documentation which lists what arguments get passed to each hook .
This is just one method of accomplishing a plugin system in PHP .
There are better alternatives , I suggest you check out the WordPress Documentation for more information .
Sorry , it appears underscore characters are replaced by HTML entities by Markdown ?
I can re-post this code when this bug gets fixed .
Edit : Nevermind , it only appears that way when you are editingIt sounds like this.Opacity is a double value , and the compiler does n't like you trying to cram a decimal value into it .The version of Subclipse ( 1.2.4 ) currently available through Aptana 's automagic Plugins Manager does not work with the newest version of Subversion .
I see on the Subclipse website however that they have 1.4.2 out for Eclipse .
So I added a new remote update site to my Update manager .
When I tried to install it , it told me I needed Mylyn 3.0.0 .
So I after much searching I found Mylyn 3.0.0 and added another new remote update site to my update manager .
Then when I tried to install that , it told me I needed org.eclipse.ui 3.3.0 or equivalent .
Looking at the configuration details for Aptana , it looks like it is built against eclipse 3.2.2 .
Does anyone know if there is a way to upgrade the version of Eclipse Aptana is built against to 3.3.0 ?
Or if there is some other way to get Subclipse to work with the very newest version of Subversion ?
I know this is n't necessarily a `` programming '' question , but I hope it 's ok since it 's highly relevant to the programming experience ...I 've written a database generation script in SQL and want to execute it in my Adobe AIR application : @ code1 @ I execute this in Adobe AIR using the following methods : @ code2 @ No errors are generated , however only tRole exists .
It seems that it only looks at the first query ( up to the semicolon- if I remove it , the query fails ) .
Is there a way to call multiple queries in one statement ?I believe you have answered your own question ... System.totalMemory gives you the total amount of memory being `` used '' , not allocated .
It is accurate that your application may only be using 20mb , but it has 5mb that is free for future allocations .
I 'm not sure if the Adobe docs would shed light on the way that it manages memory ...Have a DTS job ( or a job that is started by a windows service ) that runs at a given interval .
Each time it is run , it gets information about the given table by using the system INFORMATION_SCHEMA tables , and records this data in the data repository .
Compare the data returned regarding the structure of the table with the data returned the previous time .
If it is different , then you know that the structure has changed .
Example query to return information regarding all of the columns in table ABC ( ideally listing out just the columns from the INFORMATION_SCHEMA table that you want , instead of using select * like I do here ) : @ code1 @ You would monitor different columns and INFORMATION_SCHEMA views depending on how exactly you define `` changes to a table '' .If you use a return type of IEnumerable , you can return your query variable directly .What are your best practices around creating flat file database structures in PHP ?
A lot of the more mature PHP flat file frameworks I see out there attempt to implement SQL-like query syntax , which is over the top for my purposes in most cases ( I would just use a database at that point ) .
Are there any elegant tricks out there to get good performance and features with the small code overhead one would want by taking on this problem in the first place ?A more generic answer for the generic question `` Decimal vs Double ?
`` : Decimal for monetary calculations to preserve the precision , Double for scientific calculations that do not get affected by small differences .
Since Double is a type which is native to the CPU ( internal representation is stored in base 2 ) , calculations made with Double perform better then Decimal ( which is represented in base 10 internally ) .One framework I 'm considering would be for a blogging platform .
Since just about any possible view of data you would want would be sorted by date , I was thinking about this structure : One directory per content node : ./content/YYYYMMDDHHMMSS/ Subdirectories of each node including @ code1 @ As well as simple text files in the node directory for pre- and post-rendered content and the like .
This would allow a simple PHP glob ( ) call ( and probably a reversal of the result array ) to query on just about anything within the content structure : @ code2 @ Would return paths including all articles tagged `` funny '' .So I find myself porting a game , that was originally written for the Win32 API , to Linux ( well , porting the OS X port of the Win32 port to Linux ) .
I have implemented QueryPerformanceCounter by giving the uSeconds since the process start up : @ code1 @ This , coupled with QueryPerformanceFrequency ( ) giving a constant 1000000 as the frequency , works well on my machine , giving me a 64 bit variable that contains uSeconds since the program 's start up .
So is this portable ?
I do n't want to discover it works differently if the kernel was compiled in a certain way or anything like that .
I am fine with it being non-portable to something other than Linux , however .I 've had problems with JavaHL in Eclipse Ganymede , when it worked fine in Eclipse Europa .
I 'm not sure how Aptana is different , but try either upgrading JavaHL or switching to the pure-java SVNKit implementation within the Subclipse config .Are there any really good tutorials explaining branching and merging with Apache Subversion ?
All the better if it 's specific to TortoiseSVN client .Version Control with Subversion A very good resource for source control in general .
Not really TortoiseSVN specific , though .From my experience , and from what I 've read across the internet , the answer is `` No , '' it is not guaranteed .
It depends on CPU speed , operating system , flavor of Linux , etc .One way to store flat-file content would be to save literal arrays to php files .
For example : @ code1 @Maybe .
But you have bigger problems .
gettimeofday ( ) can result in incorrect timings if there are processes on your system that change the timer ( ie , ntpd ) .
On a `` normal '' linux , though , I believe the resolution of gettimeofday ( ) is 10us .
It can jump forward and backward and time , consequently , based on the processes running on your system .
This effectively makes the answer to your question no .
You should look into clock_gettime ( CLOCK_MONOTONIC ) for timing intervals .
It suffers from several less issues due to things like multi-core systems and external clock settings .
Also , look into the clock_getres ( ) function .The actual resolution of gettimeofday ( ) depends on the hardware architecture .
Intel processors as well as SPARC machines offer high resolution timers that measure microseconds .
Other hardware architectures fall back to the system’s timer , which is typically set to 100 Hz .
In such cases , the time resolution will be less accurate .
I obtained this answer from High Resolution Time Measurement and Timers , Part IThat 's true .
serialize ( ) can be pretty useful for that as well .
I think the trick to coming up with a viable system is finding some way to index the data nodes without killing yourself with complexity .Javascript is the easiest way to get the client 's local time .
I would suggest using an XMLHttpRequest to send back the local time , and if that fails , fall back to the timezone detected based on their IP address .
As far as geolocation , I 've used MaxMind GeoIP on several projects and it works well , though I 'm not sure if they provide timezone data .
It 's a service you pay for and they provide monthly updates to your database .
They provide wrappers in several web languages .In .NET perspective : What is a Memory Leak ?
How can you determine whether your application leaks ?
What are the effects ?
How can you prevent a memory leak ?
If your application has memory leak , does it go away when the process exits or is killed ?
Or do memory leaks in your application affect other processes on the system even after process completion ?
And what about unmanaged code accessed via COM Interop and/or P/Invoke ?
I have some answers for these questions myself , but they are incomplete .
What do you think ?I guess in a managed environment , a leak would be you keeping an unnecessary reference to a large chunk of memory around .I 've been using TortoiseSVN in a Windows environment for quite some time .
It seems very feature-complete and nicely integrated into the Windows shell , and more importantly , it 's fairly painless to teach to colleagues with little or no experience with source control .
However , since we have moved to Windows Vista 64bit , Tortoise has been very buggy and has seemed to cause lots of explorer.exe abnormalities and crashes .
This has happened both with older versions of the software and the latest version ( 1.5.1 build 13563 ) .
I was curious if anyone has suggestions for other Subversion clients that will run on Windows ( specifically Vista 64bit ) .
Developers here use a variety of text editors so using Visual Studio or Dreamweaver for SVN is not ideal .
I have heard great things about Cornerstone , and would love something similar for Windows if it exists .
I 'm correlating the Vista/explorer problems with Tortoise because they normally occur when I 'm using the functionality in Tortoise .
Sometimes bringing up the `` merge '' screen will cause the GUI to start acting very strange and eventually hang or crash .
I did not see 1.5.2 -- I 'm installing now , maybe that will fix some of my issues .Recently our site has been deluged with the resurgence of the Asprox botnet SQL injection attack .
Without going into details , the attack attempts to execute SQL code by encoding the T-SQL commands in an ASCII encoded BINARY string .
It looks something like this : @ code1 @ I was able to decode this in SQL , but I was a little wary of doing this since I did n't know exactly what was happening at the time .
I tried to write a simple decode tool , so I could decode this type of text without even touching SQL Server .
The main part I need decoded is : @ code2 @ I 've tried all of the following commands with no luck : @ code3 @ What is the proper way to translate this encoding without using SQL Server ?
Is it possible ?
I 'll take VB.NET code since I 'm familiar with that too .
Okay , I 'm sure I 'm missing something here , so here 's where I 'm at .
Since my input is a basic string , I started with just a snippet of the encoded portion - 4445434C41 ( which translates to DECLA ) - and the first attempt was to do this ... @ code4 @ ... and all it did was return the exact same thing that I put in , since it converted each character into is byte .
I realized that I need to parse each two characters into a byte manually since I do n't know of any methods yet that will do that , so now my little decoder looks something like this : @ code5 @ Things look good for the first couple of pairs , but then the loop balks when it gets to the `` 4C '' pair and says that the string is in the incorrect format .
Interestingly enough , when I step through the debugger and to the GetString method on the byte array that I was able to parse up to that point , I get `` , -+ '' as the result .
How do I figure out what I 'm missing - do I need to do a `` direct cast '' for each byte instead of attempting to parse it ?I will concur with Bernard as to in .net what a mem leak would be .
You could profile your application to see its memory use , and determine that if its managing a lot of memory when it should not be you could say it has a leak .
In managed terms I will put my neck on the line to say it does go away once the process is killed/removed .
Unmanaged code is its own beast and if a leak exists within it , it will follow a standard mem .
leak definition .I have been using the 64Bit version of TortoiseSVN for ages and I have never had issues with it on Windows 64Bit or Vista 64Bit .
I am currently not aware of any other similiar SVN clients that do work on Vista .
Is it possible the problem could lie within the configuration of TortoiseSVN or even the installation of Vista ?
Is the problem occurring on Vista native or SP 1 ?The best explanation I 've seen is in Chapter 7 of the free Foundations of Programming ebook .
Basically , in .NET a memory leak occurs when referenced objects are rooted and thus can not be garbage collected .
This occurs accidentally when you hold on to references beyond the intended scope .
You 'll know that you have leaks when you start getting outofmemoryexceptions or your memory usage goes up beyond what you 'd expect ( perfmon has nice memory counters ) .
Understanding .NET 's memory model is your best way of avoiding it .
Specifically , understanding how the garbage collector works and how references work ( again , I refer you to chapter 7 of the ebook ) .
Also , be mindful of common pitfalls , probably the most common being events .
If object A registered to an event on object B , then object A will stick around until object B disappears because B holds a reference to A .
The solution is to unregister your events when you 're done .
Of course , a good memory profile will let you see your object graphs and explore the nesting/referencing of your objects to see where references are coming from and what root object is responsible ( red-gate ants profile , JetBrains dotMemory , memprofiler are really good choices , or you can use the text-only windbg and sos , but I 'd strongly recommend a commercial/visual product unless you 're a real guru ) .
I believe unmanaged code is subject to typical memory leaks of unamanged code , except that references shared between the two are managed by the garbage collector .
Could be wrong about this last point .I 'll second Diago 's answer .
I use TortoiseSVN on Vista x64 pretty heavily .
I did upgrade directly from an older version to 1.5.2 though , and never used 1.5.1 .
Have you tried 1.5.2 ?TortoiseSVN in combination with VisualSVN for Visual Studio .I too get explorer crashes in Vista ( I 'm not in the 64Bit version though ) .
I 'm using Vista Super Saijen ( or whatever they are calling the most expensive version ) .
I 'm not having any bugs with Tortoise .
My explorer does , however , crash about every other day ( sometimes multiple times a day if it 's having an `` off '' day ) .
I 'm not positive it 's being caused by TortoiseSVN though .
From what I hear , the explorer just crashes a lot in Vista ... Have you tried uninstalling Tortoise and using Windows for a day or two and seeing if it still crashes ?
Do you restart your computer at least once a day ( It seems the longer I go between restarts , the worse the crashes get ) ?Has anyone got experience creating SQL-based ASP.NET site-map providers ?
I 've got the default XML file web.sitemap working properly with my Menu and SiteMapPath controls , but I 'll need a way for the users of my site to create and modify pages dynamically .
I need to tie page viewing permissions into the standard ASP.NET membership system as well .Is there an existing application or library in Java which will allow me to convert a CSV data file to XML file ?
The XML tags would be provided through possibly the first row containing column headings .I wound up using this .
It is a kind of a hack , but it actually works pretty well .
The only thing is you have to be very careful with your semicolons .
: D @ code1 @What is the `` purist '' or `` correct '' way to access an object 's properties from within an object method that is not a getter/setter method ?
I know that from outside of the object you should use a getter/setter , but from within would you just do : Java : @ code1 @ PHP : @ code2 @ or would you do : Java : @ code3 @ PHP : @ code4 @ Forgive me if my Java is a little off , it 's been a year since I programmed in Java ... EDIT : It seems people are assuming I am talking about private or protected variables/properties only .
When I learned OO I was taught to use getters/setters for every single property even if it was public ( and actually I was told never to make any variable/property public ) .
So , I may be starting off from a false assumption from the get go .
It appears that people answering this question are maybe saying that you should have public properties and that those do n't need getters and setters , which goes against what I was taught , and what I was talking about , although maybe that needs to be discussed as well .
That 's probably a good topic for a different question though ...This has religious war potential , but it seems to me that if you 're using a getter/setter , you should use it internally as well - using both will lead to maintenance problems down the road ( e.g .
somebody adds code to a setter that needs to run every time that property is set , and the property is being set internally w/o that setter being called ) .It depends on how the property is used .
For example , say you have a student object that has a name property .
You could use your Get method to pull the name from the database , if it has n't been retrieved already .
This way you are reducing unnecessary calls to the database .
Now let 's say you have a private integer counter in your object that counts the number of times the name has been called .
You may want to not use the Get method from inside the object because it would produce an invalid count .I 've been banging my head against SQL Server 2005 trying to get a lot of data out .
I 've been given a database with nearly 300 tables in it and I need to turn this into a MySQL database .
My first call was to use bcp but unfortunately it does n't produce valid CSV - strings are n't encapsulated , so you ca n't deal with any row that has a string with a comma in it ( or whatever you use as a delimiter ) and I would still have to hand write all of the create table statements , as obviously CSV does n't tell you anything about the data types .
What would be better is if there was some tool that could connect to both SQL Server and MySQL , then do a copy .
You lose views , stored procedures , trigger , etc , but it is n't hard to copy a table that only uses base types from one DB to another ... is it ?
Does anybody know of such a tool ?
I do n't mind how many assumptions it makes or what simplifications occur , as long as it supports integer , float , datetime and string .
I have to do a lot of pruning , normalising , etc .
anyway so I do n't care about keeping keys , relationships or anything like that , but I need the initial set of data in fast !This may be too basic or limited of a solution , but could n't you do a String.split ( ) on each line of the file , remembering the result array of the first line to generate the XML , and just spit each line 's array data out with the proper XML elements padding each iteration of a loop ?I have a pretty standard table set-up in a current application using the .NET XSD DataSet and TableAdapter features .
My contracts table consists of some standard contract information , with a column for the primary department .
This column is a foreign key to my Departments table , where I store the basic department name , id , notes .
This is all setup and functioning in my SQL Server .
When I use the XSD tool , I can drag both tables in at once and it auto detects/creates the foreign key I have between these two tables .
This works great when I 'm on my main page and am viewing contract data .
However , when I go to my administrative page to modify the department data .
I typically do something like this : @ code1 @ However , at this point an exception is thrown saying to the effect that there is a foreign key reference broken here , I 'm guessing since I do n't have the Contract DataTable filled .
How can I fix this problem ?
I know I can simply remove the foreign key from the XSD to make things work fine , but having the additional integrity check there and having the XSD schema match the SQL schema in the database is nice .Rolling your own PHP solution will certainly work though I 'm not sure if there is a good way to automatically duplicate the schema from one DB to the other ( maybe this was your question ) .
If you are just copying data , and/or you need custom code anyway to convert between modified schemas between the two DB 's , I would recommend using PHP 5.2+ and the PDO libraries .
You 'll be able to connect using PDO ODBC ( and use MSSQL drivers ) .
I had a lot of problems getting large text fields and multi-byte characters from MSSQL into PHP using other libraries .Am I just going overboard here ?
Perhaps ; ) Another approach would be to utilize a private/protected method to actually do the getting ( caching/db/etc ) , and a public wrapper for it that increments the count : PHP : public function getName ( ) { $ this- > incrementNameCalled ( ) ; return $ this- > _getName ( ) ; } protected function _getName ( ) { return $ this- > name ; } and then from within the object itself : PHP : $ name = $ this- > _getName ( ) ; This way you can still use that first argument for something else ( like sending a flag for whether or not to used cached data here perhaps ) .Another tool to try would be the SQLMaestro suite - http : //www.sqlmaestro.com It is a little tricky nailing down the precise tool , but they have a variety of tools , both free and for purchase that handle a wide variety of tasks for multiple database platforms .
I 'd suggest trying the Data Wizard tool first for MySQL , since I believe that will have the proper `` import '' tool you need .Well , it seems with C # 3.0 properties ' default implementation , the decision is taken for you ; you HAVE to set the property using the ( possibly private ) property setter .
I personally only use the private member-behind when not doing so would cause the object to fall in an less than desirable state , such as when initializing or when caching/lazy loading is involved .As stated in some of the comments : Sometimes you should , sometimes you should n't .
The great part about private variables is that you are able to see all the places they are used when you change something .
If your getter/setter does something you need , use it .
If it does n't matter you decide .
The opposite case could be made that if you use the getter/setter and somebody changes the getter/setter they have to analyze all the places the getter and setter is used internally to see if it messes something up .Does anyone know of a good way to compress or decompress files and folders in C # quickly ?
Handling large files might be necessary .I have a website that plays mp3s in a flash player .
If a user clicks 'play ' the flash player automatically downloads an mp3 and starts playing it .
Is there an easy way to track how many times a particular song clip ( or any binary file ) has been downloaded ?
Is the play link a link to the actual mp3 file or to some javascript code that pops up a player ?
If the latter , you can easily add your own logging code in there to track the number of hits to it .
If the former , you 'll need something that can track the web server log itself and make that distinction .
My hosting plan comes with webalizer , which does this nicely .
It 's javascript code , so that answers that .
However , it would be nice to know how to track downloads using the other method ( without switching hosts ) .The hook and listener method is the most commonly used , but there are other things you can do .
Depending on the size of your app , and who your going to allow see the code ( is this going to be a FOSS script , or something in house ) will influence greatly how you want to allow plugins .
kdeloach has a nice example , but his implementation and hook function is a little unsafe .
I would ask for you to give more information of the nature of php app your writing , And how you see plugins fitting in .
+1 to kdeloach from me .Is the play link a link to the actual mp3 file or to some javascript code that pops up a player ?
If the latter , you can easily add your own logging code in there to track the number of hits to it .
If the former , you 'll need something that can track the web server log itself and make that distinction .
My hosting plan comes with webalizer , which does this nicely .As of .Net 1.1 the only available method is reaching into the java libraries .
Using the Zip Classes in the J # Class Libraries to Compress Files and Data with C # Not sure if this has changed in recent versions .I 've always used the SharpZip Library .
Here 's a linkThe funny thing is i wrote a php media gallery for all my music 2 days ago .
I had a similar problem .
Im using http : //musicplayer.sourceforge.net/ for the player .
and the playlis are built via php .
all music request go there a script called xfer.php ? file=WHATEVER @ code1 @ And when you call files use something like @ code2 @ http : //us.php.net/manual/en/function.base64-encode.php If you are using some javascript or a flash player ( JW player for example ) that requires the actual link to be an mp3 file or whatever , you can append the text `` & type=.mp3 '' so the final linke becomes something like `` www.example.com/xfer.php ? file=34842ffjfjxfh & type=.mp3 '' .
That way it looks like it ends with an mp3 extension without affecting the file link .You could even set up an Apache .htaccess directive that converts *.mp3 requests into the querystring dubayou is working with .
It might be an elegant way to keep the direct request and still be able to slipstream log function into the response .Well , what is the nature of the flat databases .
Are they large or small .
Is it simple arrays with arrays in them ?
if its something simple say userprofiles built as such : @ code1 @ and to save or update the db record for that user .
@ code2 @ and to load the record for the user @ code3 @ but again this implementation will vary on the application and nature of the database you need .I used to have lots of Explorer crashes ( on 32-bit ) caused by Tortoise .
They seem to have gone away since I used the Include/Exclude path settings in the `` Icon Overlays '' configuration of TSVN .
Constraining icon overlays to specific directories where I keep my source made this much more stable .Use your httpd log files .
Install http : //awstats.sourceforge.net/Strictly speaking , a memory leak is consuming memory that is `` no longer used '' by the program .
`` No longer used '' has more than one meaning , it could mean `` no more reference to it '' , that is , totally unrecoverable , or it could mean , referenced , recoverable , unused but the program keeps the references anyway .
Only the later applies to .Net for perfectly managed objects .
However , not all classes are perfect and at some point an underlying unmanaged implementation could leak resources permanently for that process .
In all cases , the application consumes more memory than strictly needed .
The sides effects , depending on the ammount leaked , could go from none , to slowdown caused by excessive collection , to a series of memory exceptions and finally a fatal error followed by forced process termination .
You know an application has a memory problem when monitoring shows that more and more memory is allocated to your process after each garbage collection cycle .
In such case , you are either keeping too much in memory , or some underlying unmanaged implementation is leaking .
For most leaks , resources are recovered when the process is terminated , however some resources are not always recovered in some precise cases , GDI cursor handles are notorious for that .
Of course , if you have an interprocess communication mechanism , memory allocated in the other process would not be freed until that process frees it or terminates .The best way that I have found is the MySQL Migration Toolkit provided by MySQL .
I have used it successfully for some large migration projects .I would define memory leaks as an object not freeing up all the memory allocated after it has completed .
I have found this can happen in your application if you are using Windows API and COM ( i.e .
unmanaged code that has a bug in it or is not being managed correctly ) , in the framework and in third party components .
I have also found not tiding up after using certain objects like pens can cause the issue .
I personally have suffered Out of Memory Exceptions which can be caused but are not exclusive to memory leaks in dot net applications .
( OOM can also come from pinning see Pinning Artical ) .
If you are not getting OOM errors or need to confirm if it is a memory leak causing it then the only way is to profile your application .
I would also try and ensure the following : a ) Everything that implements Idisposable is disposed either using a finally block or the using statement these include brushes , pens etc .
( some people argue to set everything to nothing in addition ) b ) Anything that has a close method is closed again using finally or the using statement ( although I have found using does not always close depending if you declared the object outside the using statement ) c ) If you are using unmanaged code/windows API 's that these are dealt with correctly after .
( some have clean up methods to release resources ) Hope this helps .Stack Overflow has a subversion version number at the bottom : svn revision : 679 I want to use such automatic versioning with my .NET Web Site/Application , Windows Forms , WPD projects/solutions .
How do I implement this ?We are using WMV videos on an internal site , and we are embedding them into web sites .
Now , this works quite well on Internet Explorer , but not on Firefox .
I 've found ways to make it work in Firefox , but then it stops working in Internet Explorer .
We do not want to use Silverlight just yet , especially since we can not be sure that all clients will be running Windows XP with Windows Media Player installed .
Is there some sort of Universal Code that embeds WMP into both Internet Explorer and Firefox , or do we need to implement some user-agent-detection and deliver different HTML for different browsers ?You can use a 3rd-party library such as SharpZip as Tom pointed out .
Another way ( without going 3rd-party ) is to use the Windows Shell API .
You 'll need to set a reference to the Microsoft Shell Controls and Automation COM library in your C # project .
Gerald Gibson has an example at : http : //geraldgibson.net/dnn/Home/CZipFileCompression/tabid/148/Default.aspxYou can do it by adding the following anywhere in your code $ Id : $ So for example @ Jeff did : @ code1 @ and when checked in the server replaced $ Id : $ with the current revision number .
I also found this reference .
There is also $ Date : $ , $ Rev : $ , $ Revision : $You could use conditional comments to get IE and Firefox to do different things < !
[ if ! IE ] > < p > Firefox only code < /p > < !
[ endif ] > < ! -- [ if IE ] > < p > Internet Explorer only code < /p > < !
[ endif ] -- > The browsers themselves will ignore code that is n't meant for them to read .Personally , I feel like it 's important to remain consistent .
If you have getters and setters , use them .
The only time I would access a field directly is when the accessor has a lot of overhead .
It may feel like you 're bloating your code unnecessarily , but it can certainly save a whole lot of headache in the future .
The classic example : Later on , you may desire to change the way that field works .
Maybe it should be calculated on-the-fly or maybe you would like to use a different type for the backing store .
If you are accessing properties directly , a change like that can break an awful lot of code in one swell foop .Looks like Jeff is using CruiseControl.NET based on some leafing through the podcast transcripts .
This seems to have automated deployment capabilities from source control to production .
Might this be where the insertion is happening ?I 've been wanting to get my databases under version control .
Does anyone have any advice or recommended articles to get me started ?
I 'll always want to have at least some data in there ( as alumb mentions : user types and administrators ) .
I 'll also often want a large collection of generated test data for performance measurement .I want to print HTML from a C # web service .
The Web Browser control is overkill , and does not function well in a service environment , nor does it function well on a system with very tight security constraints .
Is there any sort of free .NET library that will support the printing of a basic HTML page ?
Here is the code I have so far , which does not run properly .
@ code1 @ This works fine when called from UI-type threads , but nothing happens when called from a service-type thread .
Changing Print ( ) to ShowPrintPreviewDialog ( ) yields the following IE script error : Error : 'dialogArguments.___IE_PrintType ' is null or not an object URL : res : //ieframe.dll/preview.dlg And a small empty print preview dialog appears .I want to be able to display a normal You Tube video with overlaid annotations , consisting of coloured rectangles for each frame .
The only requirement is that this be done programatically .
YouTube has annotations now , but require you to use their front end to create them by hand .
I want to be able to generate them .
What 's the best way of doing this ?
Some ideas : Build your own Flash player ( ew ? )
Somehow draw over the YouTube Flash player .
Will this work ?
Reverse engineer & hijack YouTube 's annotation system .
Either messing with the local files or redirecting its attempt to download the annotations .
( using Greasemonkey ?
Firefox plugin ? )
Idea that does n't count : download the videoOn one Linux Server running Apache and PHP 5 , we got multiple Virtual Hosts with separate logfiles and everything .
The only thing we can not seem to separate between virtual hosts is the php error_log .
Overriding this setting in the & lt ; Location & gt ; of the httpd.conf does not seem to do anything .
Did I overlook something ?
Is there a way to have separate php error_logs for each Virtual Host ?This is something I 've pseudo-solved many times and never quite found a solution .
That 's stuck with me .
The problem is to come up with a way to generate N colors , that are as distinguishable as possible where N is a parameter .Maybe this might help : JSefa You can read CSV file with this tool and serialize it to XML .I do n't understand why you would want to do this .
It sounds almost like cargo cult coding .
Converting a CSV file to XML does n't add any value .
Your program is already reading the CSV file , so arguing that you need XML does n't work .
On the other hand , reading the CSV file , doing something with the values , and then serializing to XML does make sense ( well , as much as using XML can make sense ... ; ) ) but you would supposedly already have a means of serializing to XML .One of the fun parts of multi-cultural programming is number formats .
Americans use 10,000.50 Germans use 10.000,50 French use 10 000,50 My first approach would be to take the string , parse it backwards , until I encounter a separator and use this as my decimal separator .
There is an obvious flaw with that : 10.000 would be interpreted as 10 .
Another approach : if the string contains 2 different non-numeric characters , use the last one as the decimal separator and discard the others .
If I only have one , check if it occurs more than once and discard it if it does .
If it only appears once , check if it has 3 digits after it .
If yes , discard it , otherwise use it as decimal separator .
The obvious `` best solution '' would be to detect the User 's culture or Browser , but that does not work if you have a Frenchman using an en-US Windows/Browser .
Does the .net Framework contain some mythical black magic floating point parser that is better than Double .
( Try ) Parse ( ) in trying to auto-detect the number format ?Yes , I know .
The existence of a running copy of SQL Server 6.5 in 2008 is absurd .
That stipulated , what is the best way to migrate from 6.5 to 2005 ?
Is there any direct path ?
Most of the documentation I 've found deals with upgrading 6.5 to 7 .
Should I forget about the native SQL Server upgrade utilities , script out all of the objects and data , and try to recreate from scratch ?
I was going to attempt the upgrade this weekend , but server issues pushed it back till next .
So , any ideas would be welcomed during the course of the week .
Update .
This is how I ended up doing it : Back up the database in question and Master on 6.5 .
Execute SQL Server 2000 's instcat.sql against 6.5 's Master .
This allows SQL Server 2000 's OLEDB provider to connect to 6.5 .
Use SQL Server 2000 's standalone `` Import and Export Data '' to create a DTS package , using OLEDB to connect to 6.5 .
This successfully copied all 6.5 's tables to a new 2005 database ( also using OLEDB ) .
Use 6.5 's Enterprise Manager to script out all of the database 's indexes and triggers to a .sql file .
Execute that .sql file against the new copy of the database , in 2005 's Management Studio .
Use 6.5 's Enterprise Manager to script out all of the stored procedures .
Execute that .sql file against the 2005 database .
Several dozen sprocs had issues making them incompatible with 2005 .
Mainly non-ANSI joins and quoted identifier issues .
Corrected all of those issues and re-executed the .sql file .
Recreated the 6.5 's logins in 2005 and gave them appropriate permissions .
There was a bit of rinse/repeat when correcting the stored procedures ( there were hundreds of them to correct ) , but the upgrade went great otherwise .
Being able to use Management Studio instead of Query Analyzer and Enterprise Manager 6.5 is such an amazing difference .
A few report queries that took 20-30 seconds on the 6.5 database are now running in 1-2 seconds , without any modification , new indexes , or anything .
I did n't expect that kind of immediate improvement .This is one of the `` hard problems '' surrounding development .
As far as I know there are no perfect solutions .
If you only need to store the database structure and not the data you can export the database as SQL queries .
( in Enterprise Manager : Right click on database - > Generate SQL script .
I recommend setting the `` create one file per object '' on the options tab ) You can then commit these text files to svn and make use of svn 's diff and logging functions .
I have this tied together with a Batch script that takes a couple parameters and sets up the database .
I also added some additional queries that enter default data like user types and the admin user .
( If you want more info on this , post something and I can put the script somewhere accessible ) If you need to keep all of the data as well , I recommend keeping a back up of the database and using Redgate ( http : //www.red-gate.com/ ) products to do the comparisons .
They do n't come cheap , but they are worth every penny .Hey , I 'm still stuck in that camp too .
The third party application we have to support is FINALLY going to 2K5 , so we 're almost out of the wood .
But I feel your pain 8^D That said , from everything I heard from our DBA , the key is to convert the database to 8.0 format first , and then go to 2005 .
I believe they used the built in migration/upgrade tools for this .
There are some big steps between 6.5 and 8.0 that are better solved there than going from 6.5 to 2005 directly .
Your BIGGEST pain , if you did n't know already , is that DTS is gone in favor of SSIS .
There is a shell type module that will run your existing DTS packages , but you 're going to want to manually recreate them all in SSIS .
Ease of this will depend on the complexity of the packages themselves , but I 've done a few at work so far and they 've been pretty smooth .I 've read somewhere the human eye ca n't distinguish between less than 4 values apart .
so This is something to keep in mind .
The following algorithm does not compensate for this .
I 'm not sure this is exactly what you want , but this is one way to randomly generate non-repeating color values : ( beware , inconsistent pseudo-code ahead ) //colors entered as 0-255 [ R , G , B ] colors = [ ] ; //holds final colors to be usedrand = new Random ( ) ; //assumes n is less than 16,777,216randomGen ( int n ) { while ( len ( colors ) < n ) { //generate a random number between 0,255 for each color newRed = rand.next ( 256 ) ; newGreen = rand.next ( 256 ) ; newBlue = rand.next ( 256 ) ; temp = [ newRed , newGreen , newBlue ] ; //only adds new colors to the array if temp not in colors { colors.append ( temp ) ; } } } One way you could optimize this for better visibility would be to compare the distance between each new color and all the colors in the array : for item in color { itemSq = ( item [ 0 ] ^2 + item [ 1 ] ^2 + item [ 2 ] ^2 ] ) ^ ( .5 ) ; tempSq = ( temp [ 0 ] ^2 + temp [ 1 ] ^2 + temp [ 2 ] ^2 ] ) ^ ( .5 ) ; dist = itemSq - tempSq ; dist = abs ( dist ) ; } //NUMBER can be your chosen distance apart.if dist < NUMBER and temp not in colors { colors.append ( temp ) ; } But this approach would significantly slow down your algorithm .
Another way would be to scrap the randomness and systematically go through every 4 values and add a color to an array in the above example .You can try turning Check-constraints off on the DataSet ( it 's in its properties ) , or altering the properties of that relationship , and change the key to a simple reference - up to you .To set the Apache ( not the PHP ) log , the easiest way to do this would be to do : @ code1 @ If there is no leading `` / '' it is assumed to be relative .
Apache Error Log PageI do n't know the ASP.NET side of the problem but .NET has a pretty powerful class : System.Globalization.CultureInfo .
You can use the following code to parse a string containing a double value : double d = double.Parse ( `` 100.20 '' , CultureInfo.CurrentCulture ) ; // -- OR -- double d = double.Parse ( `` 100.20 '' , CultureInfo.CurrentUICulture ) ; If ASP.NET somehow ( i.e .
using HTTP Request headers ) passes current user 's CultureInfo to either CultureInfo.CurrentCulture or CultureInfo.CurrentUICulture , these will work fine .You ca n't please everyone .
If I enter ten as 10.000 , and someone enters ten thousand as 10.000 , you can not handle that without some knowledge of the culture of the input .
Detect the culture somehow ( browser , system setting - what is the use case ?
ASP ?
Internal app , or open to the world ?
) , or provide an example of the expected formatting , and use the most lenient parser you can .
Probably something like : double d = Double.Parse ( `` 5,000.00 '' , NumberStyles.Any , CultureInfo.InvariantCulture ) ;I have the following arrays : @ code1 @ I want to generate a tag cloud that will have artists with a higher number in $ count enclosed in h6 tags and the lowest enclosed h1 tags .Another function , not my me but found on the web and a bit refined : @ code1 @ Just two things that come into my mind : What about people from countries that do not use the gregorian calendar ?
DateTime.Now is in the server-specific culture i think .
I have absolutely 0 knowledge about actually working with Asian calendars and I do not know if there is an easy way to convert dates between calendars , but just in case you 're wondering about those chinese guys from the year 4660 : - )How do I make it so mailto : links will be registered with my program ?
How would I then handle that event in my program ?
Most of the solutions I found from a quick Google search are how to do this manually , but I need to do this automatically for users of my program if they click a button , such as `` set as default email client '' .
Edit : Removed reference to Delphi , because the answer is independent of your language .From what I 've seen , there are a few registry keys that set the default mail client .
One of them being : System Key : [ HKEY_CLASSES_ROOT\mailto\shell\open\command ] Value Name : ( Default ) Data Type : REG_SZ ( String Value ) Value Data : Mail program command-line .
I 'm not familar with Delphi 7 , but I 'm sure there are some registry editing libraries in there that you could use to modify this value .
Some places list more than this key , others just this key , so you may need to test a little bit to find the proper one ( s ) .MySQL has this incredibly useful yet properitary REPLACE INTO SQL Command .
I wonder : Can this easily be emulated in SQL Server 2005 ?
Starting a new Transaction , doing a Select ( ) and then either UPDATE or INSERT and COMMIT is always a little bit of a pain , especially when doing it in the application and therefore always keeping 2 versions of the statement .
I wonder if there is an easy and universal way to implement such a function into SQL Server 2005 ?Here is the official Microsoft Solution to programmatically change the default mail client .
It 's Visual Basic , but I hope the concept is clear from this : http : //support.microsoft.com/kb/299853The default behaviour is for error_log ( ) to output to the Apache error log .
If this is n't happening check your php.ini settings for the error_log directive - leave it unset to use the Apache log file for the current vhost .The functionality you 're looking for is traditionally called an UPSERT .
Atleast knowing what it 's called might help you find what you 're looking for .
I do n't think SQL Server 2005 has any great ways of doing this .
2008 introduces the MERGE statement that can be used to accomplish this as shown in : http : //www.databasejournal.com/features/mssql/article.php/3739131 or http : //blogs.conchango.com/davidportas/archive/2007/11/14/SQL-Server-2008-MERGE.aspx Merge was available in the beta of 2005 , but they removed it out in the final release .What the upsert/merge is doing is something to the effect of ... @ code1 @ So hopefully the combination of those articles and this pseudo code can get things moving .Does anybody know if there is a way to create a SQLite database based on an XSD DataSet ?
In the past I 've just used a basic SQLite manager , but want to fuse things a bit more with my .NET development if possible .Off the top of my head ... $ artist = array ( `` the roots '' , '' michael jackson '' , '' billy idol '' , '' more '' , '' and more '' , '' and_YET_MORE '' ) ; $ count = array ( 5,3,9,1,1,3 ) ; $ highest = max ( $ count ) ; for ( int $ x = 0 ; x < count ( $ artist ) ; $ x++ ) { $ normalized = $ count [ $ x ] / $ highest ; $ heading = ceil ( $ normalized * 6 ) ; // 6 heading types echo `` < h '' . $ heading. '' > '' . $ artist [ $ x ] . `` < /h '' . $ heading .
`` > '' ; }I think the best you can do in this case is to take their input and then show them what you think they meant .
If they disagree , show them the format you 're expecting and get them to enter it again .I have a little game written in C # .
It uses a database as back-end .
It 's a trading card game , and I wanted to implement the function of the cards as a script .
What I mean is that I essentially have an interface , ICard , which a card class implements ( public class Card056 : ICard ) and which contains function that are called by the game .
Now , to make the thing maintainable/moddable , I would like to have the class for each card as source code in the database and essentially compile it on first use .
So when I have to add/change a card , I 'll just add it to the database and tell my application to refresh , without needing any assembly deployment ( especially since we would be talking about 1 assembly per card which means hundreds of assemblies ) .
Is that possible ?
Register a class from a source file and then instantiate it , etc .
@ code1 @ The language is C # , but extra bonus if it 's possible to write the script in any .NET language .I have been trying to implement Win32 's MessageBox using GTK .
The app using SDL/OpenGL , so this is n't a GTK app .
I handle the initialisation ( gtk_init ) sort of stuff inside the MessageBox function as follows : @ code1 @ Now , I am by no means an experienced GTK programmer , and I realise that I 'm probably doing something ( s ) horribly wrong .
However , my problem is that the last dialog popped up with this function stays around until the process exits .
Any ideas ?What 's the optimal level of concurrency that the C++ implementation of BerkeleyDB can reasonably support ?
How many threads can I have hammering away at the DB before throughput starts to suffer because of resource contention ?
I 've read the manual and know how to set the number of locks , lockers , database page size , etc. , but I 'd just like some advice from someone who has real-world experience with BDB concurrency .
My application is pretty simple , I 'll be doing gets and puts of records that are about 1KB each .
No cursors , no deleting .What are the best practices for checking in BIN directories in a collaborative development environment using SVN ?
Should project level references be excluded from checkin ?
Is it easier to just add all bin directories ?
I develop a lot of DotNetNuke sites and it seems that in a multi-developer environment , it 's always a huge task to get the environment setup correctly .
The ultimate goal ( of course ) is to have a new developer checkout the trunk from SVN , restore the DNN database and have it all just 'work ' ...Any assemblies that are expected to be in the GAC should stay in the GAC .
This includes System.web.dll or any other 3rd party dll that you 'll deploy to the GAC in production .
This means a new developer would have to install these assemblies .
All other 3rd party assemblies should be references through a relative path .
My typical structure is : -Project -- Project.sln -- References -- -StructureMap.dll -- -NUnit.dll -- -System.Web.Mvc.dll -- Project.Web -- -Project.Web.Proj -- -Project.Web.Proj files -- Project -- -Project.Proj -- -Project.Proj files Project.Web and Project reference the assemblies in the root/References folder relatively .
These .dlls are checked into subversion .
Aside from that , */bin */bin/* obj should be in your global ignore path .
With this setup , all references to assemblies are either through the GAC ( so should work across all computers ) , or relative to each project within your solution .Is this a .Net specific question ?
Generally the best practice is to not check in anything which is built automatically from files that are already in SCM .
All of that is ideally created as part of your automatic build process .
If the bin directory you 're referring to contains third-party binaries , rather than a build of your project , ignore ( downvote ? )
this advice .Yes , I thought about that , but I soon figured out that another Domain-Specific-Language ( DSL ) would be a bit too much .
Essentially , they need to interact with my gamestate in possibly unpredictable ways .
For example , a card could have a rule `` When this cards enter play , all your undead minions gain +3 attack against flying enemies , except when the enemy is blessed '' .
As trading card games are turn based , the GameState Manager will fire OnStageX events and let the cards modify other cards or the GameState in whatever way the card needs .
If I try to create a DSL , I have to implement a rather large feature set and possibly constantly update it , which shifts the maintenance work to another part without actually removing it .
That 's why I wanted to stay with a `` real '' .NET language to essentially be able to just fire the event and let the card manipulate the gamestate in whatever way ( within the limits of the code access security ) .@ Ryan That 's correct but it actually makes the tags with the least number , larger .
This code has been tested : $ artist = array ( `` the roots '' , '' michael jackson '' , '' billy idol '' , '' more '' , '' and more '' , '' and_YET_MORE '' ) ; $ count = array ( 5,3,9,1,1,3 ) ; $ highest = max ( $ count ) ; for ( $ x = 0 ; $ x < count ( $ artist ) ; $ x++ ) { $ normalized = ( $ highest - $ count [ $ x ] +1 ) / $ highest ; $ heading = ceil ( $ normalized * 6 ) ; // 6 heading types echo `` < h $ heading > { $ artist [ $ x ] } < /h $ heading > '' ; }kevind wrote : @ Ryan That 's correct but it actually makes the tags with the least number , larger .
This code has been tested : I actually meant to mention this in my answer- the original poster specified higher frequencies in higher-number tags , but HTML uses lower numbers for more significant headings .
I wrote my code to spec .
; PI actually meant to mention this in my answer- the original poster specified higher frequencies in higher-number tags , but HTML uses lower numbers for more significant headings .
I wrote my code to spec .
; P AH I see now , it seemed counter-intuitive to me that the higher the count the smaller the tag ?
My mistake .I often have to sort a dictionary , consisting of keys & values , by value .
For example , I have a hash of words and respective frequencies , that I want to order by frequency .
There is a SortedList which is good for a single value ( say frequency ) , that I want to map it back to the word .
SortedDictionary orders by key , not value .
Some resort to a custom class , but is there a cleaner way ?I know that Visual Studio itself ( at least in 2003 version ) references the IE dll directly to render the `` Design View '' .
It may be worth looking into that .
Otherwise , I ca n't think of anything beyond the Web Browser control .Looking around , and using some C # 3.0 features we can do this : @ code1 @ This is the cleanest way I 've seen and is similar to the Ruby way of handling hashes .On a high level , you have no other choice then to walk through the whole Dictionary and look at each value .
Maybe this helps : http : //bytes.com/forum/thread563638.html Copy/Pasting from John Timney : @ code1 @I usually just specify this in an .htaccess file or the vhost.conf on the domain I 'm working on .
Add this to one of these files : @ code1 @@ code1 @ Since you 're targeting .net 2.0 or above , you can simplify this into lambda syntax -- it 's equivalent but shorter .
If you 're targeting .net 2.0 you can only use this syntax if you 're using the compiler from vs2008 ( or above ) .
@ code2 @Try removing the 0x first and then call Encoding.UTF8.GetString .
I think that may work .
Essentially : 0x44004500 Remove the 0x , and then always two bytes are one character : @ code1 @ So it 's definitely a Unicode/UTF format with two bytes/character .Oleg Shilo 's C # Script solution ( at The Code Project ) really is a great introduction to providing script abilities in your application .
A different approach would be to consider a language that is specifically built for scripting , such as IronRuby , IronPython , or Lua .
IronPython and IronRuby are both available today .
For a guide to embedding IronPython read How to embed IronPython script support in your existing app in 10 easy steps .
Lua is a scripting language commonly used in games .
There is a Lua compiler for .NET , available from CodePlex -- http : //www.codeplex.com/Nua That codebase is a great read if you want to learn about building a compiler in .NET .
A different angle altogether is to try PowerShell .
There are numerous examples of embedding PowerShell into an application -- here 's a thorough project on the topic : Powershell TunnelI often run into the following problem .
I work on some changes to a project that require new tables or columns in the database .
I make the database modifications and continue my work .
Usually , I remember to write down the changes so that they can be replicated on the live system .
However , I do n't always remember what I 've changed and I do n't always remember to write it down .
So , I make a push to the live system and get a big , obvious error that there is no NewColumnX , ugh .
Regardless of the fact that this may not be the best practice for this situation , is there a version control system for databases ?
I do n't care about the specific database technology .
I just want to know if one exists .
If it happens to work with MS SQL Server , then great .Most database engines should support dumping your database into a file .
I know MySQL does , anyway .
This will just be a text file , so you could submit that to Subversion , or whatever you use .
It 'd be easy to run a diff on the files too .What are some guidelines for maintaining responsible session security with PHP ?
There 's information all over the web and it 's about time it all landed in one place !One guideline is to call session_regenerate_id every time a session 's security level changes .
This helps prevent session hijacking .I am working on a collection of classes used for video playback and recording .
I have one main class which acts like the public interface , with methods like play ( ) , stop ( ) , pause ( ) , record ( ) etc ... Then I have workhorse classes which do the video decoding and video encoding .
I just learned about the existence of nested classes in C++ , and I 'm curious to know what programmers think about using them .
I am a little wary and not really sure what the benefits/drawbacks are , but they seem ( according to the book I 'm reading ) to be used in cases such as mine .
The book suggests that in a scenario like mine , a good solution would be to nest the workhorse classes inside the interface class , so there are no separate files for classes the client is not meant to use , and to avoid any possible naming conflicts ?
I do n't know about these justifications .
Nested classes are a new concept to me .
Just want to see what programmers think about the issue .I think one of the major problems ( which is being addressed in PHP 6 ) is register_globals .
Right now one of the standard methods used to avoid register_globals is to use the $ _REQUEST , $ _GET or $ _POST arrays .
The `` correct '' way to do it ( as of 5.2 , although it 's a little buggy there , but stable as of 6 , which is coming soon ) is through filters .
So instead of : @ code1 @ you would do : @ code2 @ or even just : @ code3 @I would be a bit reluctant to use nested classes here .
What if you created an abstract base class for a `` multimedia driver '' to handle the back-end stuff ( workhorse ) , and a separate class for the front-end work ?
The front-end class could take a pointer/reference to an implemented driver class ( for the appropriate media type and situation ) and perform the abstract operations on the workhorse structure .
My philosophy would be to go ahead and make both structures accessible to the client in a polished way , just under the assumption they would be used in tandem .
I would reference something like a QTextDocument in Qt .
You provide a direct interface to the bare metal data handling , but pass the authority along to an object like a QTextEdit to do the manipulation .This is pretty trivial and obvious , but be sure to session_destroy after every use .
This can be difficult to implement if the user does not log out explicitly , so a timer can be set to do this .
Here is a good tutorial on setTimer ( ) and clearTimer ( ) .When is it appropriate to use an unsigned variable over a signed one ?
What about in a for loop ?
I hear a lot of opinions about this and I wanted to see if there was anything resembling a consensus .
@ code1 @ I know Java does n't have unsigned values , and that must have been a concious decision on Sun Microsystems ' part .I 'm about to build a piece of a project that will need to construct and post an XML document to a web service and I 'd like to do it in Python , as a means to expand my skills in it .
Unfortunately , whilst I know the XML model fairly well in .NET , I 'm uncertain what the pros and cons are of the XML models in Python .
Anyone have experience doing XML processing in Python ?
Where would you suggest I start ?
The XML files I 'll be building will be fairly simple .Dive Into Python has a chapter .
Ca n't vouch for how good it would be though .I was glad to find a good conversation on this subject , as I had n't really given it much thought before .
In summary , signed is a good general choice - even when you 're dead sure all the numbers are positive - if you 're going to do arithmetic on the variable ( like in a typical for loop case ) .
If you 're going to do bitwise things like masks , unsigned starts to make more sense .
Or , if you 're desperate to get that extra positive range by taking advantage of the sign bit .
Personally , I like signed because I do n't trust myself to stay consistent and avoid mixing the two types ( like the article warns against ) .Personally , I 've played with several of the built-in options on an XML-heavy project and have settled on pulldom as the best choice for less complex documents .
Especially for small simple stuff , I like the event-driven theory of parsing rather than setting up a whole slew of callbacks for a relatively simple structure .
Here is a good quick discussion of how to use the API .
What I like : you can handle the parsing in a for loop rather than using callbacks .
You also delay full parsing ( the `` pull '' part ) and only get additional detail when you call expandNode ( ) .
This satisfies my general requirement for `` responsible '' efficiency without sacrificing ease of use and simplicity .You might be able to use IronRuby for that .
Otherwise I 'd suggest you have a directory where you place precompiled assemblies .
Then you could have a reference in the DB to the assembly and class , and use reflection to load the proper assemblies at runtime .
If you really want to compile at run-time you could use the CodeDOM , then you could use reflection to load the dynamic assembly .
MSDN article which might help .In your example above , when 'i ' will always be positive and a higher range would be beneficial , unsigned would be useful .
Like if you 're using 'declare ' statements , such as : # declare BIT1 ( unsigned int 1 ) # declare BIT32 ( unsigned int reallybignumber ) Especially when these values will never change .
However , if you 're doing an accounting program where the people are irresponsible with their money and are constantly in the red , you will most definitely want to use 'signed ' .
I do agree with saint though that a good rule of thumb is to use signed , which C actually defaults to , so you 're covered .Take a look at the CHECKSUM command : @ code1 @ That will return the same number each time it 's run as long as the table contents have n't changed .
See my post on this for more information : CHECKSUM Here 's how I used it to rebuild cache dependencies when tables changed : ASP.NET 1.1 database cache dependency ( without triggers )size_t is often a good choice for this , or size_type if you 're using an STL class .For Oracle , I use Toad , which can dump a schema to a number of discrete files ( e.g. , one file per table ) .
I have some scripts that manage this collection in Perforce , but I think it should be easily doable in just about any revision control system .timezone.js : @ code1 @ timezone.php : @ code2 @ When you want to use it add onLoad= '' ajaxpage ( ) ; to the body tag and it should cause the timezone to be stored in the PHP session variable $ _SESSION [ 'time ' ] Edit : P.S .
This is untested .You could use any of the DLR languages , which provide a way to really easily host your own scripting platform .
However , you do n't have to use a scripting language for this .
You could use C # and compile it with the C # code provider .
As long as you load it in its own AppDomain , you can load and unload it to your heart 's content .In Ruby on Rails , there 's a concept of a migration -- a quick script to change the database .
You generate a migration file , which has rules to increase the db version ( such as adding a column ) and rules to downgrade the version ( such as removing a column ) .
Each migration is numbered , and a table keeps track of your current db version .
To _migrate up_ , you run a command called `` db : migrate '' which looks at your version and applies the needed scripts .
You can migrate down in a similar way .
The migration scripts themselves are kept in a version control system -- whenever you change the database you check in a new script , and any developer can apply it to bring their local db to the latest version .How would I go about generating a list of all possible permutations of a string between x and y characters in length , containing a variable list of characters .
Any language would work , but it should be portable .There are several ways to do this .
Common methods use recursion , memoization , or dynamic programming .
The basic idea is that you produce a list of all strings of length 1 , then in each iteration , for all strings produced in the last iteration , add that string concatenated with each character in the string individually .
( the variable index in the code below keeps track of the start of the last and the next iteration ) Some pseudocode : @ code1 @ you 'd then need to remove all strings less than x in length , they 'll be the first ( x-1 ) * len ( originalString ) entries in the list .There 's a PHP5 `` database migration framework '' called Ruckusing .
I have n't used it , but the examples show the idea , if you use the language to create the database as and when needed , you only have to track source files .I just whipped this up quick in Ruby : def perms ( x , y , possible_characters ) all = [ `` '' ] current_array = all.clone 1.upto ( y ) { |iteration| next_array = [ ] current_array.each { |string| possible_characters.each { |c| value = string + c next_array.insert next_array.length , value all.insert all.length , value } } current_array = next_array } all.delete_if { |string| string.length < x } end You might look into language API for built in permutation type functions , and you might be able to write more optimized code , but if the numbers are all that high , I 'm not sure there is much of a way around having a lot of results .
Anyways , the idea behind the code is start with string of length 0 , then keep track of all the strings of length Z where Z is the current size in the iteration .
Then , go through each string and append each character onto each string .
Finally at the end , remove any that were below the x threshold and return the result .
I did n't test it with potentially meaningless input ( null character list , weird values of x and y , etc ) .High Resolution , Low Overhead Timing for Intel Processors If you 're on Intel hardware , here 's how to read the CPU real-time instruction counter .
It will tell you the number of CPU cycles executed since the processor was booted .
This is probably the finest-grained counter you can get for performance measurement .
Note that this is the number of CPU cycles .
On linux you can get the CPU speed from /proc/cpuinfo and divide to get the number of seconds .
Converting this to a double is quite handy .
When I run this on my box , I get @ code1 @ Here 's the Intel developer 's guide that gives tons of detail .
@ code2 @Personally I would do something like this : @ code1 @ Assuming I 'm not a complete idiot this should work .
But it is untested .This is a tricky one and I 've always relied on techniques , such as permission-based emails ( i.e .
only sending to people you have permission to send to ) and not using blatantly spamish terminology .
Of late , some of the emails I send out programmatically have started being shuffled into people 's spam folder automatically and I 'm wondering what I can do about it .
This is despite the fact that these particular emails are not ones that humans would mark as spam , specifically , they are emails that contain license keys that people have paid good money for , so I do n't think they 're going to consider them spam I figure this is a big topic in which I am essentially an ignorant simpleton .You can tell your users to add your From address to their contacts when they complete their order , which , if they do so , will help a lot .
Otherwise , I would try to get a log from some of your users .
Sometimes they have details about why it was flagged as spam in the headers of the message , which you could use to tweak the text .
Other things you can try : Put your site name or address in the subject Keep all links in the message pointing to your domain ( and not email.com ) Put an address or other contact information in the emailWhat is the meaning of the Java warning ?
Type safety : The cast from Object to List is actually checking against the erased type List I get this warning when I try to cast an Object to a type with generic information , such as in the following code : @ code1 @This warning is there because Java is not actually storing type information at run-time in an object that uses generics .
Thus , if object is actually a List < String > , there will be no ClassCastException at run-time except until an item is accessed from the list that does n't match the generic type defined in the variable .
This can cause further complications if items are added to the list , with this incorrect generic type information .
Any code still holding a reference to the list but with the correct generic type information will now have an inconsistent list .
To remove the warning , try : @ code1 @ However , note that you will not be able to use certain methods such as add because the compiler does n't know if you are trying to add an object of incorrect type .
The above will work in a lot of situations , but if you have to use add , or some similarly restricted method , you will just have to suffer the yellow underline in Eclipse ( or a SuppressWarning annotation ) .In Lucene if you had multiple indexes that covered only one partition each .
Why does the same search on different indexes return results with different scores ?
The results from different servers match exactly .
I.e .
if i searched for : Name - John Smith DOB - 11/11/1934 Partition 0 would return a score of 0.345 Partition 1 would return a score of 0.337 Both match exactly on name and DOB .I 'm not sure why you would want to do this in the first place .
The resulting set for any moderately large values of x and y will be huge , and will grow exponentially as x and/or y get bigger .
Lets say your set of possible characters is the 26 lowercase letters of the alphabet , and you ask your application to generate all permutations where length = 5 .
Assuming you do n't run out of memory you 'll get 11,881,376 ( i.e .
26 to the power of 5 ) strings back .
Bump that length up to 6 , and you 'll get 308,915,776 strings back .
These numbers get painfully large , very quickly .
Here 's a solution I put together in Java .
You 'll need to provide two runtime arguments ( corresponding to x and y ) .
Have fun .
public class GeneratePermutations { public static void main ( String [ ] args ) { int lower = Integer.parseInt ( args [ 0 ] ) ; int upper = Integer.parseInt ( args [ 1 ] ) ; if ( upper < lower || upper == 0 || lower == 0 ) { System.exit ( 0 ) ; } for ( int length = lower ; length < = upper ; length++ ) { generate ( length , `` '' ) ; } } private static void generate ( int length , String partial ) { if ( length < = 0 ) { System.out.println ( partial ) ; } else { for ( char c = 'a ' ; c < = 'z ' ; c++ ) { generate ( length - 1 , partial + c ) ; } } } }The scoring contains the Inverse Document Frequency ( IDF ) .
If the term `` John Smith '' is in one partition , 0 , 100 times and in partition 1 , once .
The score for searching for John Smith would be higher search in partition 1 as the term is more scarce .
To get round this you would wither have to have your index being over all partitions , or you would need to override the IDF .Because the score is determined on the Index if I am not completely mistaken .
If you have different indexes ( more/less or different data that was indexed ) , the score will differ : http : //lucene.apache.org/core/3_6_0/scoring.html ( Warning : Contains Math : - ) )Use email authentication methods , such as SPF , and DKIM to prove that your emails and your domain name belong together , and to prevent spoofing of your domain name .
The SPF website includes a wizard to generate the DNS information for your site .
Check your reverse DNS to make sure the IP address of your mail server points to the domain name that you use for sending mail .
Make sure that the IP-address that you 're using is not on a blacklist Make sure that the reply-to address is a valid , existing address .
Use the full , real name of the addressee in the To field , not just the email-address ( e.g .
`` John Smith '' < john @ blacksmiths-international.com > ) .
Monitor your abuse accounts , such as abuse @ yourdomain.com and postmaster @ yourdomain.com .
That means - make sure that these accounts exist , read what 's sent to them , and act on complaints .
Finally , make it really easy to unsubscribe .
Otherwise , your users will unsubscribe by pressing the spam button , and that will affect your reputation .
That said , getting Hotmail to accept your emails remains a black art .RDoc uses SimpleMarkup so it 's fairly simple to create lists , etc .
using * , - or a number .
It also treats lines that are indented at the same column number as part of the same paragraph until there is an empty line which signifies a new paragraph .
Do you have a few examples of comments you want RDoc'ed so we could show you how to do them and then you could extrapolate that for the rest of your comments ?Note : this question is from 2008 and now is of only historic interest .
What 's the best way to create an iPhone application that runs in landscape mode from the start , regardless of the position of the device ?
Both programmatically and using the Interface Builder .The best way to deploy video on the web is using Flash - it 's much easier to embed cleanly into a web page and will play on more or less any browser and platform combination .
The only reason to use Windows Media Player is if you 're streaming content and you need extraordinarily strong digital rights management , and even then providers are now starting to use Flash even for these .
See BBC 's iPlayer for a superb example .
I would suggest that you switch to Flash even for internal use .
You never know who is going to need to access it in the future , and this will give you the best possible future compatibility .
EDIT - March 20 2013 .
Interesting how these old questions resurface from time to time !
How different the world is today and how dated this all seems .
I would not recommend a Flash only route today by any means - best practice these days would probably be to use HTML 5 to embed H264 encoded video , with a Flash fallback as described here : http : //diveintohtml5.info/video.htmlIf the fact that the first button is used by default is consistent across browsers , why not put them the right way round in the source code , then use CSS to switch their apparent positions ?
float them left and right to switch them around visually , for example .Tortoise SVN with Ankhsvn for VS 2005Part of my everyday work is maintaining and extending legacy VB6 applications .
A common engine is written in C/C++ and VB6 uses these functions in order to improve performance .
When it comes to asynchronous programming , a C interface is not enough and we rely on COM controls to fire events to VB6 .
My problem is that when I register the control in VB6 , VB loads this control in memory and does not unload it until I quit the VB6 IDE .
As the control is loaded the whole time , I am unable to recompile it in VC6 , because the DLL file is locked .
A solution I found is not to enable the control in VB but use the CreateObject ( ) with the full name of my control .
The problem then is that I must declare my control as an Object because VB6 knows nothing of the interface I am using and I do not have access to IntelliSense , which is a pain .
Any idea how I can tell VB6 to unload controls after quitting the application ?
Or directly in the IDE ?Is there any way to launch IE Mobile 's `` Favorites '' screen directly by specifying any command line parameter ?The difference between 12.345 in French and English is a factor of 1000 .
If you supply an expected range where max < 1000*min , you can easily guess .
Take for example the height of a person ( including babies and children ) in mm .
By using a range of 200-3000 , an input of 1.800 or 1,800 can unambiguously be interpreted as 1 meter and 80 centimeters , whereas an input of 912.300 or 912,300 can unambiguously be interpreted as 91 centimeters and 2.3 millimeters .My Rails-app has a sign in box with a `` remember me '' checkbox .
Users who check that box should remain logged in even after closing their browser .
I 'm keeping track of whether users are logged in by storing their id in the user 's session .
But sessions are implemented in Rails as session cookies , which are not persistent .
I can make them persistent : class ApplicationController < ActionController : :Base before_filter : update_session_expiration_date private def update_session_expiration_date options = ActionController : :Base.session_options unless options [ : session_expires ] options [ : session_expires ] = 1.year.from_now end end end But that seems like a hack , which is surprising for such common functionality .
Is there a better way ?
Edit Gareth 's answer is pretty good , but I would still like an answer from someone familiar with Rails 2 ( because of its unique CookieSessionStore ) .Yahoo uses a method called Sender ID , which can be configured at The SPF Setup Wizard and entered in to your DNS .
Also one of the important ones for Exchange , Hotmail , AOL , Yahoo , and others is to have a Reverse DNS for your domain .
Those will knock out most of the issues .
However you can never prevent a person intentionally blocking your or custom rules .While you have n't said what you 're storing , and you may have a great reason for doing so , often the answer is 'as a filesystem reference ' and the actual data is on the filesystem somewhere .
http : //www.onlamp.com/pub/a/onlamp/2002/07/11/MySQLtips.htmlI am using the Photoshop 's javascript API to find the fonts in a given PSD .
Given a font name returned by the API , I want to find the actual physical font file that that font name corresponds to on the disc .
This is all happening in a python program running on OSX so I guess I 'm looking for one of : Some Photoshop javascript A Python function An OSX API that I can call from pythonI 've been writing a few web services for a .net app , now I 'm ready to consume them .
I 've seen numerous examples where there is homegrown code for consuming the service as opposed to using the auto generated methods Visual Studio creates when adding the web reference .
Is there some advantage to this ?ElementTree has a nice pythony API .
I think it 's even shipped as part of python 2.5 It 's in pure python and as I say , pretty nice , but if you wind up needing more performance , then lxml exposes the same API and uses libxml2 under the hood .
You can theoretically just swap it in when you discover you need it .No , what you 're doing is fine .
Do n't let those people confuse you .
If you 've written the web services with .net then the reference proxies generated by .net are going to be quite suitable .
The situation you describe ( where you are both producer and consumer ) is the ideal situation .
If you need to connect to a web services that is unknown at compile time , then you would want a more dynamic approach , where you deduce the 'shape ' of the web service .
But start by using the auto generated proxy class , and do n't worry about it until you hit a limitation .
And when you do -- come back to stack overflow ; - )Ok , I 'm a web developer , but I find myself dabbling in windows forms projects every now and then .
One thing that confounds me to this day is the following .
Assume you are doing something like the following @ code1 @ So now we have 2 combo boxes bound to that array , and everything works fine .
But when you change the value of one combo box , it changes BOTH combo boxes to the one you just selected .
Now , I know that Arrays are always passed by reference ( learned that when i learned C : D ) , but why on earth would the combo boxes change together ?
I do n't believe the combo box control is modifying the collection at all .
As a work around , do n't this would achieve the funcionality that is expected / desired @ code2 @You should almost certainly not be extending the session cookie to be long lived .
Although not dealing specifically with rails this article goes to some length to explain 'remember me ' best practices .
In summary though you should : Add an extra column to the user table to accept a large random value Set a long lived cookie on the client which combines the user id and the random value When a new session starts , check for the existence of the id/value cookie and authenticate the new user if they match .
The author also recommends invalidating the random value and resetting the cookie at every login .
Personally I do n't like that as you then ca n't stay logged into a site on two computers .
I would tend to make sure my password changing function also reset the random value thus locking out sessions on other machines .
As a final note , the advice he gives on making certain functions ( password change/email change etc ) unavailable to auto authenticated sessions is well worth following but rarely seen in the real world .open up a terminal ( Applications- > Utilities- > Terminal ) and type this in : locate InsertFontHere This will spit out every file that has the name you want .
Warning : there may be alot to wade through .I have a cross-platform ( Python ) application which needs to generate a JPEG preview of the first page of a PDF .
On the Mac I am spawning sips .
Is there something similarly simple I can do on Windows ?This has to do with how data bindings are set up in the dotnet framework , especially the BindingContext .
On a high level it means that if you have n't specified otherwise each form and all the controls of the form share the same BindingContext .
When you are setting the DataSource property the ComboBox will use the BindingContext to get a ConcurrenyMangager that wraps the list .
The ConcurrenyManager keeps track of such things as the current selected position in the list .
When you set the DataSource of the second ComboBox it will use the same BindingContext ( the forms ) which will yield a reference to the same ConcurrencyManager as above used to set up the data bindings .
To get a more detailed explanation see BindingContext .I have a Ruby on Rails Website that makes HTTP calls to an external Web Service .
About once a day I get a SystemExit ( stacktrace below ) error email where a call to the service has failed .
If I then try the exact same query on my site moments later it works fine .
It 's been happening since the site went live and I 've had no luck tracking down what causes it .
Ruby is version 1.8.6 and rails is version 1.2.6 .
Anyone else have this problem ?
This is the error and stacktrace .
A SystemExit occurred /usr/local/lib/ruby/gems/1.8/gems/rails-1.2.6/lib/fcgi_handler.rb:116 : in `exit' /usr/local/lib/ruby/gems/1.8/gems/rails-1.2.6/lib/fcgi_handler.rb:116 : in `exit_now_handler' /usr/local/lib/ruby/gems/1.8/gems/activesupport-1.4.4/lib/active_support/inflector.rb:250 : in `to_proc' /usr/local/lib/ruby/1.8/net/protocol.rb:133 : in `call' /usr/local/lib/ruby/1.8/net/protocol.rb:133 : in `sysread' /usr/local/lib/ruby/1.8/net/protocol.rb:133 : in `rbuf_fill' /usr/local/lib/ruby/1.8/timeout.rb:56 : in `timeout' /usr/local/lib/ruby/1.8/timeout.rb:76 : in `timeout' /usr/local/lib/ruby/1.8/net/protocol.rb:132 : in `rbuf_fill' /usr/local/lib/ruby/1.8/net/protocol.rb:116 : in `readuntil' /usr/local/lib/ruby/1.8/net/protocol.rb:126 : in `readline' /usr/local/lib/ruby/1.8/net/http.rb:2017 : in `read_status_line' /usr/local/lib/ruby/1.8/net/http.rb:2006 : in `read_new' /usr/local/lib/ruby/1.8/net/http.rb:1047 : in `request' /usr/local/lib/ruby/1.8/net/http.rb:945 : in `request_get' /usr/local/lib/ruby/1.8/net/http.rb:380 : in `get_response' /usr/local/lib/ruby/1.8/net/http.rb:543 : in `start' /usr/local/lib/ruby/1.8/net/http.rb:379 : in `get_response'Martin Fowler wrote my favorite article on the subject , http : //martinfowler.com/articles/evodb.html .
I choose not to put schema dumps in under version control as alumb and others suggest because I want an easy way to upgrade my production database .
For a web application where I 'll have a single production database instance , I use two techniques : Database Upgrade Scripts A sequence database upgrade scripts that contain the DDL necessary to move the schema from version N to N+1 .
( These go in your version control system . )
A version_history table , something like create table VersionHistory ( Version int primary key , UpgradeStart datetime not null , UpgradeEnd datetime ) ; gets a new entry every time an upgrade script runs which corresponds to the new version .
This ensures that it 's easy to see what version of the database schema exists and that database upgrade scripts are run only once .
Again , these are not database dumps .
Rather , each script represents the changes necessary to move from one version to the next .
They 're the script that you apply to your production database to `` upgrade '' it .
Developer Sandbox Synchronization A script to backup , sanitize , and shrink a production database .
Run this after each upgrade to the production DB .
A script to restore ( and tweak , if necessary ) the backup on a developer 's workstation .
Each developer runs this script after each upgrade to the production DB .
A caveat : My automated tests run against a schema-correct but empty database , so this advice will not perfectly suit your needs .I have n't been able to find anything that does this directly .
I think you 'll have to iterate through the various font folders on the system : /System/Library/Fonts , /Library/Fonts , and there can probably be a user-level directory as well ~/Library/Fonts .Using fcgi with Ruby is known to be very buggy .
Practically everybody has moved to Mongrel for this reason , and I recommend you do the same .You did n't mention any specifics about your target environment or constraints , so this may not be entirely applicable ... but if you 're looking for a way to effectively track an evolving DB schema and are n't adverse to the idea of using Ruby , ActiveRecord 's migrations are right up your alley .
Migrations programatically define database transformations using a Ruby DSL ; each transformation can be applied or ( usually ) rolled back , allowing you to jump to a different version of your DB schema at any given point in time .
The file defining these transformations can be checked into version control like any other piece of source code .
Because migrations are a part of ActiveRecord , they typically find use in full-stack Rails apps ; however , you can use ActiveRecord independent of Rails with minimal effort .
See here for a more detailed treatment of using AR 's migrations outside of Rails .So it says microseconds explicitly , but says the resolution of the system clock is unspecified .
I suppose resolution in this context means how the smallest amount it will ever be incremented ?
The data structure is defined as having microseconds as a unit of measurement , but that does n't mean that the clock or operating system is actually capable of measuring that finely .
Like other people have suggested , gettimeofday ( ) is bad because setting the time can cause clock skew and throw off your calculation .
clock_gettime ( CLOCK_MONOTONIC ) is what you want , and clock_getres ( ) will tell you the precision of your clock .Since you mentioned that you 'll be building `` fairly simple '' XML , the minidom module ( part of the Python Standard Library ) will likely suit your needs .
If you have any experience with the DOM representation of XML , you should find the API quite straight forward .Is n't it also a factor which order you set up the colors ?
Like if you use Dillie-Os idea you need to mix the colors as much as possible .
0 64 128 256 is from one to the next .
but 0 256 64 128 in a wheel would be more `` apart '' Does this make sense ?Does n't this depend on the hardware as well as number of threads and stuff ?
I would make a simple test and run it with increasing amounts of threads hammering and see what seems best .The Monte Carlo method , as mentioned , applies some great concepts but it is , clearly , not the fastest -- not by a long shot , not by any reasonable usefulness .
Also , it all depends on what kind of accuracy you are looking for .
The fastest pi I know of is the digits hard coded .
Looking at Pi and Pi [ PDF ] , there are a lot of formulas .
Here is a method that converges quickly ( ~14digits per iteration ) .
The current fastest application , PiFast , uses this formula with the FFT .
I 'll just write the formula , since the code is straight forward .
This formula was almost found by Ramanujan and discovered by Chudnovsky .
It is actually how he calculated several billion digits of the number -- so it is n't a method to disregard .
The formula will overflow quickly since we are dividing factorials , it would be advantageous then to delay such calculating to remove terms .
where , Below is the Brent–Salamin algorithm .
Wikipedia mentions that when a and b are 'close enough ' then ( a+b ) ^2/4t will be an approximation of pi .
I 'm not sure what 'close enough ' means , but from my tests , one iteration got 2digits , two got 7 , and three had 15 , of course this is with doubles , so it might have error based on its representation and the 'true ' calculation could be more accurate .
@ code1 @ Lastly , how about some pi golf ( 800 digits ) ?
160 characters !
@ code2 @Maven helps quite a lot with this problem when I 'm coding java .
We commit the pom.xml to the scs and the maven repository contains all our dependencies .
For me that seems like a nice way to do it .I 'm starting work on a hobby project with a python codebase and would like to set up some form of continuous integration ( i.e .
running a battery of test-cases each time a check-in is made and sending nag e-mails to responsible persons when the tests fail ) similar to CruiseControl or TeamCity .
I realize I could do this with hooks in most VCSes , but that requires that the tests run on the same machine as the version control server , which is n't as elegant as I would like .
Does anyone have any suggestions for a small , user-friendly , open-source continuous integration system suitable for a Python codebase ?You can use ImageMagick 's convert utility for this , see some examples in http : //studio.imagemagick.org/pipermail/magick-users/2002-May/002636.html : @ code1 @ Will convert a two page PDF file into [ 2 ] jpeg files : taxes.jpg.0 , taxes.jpg.1 I can also convert these JPEGS to a thumbnail as follows : @ code2 @ I can even convert the PDF directly to a jpeg thumbnail as follows : @ code3 @ This will result in a thumbnail.jpg.0 and thumbnail.jpg.1 for the two pages .A few things : You are creating ( and not using ) an unnecessary toplevel window , named window .
You can just delete these lines : window = gtk_window_new ( GTK_WINDOW_TOPLEVEL ) ; g_signal_connect ( G_OBJECT ( window ) , `` delete_event '' , G_CALLBACK ( delete_event ) , NULL ) ; g_signal_connect ( G_OBJECT ( window ) , `` destroy '' , G_CALLBACK ( destroy ) , NULL ) ; Also , the flow does n't seem quite right .
gtk_main ( ) starts the GTK main loop , which blocks until something exits it .
gtk_dialog_run ( ) also starts a main loop , but it exits as soon as one of the buttons is clicked .
I think it might be enough for you to remove the gtk_init_add ( ) and gtk_main ( ) calls , and simply deal with the return value .
Also the gtk_widget_destroy ( ) call is unnecessary , as the dialog window is automatically destroyed when gtk_dialog_run ( ) returns .One possibility is Hudson .
It 's written in Java , but there 's integration with Python projects : Hudson embraces Python I 've never tried it myself , however .
( Update , Sept. 2011 : After a trademark dispute Hudson has been renamed to Jenkins . )My first thought on this is `` how generate N vectors in a space that maximize distance from each other . ''
You can see that the RGB ( or any other scale you use that forms a basis in color space ) are just vectors .
Take a look at Random Point Picking .
Hope this is a good start for you !
Once you have a set of vectors that are maximized a part , you can save them in a hash table or something for later , and just perform random rotations on them to get all the colors you desire that are maximally apart from each other !
Edit : Thinking about this problem more , it would be better to map the colors in a linear manor , possibly ( 0,0,0 ) -- > ( 255,255,255 ) lexicographically , and then distribute them evenly .
I really do n't know how well this will work , but it should since , lets say : n = 10 we know we have 16777216 colors ( 256^3 ) .
We can use buckles algorithm 515 to find the lexicographically indexed color.. You 'll probably have to edit the algorithm to avoid overflow and probably add some minor speed improvements .AFAIK , there 's no ready-made library to do this for you , but producing a tool capable of translating from CSV to XML should only require you to write a crude CSV parser and hook up JDOM ( or your XML Java library of choice ) with some glue code .We run Buildbot - Trac at work , I have n't used it too much since my code base is n't part of the release cycle yet .
But we run the tests on different environments ( OSX/Linux/Win ) and it sends emails -- and it 's written in python .Form-based authentication for websites We believe that Stack Overflow should not just be a resource for very specific technical questions , but also for general guidelines on how to solve variations on common problems .
`` Form based authentication for websites '' should be a fine topic for such an experiment .
It should include topics such as : How to log in How to remain logged in How to store passwords Using secret questions Forgotten username/password functionality OpenID '' Remember me '' checkbox Browser autocompletion of usernames and passwords Secret URLs ( public URL protected by digest ) Checking password strength E-mail validation and much more about form based authentication ...
It should not include things like : roles and authorization HTTP basic authentication Please help us by Suggesting subtopics Submitting good articles about this subject Editing the official answerYou may also be interested in the output of the explain ( ) method , and the resulting Explanation object , which will give you an idea of how things are scored the way they are .Definitive Article Sending credentials The only practical way to send credentials 100 % securely is by using SSL .
Using JavaScript to hash the password is not safe .
Common pitfalls for client-side password hashing : If the connection between the client and server is unencrypted , everything you do is vulnerable to man-in-the-middle attacks .
An attacker could replace the incoming javascript to break the hashing or send all credentials to their server , they could listen to client responses and impersonate the users perfectly , etc .
etc .
SSL with trusted Certificate Authorities is designed to prevent MitM attacks .
The hashed password received by the server is less secure if you do n't do additional , redundant work on the server .
There 's another secure method called SRP , but it 's patented ( although it is freely licensed ) and there are few good implementations available .
Storing passwords Do n't ever store passwords as plaintext in the database .
Not even if you do n't care about the security of your own site .
Assume that some of your users will reuse the password of their online bank account .
So , store the hashed password , and throw away the original .
And make sure the password does n't show up in access logs or application logs .
The best hashing function seems to be bcrypt .
Hashes by themselves are also insecure .
For instance , identical passwords mean identical hashes -- this makes hash lookup tables an effective way of cracking lots of passwords at once .
Instead , store the salted hash .
A salt is a string appended to the password prior to hashing - use a different ( random ) salt per user .
The salt is a public value , so you can store them with the hash in the database .
See here for more on this .
This means that you ca n't send the user their forgotten passwords ( because you only have the hash ) .
Do n't reset the user 's password unless you have authenticated the user ( users must prove that they are able to read emails sent to the stored ( and validated ) email address . )
Security questions Security questions are insecure - avoid using them .
Why ?
Anything a security question does , a password does better .
Read PART III : Using Secret Questions in @ Jens Roland answer here in this wiki .
Session cookies After the user logs in , the server sends the user a session cookie .
The server can retrieve the username or id from the cookie , but nobody else can generate such a cookie ( TODO explain mechanisms ) .
Cookies can be hijacked : they are only as secure as the rest of the client 's machine and other communications .
They can be read from disk , sniffed in network traffic , lifted by a cross-site scripting attack , phished from a poisoned DNS so the client sends their cookies to the wrong servers .
Do n't send persistent cookies .
Cookies should expire at the end of the client session ( browser close or leaving your domain ) .
If you want to autologin your users , you can set a persistent cookie , but it should be distinct from a full-session cookie .
You can set an additional flag that the user has auto-logged in , and needs to login for real for sensitive operations .
This is popular with shopping sites that want to provide you with a seamless , personalized shopping experience but still protect your financial details .
For example , when you return to visit Amazon , they show you a page that looks like you 're logged in , but when you go to place an order ( or change your shipping address , credit card etc .
) , they ask you to confirm your password .
Financial web sites such as banks and credit cards , on the other hand , only have sensitive data and should not allow auto-login or a low-security mode .
List of external resources Dos and Don'ts of Client Authentication on the Web ( PDF ) 21 page academic article with many great tips .
Ask YC : Best Practices for User Authentication Forum discussion on the subject You 're Probably Storing Passwords Incorrectly Introductory article about storing passwords Discussion : Coding Horror : You 're Probably Storing Passwords Incorrectly Forum discussion about a Coding Horror article .
Never store passwords in a database !
Another warning about storing passwords in the database .
Password cracking Wikipedia article on weaknesses of several password hashing schemes .
Enough With The Rainbow Tables : What You Need To Know About Secure Password Schemes Discussion about rainbow tables and how to defend against them , and against other threads .
Includes extensive discussion .I would like to test a function with a tuple from a set of fringe cases and normal values .
For example , while testing a function which returns true whenever given three lengths that form a valid triangle , I would have specific cases , negative / small / large numbers , values close-to being overflowed , etc .
; what is more , main aim is to generate combinations of these values , with or without repetition , in order to get a set of test data .
@ code1 @ As a note : I actually know the answer to this , but it might be helpful for others , and a challenge for people here !
-- will post my answer later on .Uploading my first decently sized web app to my shared host provided me with a fresh set of challenges , by which I mean , sleepless nights .
The issue was that I had most certainly not developed my application for medium trust ( or had any clue what that was . )
I mitigated all of the issues , save one .
I had written an installer for the admin to be able to specify their connectionstring and other preferences , but I can not find a way to write to a web.config in medium trust .
Does anyone have a solution , or should I just be putting preferences in another file ?I was just sitting at my local Borders sipping coffee and reading More Joel on Software ( for free ) when I came across Joel Spolsky saying something about a particular type of programmer knowing the difference between an int and an Integer in Java/C # ( Object Oriented Programming Languages ) .
After a quick 'brain check , ' I realized , to my dismay , that I did n't know the answer .In Java , the 'int ' type is a primitive , whereas the 'Integer ' type is an object .
In C # , the 'int ' type is the same as System.Int32 and is a value type ( ie more like the java 'int ' ) .
An integer ( just like any other value types ) can be boxed ( `` wrapped '' ) into an object .
The differences between objects and primitives are somewhat beyond the scope of this question , but to summarize : Objects provide facilities for polymorphism , are passed by reference ( or more accurately have references passed by value ) , and are allocated from the heap .
Conversely , primitives are immutable types that are passed by value and are often allocated from the stack .Well , in Java an int is a primitive while an Integer is an Object .
Meaning , if you made a new Integer : @ code1 @ You could call some method on i : @ code2 @ Whereas with an int : @ code3 @ You can not call any methods on it , because it is simply a primitive .
So : @ code4 @ would produce an error , because int is not an object .
int is one of the few primitives in Java ( along with char and some others ) .
I 'm not 100 % sure , but I 'm thinking that the Integer object more or less just has an int property and a whole bunch of methods to interact with that property ( like the toString ( ) method for example ) .
So Integer is a fancy way to work with an int ( Just as perhaps String is a fancy way to work with a group of chars ) .
I know that Java is n't C , but since I 've never programmed in C this is the closest I could come to the answer .
Hope this helps !
Integer object javadoc Integer Ojbect vs. int primitive comparisonSign up for an account on as many major email providers as possible ( gmail/yahoo/hotmail/aol/etc ) .
If you make changes to your emails , either major rewording , changes to the code that sends the emails , changes to your email servers , etc , make sure to send test messages to all your accounts and verify that they are not being marked as spam .I wonder how you guys manage deployment of a database between 2 SQL Servers , specifically SQL Server 2005 .
Now , there is a development and a live one .
As this should be part of a buildscript ( standard windows batch , even do with current complexity of those scripts , i might switch to PowerShell or so later ) , Enterprise Manager/Management Studio Express do not count .
Would you just copy the .mdf File and attach it ?
I am always a bit careful when working with binary data , as this seems to be a compatiblity issue ( even though development and live should run the same version of the server at all time ) .
Or - given the lack of `` EXPLAIN CREATE TABLE '' in T-SQL - do you do something that exports an existing database into SQL-Scripts which you can run on the target server ?
If yes , is there a tool that can automatically dump a given Database into SQL Queries and that runs off the command line ?
( Again , Enterprise Manager/Management Studio Express do not count ) .
And lastly - given the fact that the live database already contains data , the deployment may not involve creating all tables but rather checking the difference in structure and ALTER TABLE the live ones instead , which may also need data verification/conversion when existing fields change .
Now , i hear a lot of great stuff about the Red Gate products , but for hobby projects , the price is a bit steep .
So , what are you using to automatically deploy SQL Server Databases from Test to Live ?That actually sounds like IIS 's Low level .
If it is , then you wo n't be able to write to any file , not just the web.config .
Here are the levels from IIS 's help file : Full ( internal ) - Specifies unrestricted permissions .
Grants the ASP.NET application permissions to access any resource that is subject to operating system security .
All privileged operations are supported .
High ( web_hightrust.config ) - Specifies a high level of code access security , which means that the application can not do any one of the following things by default : Call unmanaged code .
Call serviced components .
Write to the event log .
Access Message Queuing service queues .
Access ODBC , OleDb , or Oracle data sources .
Medium ( web_mediumtrust.config ) - Specifies a medium level of code access security , which means that , in addition to High Trust Level restrictions , the ASP.NET application can not do any of the following things by default : Access files outside the application directory .
Access the registry .
Make network or Web service calls .
Low ( web_lowtrust.config ) - Specifies a low level of code access security , which means that , in addition to Medium Trust Level restrictions , the application can not do any of the following things by default : Write to the file system .
Call the Assert method .
Minimal ( web_minimaltrust.config ) - Specifies a minimal level of code access security , which means that the application has only execute permissions .
I would suggest that if you are dead set on having an installer , have it create a web.config in memory that the user can save locally and FTP up afterward .In C # , int is just an alias for System.Int32 , string for System.String , double for System.Double etc ...
Personally I prefer int , string , double , etc .
because they do n't require a using System ; statement : ) A silly reason , I know ...For my projects I alternate between SQL Compare from REd Gate and the Database Publishing Wizard from Microsoft which you can download free here .
The Wizard is n't as slick as SQL Compare or SQL Data Compare but it does the trick .
One issue is that the scripts it generates may need some rearranging and/or editing to flow in one shot .
On the up side , it can move your schema and data which is n't bad for a free tool .I 've taken to hand-coding all of my DDL ( creates/alter/delete ) statements , adding them to my .sln as text files , and using normal versioning ( using subversion , but any revision control should work ) .
This way , I not only get the benefit of versioning , but updating live from dev/stage is the same process for code and database - tags , branches and so on work all the same .
Otherwise , I agree redgate is expensive if you do n't have a company buying it for you .
If you can get a company to buy it for you though , it really is worth it !I 'm writing a CMS application in PHP and one of the requirements is that it must be able to interface with the customer 's Exchange server .
I 've written up this functionality a few times before and have always used WebDAV to do it , but now I 'm leaning away from that .
I will be running the site on IIS OR Apache ( no preference ) on Windows server 2008 .
A few things I would need to do include adding contacts to a given user 's address book , sending emails as a given user and running reports on contacts for a user .
All of this is pretty easy to do with WebDAV , but if there is a better way that does n't require any functionality that is likely to be deprecated any time soon .
Any ideas ?
Update : Justin , I love the idea of using com objects , I just worry about maintaining a 3rd product to make everything work ... John , I can write a web service in C # to interface with for these functions and access it with my PHP app , but it 's also a little bit out of the way .
So far , I 'm not 100 % convinced that either of these is better than WebDAV ... Can anyone show me where I 'm being silly ?Interesting question !
I would do this by picking combinations , something like the following in python .
The hardest part is probably first pass verification , i.e .
if f ( 1,2,3 ) returns true , is that a correct result ?
Once you have verified that , then this is a good basis for regression testing .
Probably it 's a good idea to make a set of test cases that you know will be all true ( e.g .
3,4,5 for this triangle case ) , and a set of test cases that you know will be all false ( e.g .
0,1 , inf ) .
Then you can more easily verify the tests are correct .
# xpermutations from http : //code.activestate.com/recipes/190465 from xpermutations import * lengths= [ -1,0,1,5,10,0,1000 , 'inf ' ] for c in xselections ( lengths,3 ) : # or xuniqueselections print c ( -1 , -1 , -1 ) ; ( -1 , -1,0 ) ; ( -1 , -1,1 ) ; ( -1 , -1,5 ) ; ( -1 , -1,10 ) ; ( -1 , -1,0 ) ; ( -1 , -1,1000 ) ; ( -1 , -1 , inf ) ; ( -1,0 , -1 ) ; ( -1,0,0 ) ; ...If you have a company buying it , Toad from Quest Software has this kind of management functionality built in .
It 's basically a two-click operation to compare two schemas and generate a sync script from one to the other .
They have editions for most of the popular databases , including of course Sql Server .I work the same way Karl does , by keeping all of my SQL scripts for creating and altering tables in a text file that I keep in source control .
In fact , to avoid the problem of having to have a script examine the live database to determine what ALTERs to run , I usually work like this : On the first version , I place everything during testing into one SQL script , and treat all tables as a CREATE .
This means I end up dropping and readding tables a lot during testing , but that 's not a big deal early into the project ( since I 'm usually hacking the data I 'm using at that point anyway ) .
On all subsequent versions , I do two things : I make a new text file to hold the upgrade SQL scripts , that contain just the ALTERs for that version .
And I make the changes to the original , create a fresh database script as well .
This way an upgrade just runs the upgrade script , but if we have to recreate the DB we do n't need to run 100 scripts to get there .
Depending on how I 'm deploying the DB changes , I 'll also usually put a version table in the DB that holds the version of the DB .
Then , rather than make any human decisions about which scripts to run , whatever code I have running the create/upgrade scripts uses the version to determine what to run .
The one thing this will not do is help if part of what you 're moving from test to production is data , but if you want to manage structure and not pay for a nice , but expensive DB management package , is really not very difficult .
I 've also found it 's a pretty good way of keeping mental track of your DB .There are several ways to iterate over a result set , which way is the best ?There are three ways to iterate over a result set .
The best way in terms of both readability and performance is usually to use the built-in cursor iterator .
curs.execute ( 'select * from people ' ) for row in curs : print row You can fetch all the rows into a list , but this can have some bad side effects if the result set is large .
You have to wait for the entire result set to be returned to your client process .
You may eat up a lot of memory in your client to hold the built-up list .
It may take a while for Python to construct and deconstruct the list which you are going to immediately discard anyways .
for row in curs.fetchall ( ) : print row Finally , you can loop over the result set fetching one row at a time .
In general , there 's no particular advantage in doing this over using the iterator .
If there is something in your programming logic that seems to indicate there is an advantage in doing this , perhaps you should reconsider your programming logic .
row = curs.fetchone ( ) while row : print row row = curs.fetchone ( )I agree that scripting everything is the best way to go and is what I advocate at work .
You should script everything from DB and object creation to populating your lookup tables .
Anything you do in UI only wo n't translate ( especially for changes ... not so much for first deployments ) and will end up requiring a tools like what Redgate offers .Confirm that you have the correct email address before sending out emails .
If someone gives the wrong email address on sign-up , beat them over the head about it ASAP .
Always include clear `` how to unsubscribe '' information in EVERY email .
Do not require the user to login to unsubscribe , it should be a unique url for 1-click unsubscribe .
This will prevent people from marking your mails as spam because `` unsubscribing '' is too hard .The typical solution is to dump the database as necessary and backup those files .
Depending on your development platform , there may be opensource plugins available .
Rolling your own code to do it is usually fairly trivial .
Note : You may want to backup the database dump instead of putting it into version control .
The files can get huge fast in version control , and cause your entire source control system to become slow ( I 'm recalling a CVS horror story at the moment ) .This is a pretty good write-up of one guys experience of creating 30-day persistent sessions .
WARNING : blog post is from 2006 http : //grahamglass.blogs.com/main/2006/05/rails_sessionsr.htmlI 'm looking for a performant , reasonably robust RNG using no special hardware .
It can use mathematical methods ( Mersenne Twister , etc ) , it can `` collect entropy '' from the machine , whatever .
On Linux/etc we have a drand48 ( ) which generates 48 random bits .
I 'd like a similar function/class for C++ or C # which can generate more than 32 bits of randomness and which low-order bits are equally as random as high-order bits .
It does n't have to be cryptographically secure but it must not use or be based on the C-language rand ( ) or .NET System.Random .
Any source code , links to source , etc .
would be appreciated !
Failing that , what TYPE of RNG should I be looking for ?For C++ , Boost.Random is probably what you 're looking for .
It has support for MT ( among many other algorithms ) , and can collect entropy via the nondet_random class .
Check it out !
: - )The Gnu Scientific Library ( GSL ) has a pretty extensive set of RN generators , test harness , etc .
If you 're on linux , it 's probably already available on your system .Hmm , ok .
I 'd suggest code like this , then : typedef struct { int type ; int result ; } DialogData ; static gbooleandisplay_dialog ( gpointer user_data ) { DialogData *dialog_data = user_data ; GtkWidget *dialog ; if ( dialog_data- > type & MB_YESNO ) dialog = gtk_message_dialog_new ( ... ) ; else dialog = gtk_message_dialog_new ( ... ) ; // Set title , etc .
dialog_data- > result = gtk_dialog_run ( ... ) ; gtk_main_quit ( ) ; // Quits the main loop run in MessageBox ( ) return FALSE ; } int MessageBox ( ... ) { DialogData dialog_data ; dialog_data.type = type ; gtk_idle_add ( display_dialog , & dialog_data ) ; gtk_main ( ) ; // Do stuff based on dialog_data.result } The struct is because you need to pass around a couple pieces of data .
The gtk_idle_add ( ) call adds a method to be run when the main loop is running and idle , and the FALSE return value from the display_dialog ( ) call means that it 's only run once .
After we get the result from the dialog , we quit the main loop .
That 'll cause the gtk_main ( ) in your main MessageBox ( ) method to return , and you 'll be able to access the result from there .
Hope this helps !I 'll add to the excellent answers given above , and talk about boxing and unboxing , and how this applies to Java ( although C # has it too ) .
I 'll use just Java terminology , because I am more au fait with that .
As the answers mentioned , int is just a number ( called the unboxed type ) , whereas Integer is an object ( which contains the number , hence a boxed type ) .
In Java terms , that means ( apart from not being able to call methods on int ) , you can not store int or other non-object types in collections ( List , Map , etc . ) .
In order to store them , you must first box them up in its corresponding boxed type .
Java 5 onwards have something called auto-boxing and auto-unboxing which allow the boxing/unboxing to be done behind the scenes .
Compare and contrast : Java 5 version : Deque < Integer > queue ; void add ( int n ) { queue.add ( n ) ; } int remove ( ) { return queue.remove ( ) ; } Java 1.4 or earlier ( no generics either ) : Deque queue ; void add ( int n ) { queue.add ( Integer.valueOf ( n ) ) ; } int remove ( ) { return ( ( Integer ) queue.remove ( ) ) .intValue ( ) ; } It must be noted that despite the brevity in the Java 5 version , both versions generate identical bytecode .
Thus , although auto-boxing and auto-unboxing is very convenient because you write less code , these operations do happen behind the scenes , with the same runtime costs , so you still have to be aware of their existence .
Hope this helps !An MFC application that I 'm trying to migrate uses afxext.h , which causes _AFXDLL to get set , which causes this error if I set /MT : Please use the /MD switch for _AFXDLL builds My research to date indicates that it is impossible to build an application for execution on Windows NT 4.0 using Visual Studio ( C++ , in this case ) 2005 .
Is this really true ?
Are there any workaround available ?I 'm not a PHP dev but Google says that PHP 5+ can instantiate COM components .
If you can install Outlook on a box you could write a PHP web service around the COM component to handle the requests you need .
$ outlook = COM ( `` Outlook.Application '' ) Outlook API referanceIt 's been awhile since I used FCGI but I think a FCGI process could throw a SystemExit if the thread was taking too long .
This could be the web service not responding or even a slow DNS query .
Some google results show a similar error with Python and FCGI so moving to mongrel would be a good idea .
This post is my reference I used to setup mongrel and I still refer back to it .I want to print the first 10000 prime numbers .
Can anyone give me the most efficient code for this ?
Clarifications : It does not matter if your code is inefficient for n > 10000 .
The size of the code does not matter .
You can not just hard code the values in any manner .The Sieve of Atkin is probably what you 're looking for , its upper bound running time is O ( N/log log N ) .
If you only run the numbers 1 more and 1 less than the multiples of 6 , it could be even faster , as all prime numbers above 3 are 1 away from some multiple of six .
Resource for my statementIn Ruby 1.8 , there are subtle differences between proc/lambda on the one hand , and Proc.new on the other .
What are those differences ?
Can you give guidelines on how to decide which one to choose ?
In Ruby 1.9 , proc and lambda are different .
What 's the deal ?I found this page which shows what the difference between Proc.new and lambda are .
According to the page , the only difference is that a lambda is strict about the number of arguments it accepts , whereas Proc.new converts missing arguments to nil .
Here is an example IRB session illustrating the difference : irb ( main ) :001:0 > l = lambda { |x , y| x + y } = > # < Proc:0x00007fc605ec0748 @ ( irb ) :1 > irb ( main ) :002:0 > p = Proc.new { |x , y| x + y } = > # < Proc:0x00007fc605ea8698 @ ( irb ) :2 > irb ( main ) :003:0 > l.call `` hello '' , `` world '' = > `` helloworld '' irb ( main ) :004:0 > p.call `` hello '' , `` world '' = > `` helloworld '' irb ( main ) :005:0 > l.call `` hello '' ArgumentError : wrong number of arguments ( 1 for 2 ) from ( irb ) :1 from ( irb ) :5 : in `call' from ( irb ) :5 from :0 irb ( main ) :006:0 > p.call `` hello '' TypeError : ca n't convert nil into String from ( irb ) :2 : in `+' from ( irb ) :2 from ( irb ) :6 : in `call' from ( irb ) :6 from :0 The page also recommends using lambda unless you specifically want the error tolerant behavior .
I agree with this sentiment .
Using a lambda seems a tad more concise , and with such an insignificant difference , it seems the better choice in the average situation .
As for Ruby 1.9 , sorry , I have n't looked into 1.9 yet , but I do n't imagine they would change it all that much ( do n't take my word for it though , it seems you have heard of some changes , so I am probably wrong there ) .I have not used PHP to do this but have experience in using C # to achieve the same thing .
The Outlook API is a way of automating Outlook rather than connecting to Exchange directly .
I have previously taken this approach in a C # application and it does work although can be buggy .
If you wish to connect directly to the Exchange server you will need to research extended MAPI .
In the past I used this wrapper MAPIEx : Extended MAPI Wrapper .
It is a C # project but I believe you can use some .NET code on a PHP5 Windows server .
Alternatively it has a C++ core DLL that you may be a able to use .
I have found it to be very good and there are some good example applications .
Update : Sorry for the delay no current way to keep track of posts yet .
I do agree adding more layer on to your application and relying on 3rd party code can be scary ( and rightfully so . )
Today I read another interesting post tagged up as MAPI that is on a different subject .
The key thing here though is that it has linked to this important MS article .
I have been unaware of the issues until now on using managed code to interface to MAPI although the C++ code in the component should be unaffected by this error as it is unmanaged .
This blog entry also suggests other ways to connect to MAPI/Exchange server .
In this case due to these new facts http : //us3.php.net/imap may be the answer as suggested by the other user .I write a SOAP server that receives XML requests , and creates XML responses .
( Unfortunately , it 's not my project , so it 's closed source , but that 's another problem ) .
It turned out for me that creating ( SOAP ) XML documents is fairly simple , if you have a data structure that `` fits '' the schema .
I keep the envelope , since the response envelope is ( almost ) the same as the request envelope .
Then , since my data structure is a ( possibly nested ) dictionary , I create a string that turns this dictionary into < key > value < /key > items .
This is a task that recursion makes simple , and I end up with the right structure .
This is all done in python code , and is currently fast enough for production use .
You can also ( relatively ) easily build lists as well , although depending upon your client , you may hit problems unless you give length hints .
For me , this was much simpler , since a dictionary is a much easier way of working than some custom class .
For the books , generating XML is much easier than parsing !I have a database table and one of the fields ( not the primary key ) is having a unique index on it .
Now I want to swap values under this column for two rows .
How could this be done ?
Two hacks I know are : Delete both rows and re-insert them Update rows with some other value and swap and then update to actual value .
But I do n't want to go for these as they do not seem to be the appropriate solution to the problem .
Could anyone help me out ?I would like the version property of my application to be incremented for each build but I 'm not sure on how to enable this functionality in Visual Studio ( 2005/2008 ) .
I have tried to specify the AssemblyVersion as 1.0 .
* but it does n't get me exactly what I want .
I 'm also using a settings file and in earlier attempts when the assembly version changed my settings got reset to the default since the application looked for the settings file in another directory .
I would like to be able to display a version number in the form of 1.1.38 so when a user finds a problem I can log the version they are using as well as tell them to upgrade if they have an old release .
A short explanation of how the versioning works would also be appreciated .
When does the build and revision number get incremented ?I 've been having trouble getting my ASP.NET application to automatically log users into the Intranet site I 'm building .
No matter the googling or the experimentation I applied , there is always a login box displayed by IE7 .
I 've got Windows authentication mode set in the Web.config , disabled anonymous access and configured the correct default domain in IIS , but it 's still asking the user to log in and , more annoyingly , the user is required to provide the domain too ( DOMAIN\auser ) , which is causing problems with non-technical visitors .
Thank Zeus for password remembering functionality .
I 'm not the network administrator so it 's possible that something about Active Directory is set up incorrectly , or it could just be me missing something very simple .
Please note that I do n't want to impersonate the user , I just need to know that the IPrincipal.Name property matches that of a valid record in my user database , hence authenticating the user to my application .
To this end , it would be very useful to have a checklist of all configuration requirements for AD , ASP.NET and IIS to work together in this manner as a reference for debugging and hopefully reducing some user friction .With the `` Built in '' stuff , you ca n't , as using 1.0 .
* or 1.0.0 .
* will replace the revision and build numbers with a coded date/timestamp , which is usually also a good way .
For more info , see the Assembly Linker Documentation in the /v tag .
As for automatically incrementing numbers , use the AssemblyInfo Task : AssemblyInfo Task This can be configured to automatically increment the build number .
There are 2 Gotchas : Each of the 4 numbers in the Version string is limited to 65535 .
This is a Windows Limitation and unlikely to get fixed .
Why are build numbers limited to 65535 ?
Using with with Subversion requires a small change : Using MSBuild to generate assembly version info at build time ( including SubVersion fix ) Retrieving the Version number is then quite easy : @ code1 @ And , to clarify : In .net or at least in C # , the build is actually the THIRD number , not the fourth one as some people ( for example Delphi Developers who are used to Major.Minor.Release.Build ) might expect .
In .net , it 's Major.Minor.Build.Revision .What is the fastest , yet secure way to encrypt passwords in ( PHP preferred ) , and for which ever method you choose is it portable ?
In other words if I later migrate my website to a different server will my passwords continue to work ?
The method I am using now as I was told is dependent on the exact versions of the libraries installed on the server .Second the Buildbot - Trac integration .
You can find more information about the integration on the Buildbot website .
At my previous job , we wrote and used the plugin they mention ( tracbb ) .
What the plugin does is rewriting all of the Buildbot urls so you can use Buildbot from within Trac .
( http : //example.com/tracbb ) .
The really nice thing about Buildbot is that the configuration is written in Python .
You can integrate your own Python code directly to the configuration .
It 's also very easy to write your own BuildSteps to execute specific tasks .
We used BuildSteps to get the source from SVN , pull the dependencies , publish test results to WebDAV , etcetera .
I wrote an X10 interface so we could send signals with build results .
When the build failed , we switched on a red lava lamp .
When the build succeeded , a green lava lamp switched on .
Good times : - )I think you should go for solution 2 .
There is no 'swap ' function in any SQL variant I know of .
If you need to do this regularly , I suggest solution 1 , depending on how other parts of the software are using this data .
You can have locking issues if you 're not careful .
But in short : there is no other solution than the ones you provided .It depends on what kind of application you are building .
Create a representative test scenario , and start hammering away .
Then you will know the definitive answer .
Besides your use case , it also depends on CPU , memory , front-side bus , operating system , cache settings , etcetera .
Seriously , just test your own scenario .
If you need some numbers ( that actually may mean nothing in your scenario ) : Oracle Berkeley DB : Performance Metrics and Benchmarks Performance Metrics & Benchmarks : Berkeley DBIf you are choosing an encryption method for your login system then speed is not your friend , Jeff had a to-and-frow with Thomas Ptacek about passwords and the conclusion was that you should use the slowest , most secure encryption method you can afford to .
From Thomas Ptacek 's blog : Speed is exactly what you don’t want in a password hash function .
Modern password schemes are attacked with incremental password crackers .
Incremental crackers don’t precalculate all possible cracked passwords .
They consider each password hash individually , and they feed their dictionary through the password hash function the same way your PHP login page would .
Rainbow table crackers like Ophcrack use space to attack passwords ; incremental crackers like John the Ripper , Crack , and LC5 work with time : statistics and compute .
The password attack game is scored in time taken to crack password X .
With rainbow tables , that time depends on how big your table needs to be and how fast you can search it .
With incremental crackers , the time depends on how fast you can make the password hash function run .
The better you can optimize your password hash function , the faster your password hash function gets , the weaker your scheme is .
MD5 and SHA1 , even conventional block ciphers like DES , are designed to be fast .
MD5 , SHA1 , and DES are weak password hashes .
On modern CPUs , raw crypto building blocks like DES and MD5 can be bitsliced , vectorized , and parallelized to make password searches lightning fast .
Game-over FPGA implementations cost only hundreds of dollars .Perhaps this is a little academic and OT but hX tags are probably not the best choice for a tag cloud for reasons of document structure and all that sort of thing ... Maybe spans or an ol with appropriate class attributes ( plus some css ) ?I would use Javascript to submit the form .
The function would be triggered by the OnKeyPress event of the form element , and would detect whether the Enter key was selected .
If this is the case , it will submit the form .
Here are two pages that give techniques on how to do this : 1 , 2 .
Based on these , here is an example of usage ( based on here ) : < SCRIPT TYPE= '' text/javascript '' > < ! -- function submitenter ( myfield , e ) { var keycode ; if ( window.event ) { keycode = window.event.keyCode ; } else if ( e ) { keycode = e.which ; } else { return true ; } if ( keycode == 13 ) { myfield.form.submit ( ) ; return false ; } else { return true ; } } // -- > < /SCRIPT > < INPUT NAME= '' MyText '' TYPE= '' Text '' onKeyPress= '' return submitenter ( this , event ) '' / >The main problem with PHP sessions and security ( besides session hijacking ) comes with what environment you are in .
By default PHP stores the session data in a file in the OS 's temp directory .
Without any special thought or planning this is a world readable directory so all of your session information is public to anyone with access to the server .
As for maintaining sessions over multiple servers .
At that point it would be better to switch PHP to user handled sessions where it calls your provided functions to CRUD ( create , read , update , delete ) the session data .
At that point you could store the session information in a database or memcache like solution so that all application servers have access to the data .
Storing your own sessions may also be advantageous if you are on a shared server because it will let you store it in the database which you often times have more control over then the filesystem .I do n't remember whether I was dreaming or not but I seem to recall there being a function which allowed something like , @ code1 @ I 've looked over the docs but this kind of thing does n't fall under any obvious listed headersI also think that # 2 is the best bet , though I would be sure to wrap it in a transaction in case something goes wrong mid-update .
An alternative ( since you asked ) to updating the Unique Index values with different values would be to update all of the other values in the rows to that of the other row .
Doing this means that you could leave the Unique Index values alone , and in the end , you end up with the data that you want .
Be careful though , in case some other table references this table in a Foreign Key relationship , that all of the relationships in the DB remain intact .I 'm with Peter .
Developer do n't seem to understand passwords .
We all pick ( and I 'm guilty of this too ) MD5 or SHA1 because they are fast .
Thinking about it ( 'cuz someone recently pointed it out to me ) that does n't make any sense .
We should be picking a hashing algorithm that 's stupid slow .
I mean , on the scale of things , a busy site will hash passwords what ?
every 1/2 minute ?
Who cares if it take 0.8 seconds vs 0.03 seconds server wise ?
But that extra slowness is huge to prevent all types of common brute-forcish attacks .
From my reading , bcrypt is specifically designed for secure password hashing .
It 's based on blowfish , and there are many implementation .
For PHP , check out PHPPass http : //www.openwall.com/phpass/ For anyone doing .NET , check out BCrypt.NET http : //derekslager.com/blog/posts/2007/10/bcrypt-dotnet-strong-password-hashing-for-dotnet-and-mono.ashxI 've found that using the recipients real first and last name in the body is a sure fire way of getting through a spam filter .How often do you need to check for changes and how large ( in terms of row size ) are the tables in the database ?
If you use the CHECKSUM_AGG ( BINARY_CHECKSUM ( * ) ) method suggested by John , it will scan every row of the specified table .
The NOLOCK hint helps , but on a large database , you are still hitting every row .
You will also need to store the checksum for every row so that you tell one has changed .
Have you considered going at this from a different angle ?
If you do not want to modify the schema to add triggers , ( which makes a sense , it 's not your database ) , have you considered working with the application vendor that does make the database ?
They could implement an API that provides a mechanism for notifying accessory apps that data has changed .
It could be as simple as writing to a notification table that lists what table and which row were modified .
That could be implemented through triggers or application code .
From your side , ti would n't matter , your only concern would be scanning the notification table on a periodic basis .
The performance hit on the database would be far less than scanning every row for changes .
The hard part would be convincing the application vendor to implement this feature .
Since this can be handles entirely through SQL via triggers , you could do the bulk of the work for them by writing and testing the triggers and then bringing the code to the application vendor .
By having the vendor support the triggers , it prevent the situation where your adding a trigger inadvertently replaces a trigger supplied by the vendor .I 've got an upcoming project wherein I will need to connect our website ( PHP5/Apache 1.3/OpenBSD 4.1 ) to our back-end system running on a iSeries with OS400 V5R3 so that I can access some tables stored there .
I 've done some checking around but am running into some roadblocks .
From what I 've seen the DB2 extensions and DB2 software from IBM only run under Linux .
I 've tried compiling the extensions with all the software from IBM and even tried their precompiled ibm_db2 extension with no luck .
IBM only supports Linux so I turned on the Linux emulation in the kernel but that did n't seem to help anything .
If anyone has run across getting everything to run natively under OpenBSD that would be great , but what I think I may have to do is setting up a second server running CentOS with DB2 installed ( most likely via ZendCore for IBM since it seems to do all this for me ) and the driver so that I can set up a small transaction server that I can post against and get a JSON representation of the DB2 data that I need .
Does the second option seem overkill or does anyone else have any better ideas ?The following works for me in Firefox and Internet Explorer : @ code1 @No , you were not dreaming .
Python has a pretty excellent list comprehension system that lets you manipulate lists pretty elegantly , and depending on exactly what you want to accomplish , this can be done a couple of ways .
In essence , what you 're doing is saying `` For item in list if criteria.matches '' , and from that you can just iterate through the results or dump the results into a new list .
I 'm going to crib an example from Dive Into Python here , because it 's pretty elegant and they 're smarter than I am .
Here they 're getting a list of files in a directory , then filtering the list for all files that match a regular expression criteria .
@ code1 @ You could do this without regular expressions , for your example , for anything where your expression at the end returns true for a match .
There are other options like using the filter ( ) function , but if I were going to choose , I 'd go with this .
Eric SippleHave you looked at connecting to the server using unixODBC ?
If I remember correctly it has support for IBM DB2 and compiles on OpenBSD .
Check out http : //www.php.net/odbc for more information regarding the PHP side .
If you ca n't get that to work , the option to setup a web service on a Linux server may be all you can do .I was ( and still am ) looking for an embedded database to be used in a .net ( c # ) application .
The caveat : The Application ( or at least the database ) is stored on a Network drive , but only used by 1 user at a time .
Now , my first idea was SQL Server Compact edition .
That is really nicely integreated , but it can not run off a network .
Firebird seems to have the same issue , but the .net Integration seems to be not really first-class and is largely undocumented .
Blackfish SQL looks interesting , but there is no trial of the .net Version .
Pricing is also OK. Any other suggestions of something that works well with .net and runs off a network without the need of actually installing a server software ?SQLite came to my mind while reading your question , and I 'm quite sure that it 's possible to access it from a network drive if you keep yourself to the constraint of 1 user at a time .
SQLite on .NET - Get up and running in 3 minutesI 'm looking to introduce a unit testing framework into the mix at my job .
We 're using Visual Studio 2005 ( though we may be moving to 2008 within the next 6 months ) and work primarily in C # .
If the framework has some kind of IDE integration that would be best , but I 'm open to frameworks that do n't have integration but are still relatively simple to get set up .
I 'm going to get resistance to it one way or another , so if I can make sure what I 'm pushing is n't a pain in the neck , that would help my case .
The obvious choice from the research I 've done so far points to nUnit , but I 'd like to get the impressions of someone who 's actually used it before recommending it to my team .
Has anyone out there used nUnit ?
If so , are there any pitfalls or limitations of which I should be aware ?
Are there other good options out there ?
If so , if you 've used both nUnit at that , I 'd greatly appreciate an idea of the strengths and weaknesses of them .Check out VistaDB .
They have a very good product , the server version ( 3.4 ) is in Beta and is very close to release .Scott Hanselman had a good Podcast about this , entitled : '' The Past , Present and Future of .NET Unit Testing Frameworks '' : Hanselminutes # 112I think NUnit is your best bet .
With TestDriven.NET , you get great integration within VS.NET .
( Resharper also has a unit test runner if you 're using it ) .
NUnit it simple to use and follows an established paradigm .
You 'll also find plenty of projects/tutorials/guides using it which always helps .
Your other main choice is probably MBUnit , which is more and more position itself as the BDD framework of choice ( in conjunction with Gallio http : //www.gallio.org ) .I wrote a windows service using VB that read some legacy data from Visual Foxpro Databases to be inserted in SQL 2005 .
The problem is this use to run fine in Windows server 2003 32-Bits , but the client recently moved to Windows 2003 64-Bits and now the service wo n't work .
I 'm getting a message the the VFP .NET OLEdb provider is not found .
I researched and everything seems to point out that there is no solution .
Any Help , please ...Visual Studio 2008 has a built in test project type that works in a similar way to NUnit , but obviously has much tighter integration with Visual Studio ( can run on every build and shows the results in a similar way to the conversion results page when upgrading solution files ) , but it is obviously not as mature as NUnit as it 's pretty new and I 'm not sure about how it handles mocking .
But it would be worth looking into when your team moves to VS2008Another important but subtle difference is in the way procs created with lambda and procs created with Proc.new handle the return statement : In a lambda-created proc , the return statement returns only from the proc itself In a Proc.new-created proc , the return statement is a little more surprising : it returns control not just from the proc , but also from the method enclosing the proc !
Here 's lambda-created proc 's return in action .
It behaves in a way that you probably expect : def whowouldwin mylambda = lambda { return `` Freddy '' } mylambda.call # mylambda gets called and returns `` Freddy '' , and execution # continues on the next line return `` Jason '' endwhowouldwin= > `` Jason '' Now here 's a Proc.new-created proc 's return doing the same thing .
You 're about to see one of those cases where Ruby breaks the much-vaunted Principle of Least Surprise : def whowouldwin2 myproc = Proc.new { return `` Freddy '' } myproc.call # myproc gets called and returns `` Freddy '' , # but also returns control from whowhouldwin2 !
# The line below *never* gets executed .
return `` Jason '' endwhowouldwin2 = > `` Freddy '' Thanks to this surprising behaviour ( as well as less typing ) , I tend to favour using lambda over Proc.new when making procs .It sounds like you 've covered all the server-side bases -- maybe it 's a client issue ?
I assume your users have integrated authentication enabled in IE7 ?
( Tools - > Internet Options - > Advanced - > Security ) .
This is enabled by default .
Also , is your site correctly recognized by IE7 as being in the Local Intranet zone ?
The IE7 default is to allow automatic logon only in that zone , so users would be prompted if IE thinks your site is on the internet .
I believe using a hostname with a dot in it causes IE to place the site into the Internet zone .Why not use SQL Server 2005 Express edition ?
It really depends on what you mean by `` embedded '' - but you can redistribute SQLServer2005E with your applications and the user never has to know it 's there .
Embedding SQL Server Express in Applications Embedding SQL Server Express into Custom ApplicationsI think : # ! /bin/pythonbar in dict ( Foo ) Is what you are thinking of .
When trying to see if a certain key exists within a dictionary in python ( python 's version of a hash table ) there are two ways to check .
First is the has_key ( ) method attached to the dictionary and second is the example given above .
It will return a boolean value .
That should answer your question .
And now a little off topic to tie this in to the list comprehension answer previously given ( for a bit more clarity ) .
List Comprehensions construct a list from a basic for loop with modifiers .
As an example ( to clarify slightly ) , a way to use the in dict language construct in a _list comprehension_ : Say you have a two dimensional dictionary foo and you only want the second dimension dictionaries which contain the key bar .
A relatively straightforward way to do so would be to use a list comprehension with a conditional as follows : # ! /bin/pythonbaz = dict ( [ ( key , value ) for key , value in foo if bar in value ] ) Note the if bar in value at the end of the statement , this is a modifying clause which tells the list comprehension to only keep those key-value pairs which meet the conditional .
In this case baz is a new dictionary which contains only the dictionaries from foo which contain bar ( Hopefully I did n't miss anything in that code example ... you may have to take a look at the list comprehension documentation found in docs.python.org tutorials and at secnetix.de , both sites are good references if you have questions in the future .
) .It sounds like ADO/Access is perfect for your needs .
It 's baked into the MS stack , well seasoned , and multi-user .
You can programatically create a DB like so : Dim catalog as New ADOX.CatalogCatalog.Create ( `` Provider=Microsoft.Jet.OLEDB.4.0 ; Data Source=\\server\path\to\db.mdb '' ) You can then use standard ADO.NET methods to interact with the database .Django view points to a function , which can be a problem if you want to change only a bit of functionality .
Yes , I could have million keyword arguments and even more if statements in the function , but I was thinking more of an object oriented approach .
For example , I have a page that displays a user .
This page is very similar to page that displays a group , but it 's still not so similar to just use another data model .
Group also has members etc ... One way would be to point views to class methods and then extend that class .
Has anyone tried this approach or has any other idea ?Are you looking to get a list of objects that have a certain attribute ?
If so , a list comprehension is the right way to do this .
result = [ obj for obj in listOfObjs if hasattr ( obj , 'attributeName ' ) ]How do I format a string to title case ?Here is a simple static method to do this in C # : public static string ToTitleCaseInvariant ( string targetString ) { return System.Threading.Thread.CurrentThread.CurrentCulture.TextInfo.ToTitleCase ( targetString ) ; }In what language ?
In PHP it is : ucwords ( ) example : @ code1 @What I was thinking of can be achieved using list comprehensions , but I thought that there was a function that did this in a slightly neater way .
i.e .
'bar ' is a list of objects , all of which have the attribute 'id' The mythical functional way : foo = 12foo in iter_attr ( bar , 'id ' ) The list comprehension way : foo = 12foo in [ obj.id for obj in bar ] In retrospect the list comprehension way is pretty neat anyway .Without using a ready-made function , a super-simple low-level algorithm to convert a string to title case : convert first character to uppercase.for each character in string , if the previous character is whitespace , convert character to uppercase .
This asssumes the `` convert character to uppercase '' will do that correctly regardless of whether or not the character is case-sensitive ( e.g. , '+ ' ) .One may not always know the Type of an object at compile-time , but may need to create an instance of the Type .
How do you get a new object instance from a Type ?One implementation of this problem is to attempt to call the parameter-less constructor of the Type : public static object GetNewObject ( Type t ) { try { return t.GetConstructor ( new Type [ ] { } ) .Invoke ( new object [ ] { } ) ; } catch { return null ; } } Here is the same approach , contained in a generic method : public static T GetNewObject < T > ( ) { try { return ( T ) typeof ( T ) .GetConstructor ( new Type [ ] { } ) .Invoke ( new object [ ] { } ) ; } catch { return default ( T ) ; } }If the language you are using has a supported method/function then just use that ( as in the C # ToTitleCase method ) If it does not , then you will want to do something like the following : Read in the string Take the first word Capitalize the first letter of that word 1 Go forward and find the next word Go to 3 if not at the end of the string , otherwise exit 1 To capitalize it in , say , C - use the ascii codes to find the integer value of the char and subtract 32 from it .
There would need to be much more error checking in the code ( ensuring valid letters etc .
) , and the `` Capitalize '' function will need to impose some sort of `` title-case scheme '' on the letters to check for words that do not need to be capatilised ( 'and ' , 'but ' etc .
Here is a good scheme )The Activator class within the root System namespace is pretty powerful .
There are a lot of overloads for passing parameters to the constructor and such .
Check out the documentation at : http : //msdn.microsoft.com/en-us/library/system.activator.createinstance.aspx Here are some simple examples : @ code1 @No , there are many applications built with VS2005 that have to support Windows XP , 2000 , NT , the whole stack .
The issue is that ( by default ) VS2005 wants to use libraries/exports not present on NT .
See this thread for some background .
Then start limiting your dependencies via preprocessor macros , and avoiding APIs which are n't supported on NT .What is the best way to localise a date format descriptor ?
As anyone from a culture which does not use the mm/dd/yyyy format knows , it is annoying to have to enter dates in this format .
The .NET framework provides some very good localisation support , so it 's trivial to parse dates according to the users culture , but you often want to also display a helpful hint as to the format required ( especially to distinguish between yy and yyyy which is interchangeable in most cultures ) .
What is the best way to do this in a way that make sense to most users ( e.g .
dd/M/yyy is confusing because of the change in case and the switching between one and two letters ) .Here is my current method .
Any suggestions ?
Regex singleMToDoubleRegex = new Regex ( `` ( ? < ! m ) m ( ?
! m ) '' ) ; Regex singleDToDoubleRegex = new Regex ( `` ( ? < ! d ) d ( ?
! d ) '' ) ; CultureInfo currentCulture = CultureInfo.CurrentUICulture ; // If the culture is netural there is no date pattern to use , so use the default.if ( currentCulture.IsNeutralCulture ) { currentCulture = CultureInfo.InvariantCulture ; } // Massage the format into a more general user friendly form.string shortDatePattern = CultureInfo.CurrentUICulture.DateTimeFormat.ShortDatePattern.ToLower ( ) ; shortDatePattern = singleMToDoubleRegex.Replace ( shortDatePattern , `` mm '' ) ; shortDatePattern = singleDToDoubleRegex.Replace ( shortDatePattern , `` dd '' ) ;How about giving the format ( mm/dd/yyyy or dd/mm/yyyy ) followed by a printout of today 's date in the user 's culture .
MSDN has an article on formatting a DateTime for the person 's culture , using the CultureInfo object that might be helpful in doing this .
A combination of the format ( which most people are familiar with ) combined with the current date represented in that format should be enough of a clue to the person on how they should enter the date .
( Also include a calendar control for those who still cant figure it out ) .Sounds to me like you 're trying to combine things that should n't be combined .
If you need to do different processing in your view depending on if it 's a User or Group object you 're trying to look at then you should use two different view functions .
On the other hand there can be common idioms you 'd want to extract out of your object_detail type views ... perhaps you could use a decorator or just helper functions ?
-DanI can get Python to work with Postgresql but I can not get it to work with MySQL .
The main problem is that on the shared hosting account I have I do not have the ability to install things such as Django or PySQL , I generally fail when installing them on my computer so maybe it 's good I ca n't install on the host .
I found bpgsql really good because it does not require an install , it 's a single file that I can look at , read and then call the functions of .
Does anybody know of something like this for MySQL ?You can print from the command line using the following : rundll32.exe % WINDIR % \System32\mshtml.dll , PrintHTML `` % 1 '' Where % 1 is the file path of the html file to be printed .
If you do n't need to print from memory ( or can afford to write to the disk in a temp file ) you can use : using ( Process printProcess = new Process ( ) ) { string systemPath = Environment.GetFolderPath ( Environment.SpecialFolder.System ) ; printProcess.StartInfo.FileName = systemPath + @ '' \rundll32.exe '' ; printProcess.StartInfo.Arguments = systemPath + @ '' \mshtml.dll , PrintHTML `` '' '' + fileToPrint + @ '' '' '' '' ; printProcess.Start ( ) ; } N.B .
This only works on Windows 2000 and above I think .I need to programmatically solve a system of linear equations in C , Objective C , or ( if needed ) C++ .
Here 's an example of the equations : @ code1 @ From this , I 'd like to get the best approximation for a , b , and tx .Just use ISO-8601 .
It 's an international standard .
@ code1 @Here 's a Perl solution http : //daringfireball.net/2008/05/title_case Here 's a Ruby solution http : //frankschmitt.org/projects/title-case Here 's a Ruby one-liner solution : http : //snippets.dzone.com/posts/show/4702 @ code1 @ What the one-liner is doing is using a regular expression substitution of the first character of each word with the uppercase version of it .I have n't been able to find an understandable explanation of how to actually use Python 's itertools.groupby ( ) function .
What I 'm trying to do is this : Take a list - in this case , the children of an objectified lxml element Divide it into groups based on some criteria Then later iterate over each of these groups separately .
I 've reviewed the documentation , and the examples , but I 've had trouble trying to apply them beyond a simple list of numbers .
So , how do I use of itertools.groupby ( ) ?
Is there another technique I should be using ?
Pointers to good `` prerequisite '' reading would also be appreciated .Are you looking for a software package that 'll do the work or actually doing the matrix operations and such and do each step ?
The the first , a coworker of mine just used Ocaml GLPK .
It is just a wrapper for the GLPK , but it removes a lot of the steps of setting things up .
It looks like you 're going to have to stick with the GLPK , in C , though .
For the latter , thanks to delicious for saving an old article I used to learn LP awhile back , PDF .
If you need specific help setting up further , let us know and I 'm sure , me or someone will wander back in and help , but , I think it 's fairly straight forward from here .
Good Luck !I do n't have any experience with http : //www.SiteGround.com as a web host personally .
This is just a guess , but it 's common for a shared host to support Python and MySQL with the MySQLdb module ( e.g. , GoDaddy does this ) .
Try the following CGI script to see if MySQLdb is installed .
# ! /usr/bin/pythonmodule_name = 'MySQLdb'head = ' '' Content-Type : text/html % s is ' '' % module_nametry : __import__ ( module_name ) print head + 'installed'except ImportError : print head + 'not installed 'Cramer 's Rule and Gaussian Elimination are two good , general-purpose algorithms ( also see Simultaneous Linear Equations ) .
If you 're looking for code , check out GiNaC , Maxima , and SymbolicC++ ( depending on your licensing requirements , of course ) .
EDIT : I know you 're working in C land , but I also have to put in a good word for SymPy ( a computer algebra system in Python ) .
You can learn a lot from its algorithms ( if you can read a bit of python ) .
Also , it 's under the new BSD license , while most of the free math packages are GPL .Can you show us your code ?
The example on the Python docs is quite straightforward : @ code1 @ So in your case , data is a list of nodes , keyfunc is where the logic of your criteria function goes and then groupby ( ) groups the data .
You must be careful to sort the data by the criteria before you call groupby or it wo n't work .
groupby method actually just iterates through a list and whenever the key changes it creates a new group .What source control system are you using ?
Almost all of them have some form of $ Id $ tag that gets expanded when the file is checked in .
I usually use some form of hackery to display this as the version number .
The other alternative is use to use the date as the build number : 080803-1448Not efficient at all , but you can use a regular expression to test for prime numbers .
/^1 ? $ |^ ( 11+ ?
) \1+ $ / This tests if , for a string consisting of k “1”s , k is not prime ( i.e .
whether the string consists of one “1” or any number of “1”s that can be expressed as an n-ary product ) .Absolutely , especially dealing with lots of these permutations/combinations I can definitely see that the first pass would be an issue .
Interesting implementation in python , though I wrote a nice one in C and Ocaml based on `` Algorithm 515 '' ( see below ) .
He wrote his in Fortran as it was common back then for all the `` Algorithm XX '' papers , well , that assembly or c. I had to re-write it and make some small improvements to work with arrays not ranges of numbers .
This one does random access , I 'm still working on getting some nice implementations of the ones mentioned in Knuth 4th volume fascicle 2 .
I 'll an explanation of how this works to the reader .
Though if someone is curious , I would n't object to writing something up .
/** [ combination c n p x ] * get the [ x ] th lexicographically ordered set of [ p ] elements in [ n ] * output is in [ c ] , and should be sizeof ( int ) * [ p ] */void combination ( int* c , int n , int p , int x ) { int i , r , k = 0 ; for ( i=0 ; i < p-1 ; i++ ) { c [ i ] = ( i ! = 0 ) ?
c [ i-1 ] : 0 ; do { c [ i ] ++ ; r = choose ( n-c [ i ] , p- ( i+1 ) ) ; k = k + r ; } while ( k < x ) ; k = k - r ; } c [ p-1 ] = c [ p-2 ] + x - k ; } ~ '' Algorithm 515 : Generation of a Vector from the Lexicographical Index '' ; Buckles , B. P. , and Lybanon , M. ACM Transactions on Mathematical Software , Vol .
3 , No .
2 , June 1977 .I would be wary of automatically upcasing all whitespace-preceded-words in scenarios where I would run the risk of attracting the fury of nitpickers .
I would at least consider implementing a dictionary for exception cases like articles and conjunctions .
Behold : '' Beauty and the Beast '' And when it comes to proper nouns , the thing gets much uglier .The most popular ( ==standard ? )
way of determining the time zone I 've seen around is simply asking the users themselves .
If your website requires subscription , this could be saved in the users ' profile data .
For anon users , the dates could be displayed as UTC or GMT or some such .
I 'm not trying to be a smart aleck .
It 's just that sometimes some problems have finer solutions outside of any programming context .mbUnit is worth alook , it has a set of features comparable to NUnit , it has its own GUI , or can be integrated into VS if you have Resharper .
I would also recommend Rhino Mocks if you are doing any sort of TDD .Have you tried changing the target CPU to x86 instead of `` Any CPU '' in the advanced compiler options ?
I know that this solves some problems with other OLEDB providers by forcing the use of the 32-bit version .I uploaded it and got an internal error Premature end of script headers After much playing around , I found that if I had import cgiimport cgitb ; cgitb.enable ( ) import MySQLdb It would give me a much more useful answer and say that it was not installed , you can see it yourself - > http : //woarl.com/db.py Oddly enough , this would produce an error import MySQLdbimport cgiimport cgitb ; cgitb.enable ( ) I looked at some of the other files I had up there and it seems that library was one of the ones I had already tried .This is ASP classic , not .Net .
We have to get a way to SFTP into a server to upload and download a couple of files , kicked off by a user .
What have other people used to do SFTP in ASP classic ?
Not necessarily opposed to purchasing a control .I 'm trying to maintain a Setup Project in Visual Studio 2003 ( yes , it 's a legacy application ) .
The problem we have at the moment is that we need to write registry entries to HKCU for every user on the computer .
They need to be in the HKCU rather than HKLM because they are the default user settings , and they do change per user .
My feeling is that This is n't possible This is n't something the installer should be doing , but something the application should be doing ( after all what happens when a user profile is created after the install ? ) .
With that in mind , I still want to change as little as possible in the application , so my question is , is it possible to add registry entries for every user in a Visual Studio 2003 setup project ?
And , at the moment the project lists five registry root keys ( HKEY_CLASSES_ROOT , HKEY_CURRENT_USER , HKEY_LOCAL_MACHINE , HKEY_USERS , and User/Machine Hive ) .
I do n't really know anything about the Users root key , and have n't seen User/Machine Hive .
Can anyone enlighten me on what they are ?
Perhaps they could solve my problem above .The magic all seems to be in visitortime.getTimezoneOffset ( ) That 's cool , I did n't know about that .
Does it work in IE , etc ?
From there you should be able to use JS to ajax , set cookies , whatever .
I 'd probably go the cookie route myself .
You 'll need to allow the user to change it though .
We tried to use geolocation ( via maxmind ) to do this a while ago , and it was wrong reasonably often - enough to make it not worth doing , so we just let the user set it in their profile , and show a notice to users who have n't set theirs yet .I 'm partway to my solution with this entry on MSDN ( do n't know how I could n't find it before ) .
User/Machine Hive Subkeys and values entered under this hive will be installed under the HKEY_CURRENT_USER hive when a user chooses `` Just Me '' or the HKEY_USERS hive or when a user chooses `` Everyone '' during installation .
Registry EditorFirst : Yes , this is something that belongs in the Application for the exact reson you specified : What happens after new user profiles are created ?
Sure , if you 're using a domain it 's possible to have some stuff put in the registry on creation , but this is not really a use case .
The Application should check if there are seetings and use the default settings if not .
That being said , it IS possible to change other users Keys through the HKEY_USERS Hive .
I have no experience with the Visual Studio 2003 Setup Project , so here is a bit of ( totally unrelated ) VBScript code that might just give you an idea where to look : const HKEY_USERS = & H80000003strComputer = `` .
`` Set objReg=GetObject ( `` winmgmts : { impersonationLevel=impersonate } ! \\ '' & strComputer & `` \root\default : StdRegProv '' ) strKeyPath = `` '' objReg.EnumKey HKEY_USERS , strKeyPath , arrSubKeysstrKeyPath = `` \Software\Microsoft\Windows\CurrentVersion\WinTrust\Trust Providers\Software Publishing '' For Each subkey In arrSubKeys objReg.SetDWORDValue HKEY_USERS , subkey & strKeyPath , `` State '' , 146944Next ( Code Courtesy of Jeroen Ritmeijer )You have an ascending list of numbers , what is the most efficient algorithm you can think of to get the ascending list of sums of every two numbers in that list .
Duplicates in the resulting list are irrelevant , you can remove them or avoid them if you like .
To be clear , I 'm interested in the algorithm .
Feel free to post code in any language and paradigm that you like .I 'm guessing that because you want to set it for all users , that you 're on some kind of shared computer , which is probably running under a domain ?
HERE BE DRAGONS Let 's say Joe and Jane regularly log onto the computer , then they will each have 'registries ' .
You 'll then install your app , and the installer will employ giant hacks and disgusting things to set items under HKCU for them .
THEN , bob will come along and log on ( he , and 500 other people have accounts in the domain and so can do this ) .
He 's never used this computer before , so he has no registry .
The first time he logs in , windows creates him one , but he wo n't have your setting .
Your app then falls over or behaves incorrectly , and bob complains loudly about those crappy products from raynixon incorporated .
The correct answer is to just have some default settings in your app , which can write them to the registry if it does n't find them .
It 's general good practice that your app should never depend on the registry , and should create things as needed , for any registry entry , not just HKCU , anywaySQL : @ code1 @I have designed database tables ( normalised , on an MS SQL server ) and created a standalone windows front end for an application that will be used by a handful of users to add and edit information .
We will add a web interface to allow searching accross our production area at a later date .
I am concerned that if two users start editing the same record then the last to commit the update would be the 'winner ' and important information may be lost .
A number of solutions come to mind but I 'm not sure if I am going to create a bigger headache .
Do nothing and hope that two users are never going to be editing the same record at the same time .
- Might never happed but what if it does ?
Editing routine could store a copy of the original data as well as the updates and then compare when the user has finished editing .
If they differ show user and comfirm update - Would require two copies of data to be stored .
Add last updated DATETIME column and check it matches when we update , if not then show differences .
- requires new column in each of the relevant tables .
Create an editing table that registers when users start editing a record that will be checked and prevent other users from editing same record .
- would require carful thought of program flow to prevent deadlocks and records becoming locked if a user crashes out of the program .
Are there any better solutions or should I go for one of these ?If you are looking for a truly language agnostic solution then you will be sorely disappointed in my opinion because you 'll be stuck with a for loop and some conditionals .
However if you opened it up to functional languages or functional language features ( I 'm looking at you LINQ ) then my colleagues here can fill this page with elegant examples in Ruby , Lisp , Erlang , and others .I 'm trying to setup CruiseControl.net at the moment .
So far it works nice , but I have a Problem with the MSBuild Task .
According to the Documentation , it passes CCNetArtifactDirectory to MSBuild .
But how do I use it ?
I tried this : @ code1 @ But that does not work .
In fact , it kills the service with this error : ThoughtWorks.CruiseControl.Core.Config.Preprocessor.EvaluationException : Reference to unknown symbol CCNetArtifactDirectory Documentation is rather sparse , and google und mainly offers modifying the .sln Project file , which is what I want to avoid in order to be able to manually build this project later - I would really prefer /p : OutputPath .Looks like a web service is going to be the answer for me .
On a production box I 'd rather not have to go through compiling and maintaining my own special installation of PHP since ODBC support needs to be compiled in , according to the PHP documentation .A solution , albeit one that defers handling of the null value to the code , could be : DateTime yesterday = DateTime.Now.Date.AddDays ( -1 ) ; @ code1 @ This does not produce exactly the same SQL , but does provide the same logical result .
Translating `` complex '' SQL queries to LINQ is not always straightforward .If you expect infrequent collisions , Optimistic Concurrency is probably your best bet .
Scott Mitchell wrote a comprehensive tutorial on implementing that pattern : Implementing Optimistic ConcurrencyThe best I could come up with is to produce a matrix of sums of each pair , and then merge the rows together , a-la merge sort .
I feel like I 'm missing some simple insight that will reveal a much more efficient solution .
My algorithm , in Haskell : matrixOfSums list = [ [ a+b | b < - list , b > = a ] | a < - list ] sortedSums = foldl merge [ ] matrixOfSums -- A normal merge , save that we remove duplicatesmerge xs [ ] = xsmerge [ ] ys = ysmerge ( x : xs ) ( y : ys ) = case compare x y of LT - > x : ( merge xs ( y : ys ) ) EQ - > x : ( merge xs ( dropWhile ( ==x ) ys ) ) GT - > y : ( merge ( x : xs ) ys ) I found a minor improvement , one that 's more amenable to lazy stream-based coding .
Instead of merging the columns pair-wise , merge all of them at once .
The advantage being that you start getting elements of the list immediately .
-- wide-merge does a standard merge ( ala merge-sort ) across an arbitrary number of lists -- wideNubMerge does this while eliminating duplicateswideNubMerge : : Ord a = > [ [ a ] ] - > [ a ] wideNubMerge ls = wideNubMerge1 $ filter ( /= [ ] ) lswideNubMerge1 [ ] = [ ] wideNubMerge1 ls = mini : ( wideNubMerge rest ) where mini = minimum $ map head ls rest = map ( dropWhile ( == mini ) ) lsbetterSortedSums = wideNubMerge matrixOfSums However , if you know you 're going to use all of the sums , and there 's no advantage to getting some of them earlier , go with 'foldl merge [ ] ' , as it 's faster .Suppose I have the following CSS rule in my page : @ code1 @ How can I detect which one of the defined fonts was used in the user 's browser ?
Edit for people wondering why I want to do this : The font I 'm detecting contains glyph 's that are not available in other fonts and when the user does not have the font I want to display a link asking the user to download that font so they can use my web application with the correct font .
Currently I am displaying the download font link for all users , I want to only display this for people who do not have the correct font installed .I 've seen it done in a kind of iffy , but pretty reliable way .
Basically , an element is set to use a specific font and a string is set to that element .
If the font set for the element does not exist , it takes the font of the parent element .
So , what they do is measure the width of the rendered string .
If it matches what they expected for the desired font as opposed to the derived font , it 's present .
Here 's where it came from : Javascript/CSS Font Detector ( ajaxian.com ; 12 Mar 2007 )I ran across this : Javascript/CSS Font Detector which lets you detect what fonts are available based on the size of fonts .
From here you can tell what font was loaded onto the page by using the same technique .Is there an easy way to produce MSDN-style documentation from the Visual Studio XML output ?
I 'm not patient enough to set up a good xslt for it because I know I 'm not the first person to cross this bridge .
Also , I tried setting up sandcastle recently , but it really made my eyes cross .
Either I was missing something important in the process or it is just way too involved .
I know somebody out there has a really nice dead-simple solution .
I 'm reiterating here because I think my formatting made that paragraph non-inviting to read : I gave sandcastle a try but had a really hard time getting it set up .
What I really have in mind is something much simpler .
That is , unless I just do n't understand the sandcastle process .
It seemed like an awful lot of extra baggage to me just to produce something nice for the testers to work with .You 're looking for Sandcastle Project Page : Sandcastle Releases Blog : Sandcastle Blog NDoc Code Documentation Generator for .NET used to be the tool of choice , but support has all but stopped .Have a look at Sandcastle , which does exactly that .
It 's also one of the more simpler solutions out there , and it 's more or less the tool of choice , so in the long run , maybe we could help you to set up Sandcastle if you specify what issues you encountered during setup ?Despite what the MSDN article says about User/Machine Hive , it does n't write to HKEY_USERS .
Rather it writes to HKCU if you select Just Me and HKLM if you select Everyone .
So my solution is going to be to use the User/Machine Hive , and then in the application it checks if the registry entries are in HKCU and if not , copies them from HKLM .
I know this probably is n't the most ideal way of doing it , but it has the least amount of changes .I 've been using Subversion for a few years and after using SourceSafe , I just love Subversion .
Combined with TortoiseSVN , I ca n't really imagine how it could be any better .
Yet there 's a growing number of developers claiming that Subversion has problems and that we should be moving to the new breed of distributed version control systems , such as Git .
How does Git improve upon Subversion ?Google Tech Talk : Linus Torvalds on git http : //www.youtube.com/watch ? v=4XpnKHJAok8 The Git Wiki 's comparison page http : //git.or.cz/gitwiki/GitSvnComparsionGit is not better than Subversion .
But is also not worse .
It 's different .
The key difference is that it is decentralized .
Imagine you are a developer on the road , you develop on your laptop and you want to have source control so that you can go back 3 hours .
With Subversion , you have a Problem : The SVN Repository may be in a location you ca n't reach ( in your company , and you do n't have internet at the moment ) , you can not commit .
If you want to make a copy of your code , you have to literally copy/paste it .
With Git , you do not have this problem .
Your local copy is a repository , and you can commit to it and get all benefits of source control .
When you regain connectivity to the main repository , you can commit against it .
This looks good at first , but just keep in mind the added complexity to this approach .
Git seems to be the `` new , shiny , cool '' thing .
It 's by no means bad ( there is a reason Linus wrote it for the Linux Kernel development after all ) , but I feel that many people jump on the `` Distributed Source Control '' train just because it 's new and is written by Linus Torvalds , without actually knowing why/if it 's better .
Subversion has Problems , but so does Git , Mercurial , CVS , TFS or whatever .
Edit : So this answer is now a year old and still generates many upvotes , so I thought I 'll add some more explanations .
In the last year since writing this , Git has gained a lot of momentum and support , particularly since sites like GitHub really took off .
I 'm using both Git and Subversion nowadays and I 'd like to share some personal insight .
First of all , Git can be really confusing at first when working decentralized .
What is a remote ?
and How to properly set up the initial repository ?
are two questions that come up at the beginning , especially compared to SVN 's simple `` svnadmin create '' , Git 's `` git init '' can take the parameters -- bare and -- shared which seems to be the `` proper '' way to set up a centralized repository .
There are reasons for this , but it adds complexity .
The documentation of the `` checkout '' command is very confusing to people changing over - the `` proper '' way seems to be `` git clone '' , while `` git checkout '' seems to switch branches .
Git REALLY shines when you are decentralized .
I have a server at home and a Laptop on the road , and SVN simply does n't work well here .
With SVN , I ca n't have local source control if I 'm not connected to the repository ( Yes , I know about SVK or about ways to copy the repo ) .
With Git , that 's the default mode anyway .
It 's an extra command though ( git commit commits locally , whereas git push origin master pushes the master branch to the remote named `` origin '' ) .
As said above : Git adds complexity .
Two modes of creating repositories , checkout vs. clone , commit vs. push ... You have to know which commands work locally and which work with `` the server '' ( I 'm assuming most people still like a central `` master-repository '' ) .
Also , the tooling is still insufficient , at least on Windows .
Yes , there is a Visual Studio AddIn , but I still use git bash with msysgit .
SVN has the advantage that it 's MUCH simpler to learn : There is your repository , all changes to towards it , if you know how to create , commit and checkout and you 're ready to go and can pickup stuff like branching , update etc .
later on .
Git has the advantage that it 's MUCH better suited if some developers are not always connected to the master repository .
Also , it 's much faster than SVN .
And from what I hear , branching and merging support is a lot better ( which is to be expected , as these are the core reasons it was written ) .
This also explains why it gains so much buzz on the Internet , as Git is perfectly suited for Open Source projects : Just Fork it , commit your changes to your own Fork , and then ask the original project maintainer to pull your changes .
With Git , this just works .
Really , try it on Github , it 's magic .
What I also see are Git-SVN Bridges : The central repository is a Subversion repo , but developers locally work with Git and the bridge then pushes their changes to SVN .
But even with this lengthy addition , I still stand by my core message : Git is not better or worse , it 's just different .
If you have the need for `` Offline Source Control '' and the willingness to spend some extra time learning it , it 's fantastic .
But if you have a strictly centralized Source Control and/or are struggling to introduce Source Control in the first place because your co-workers are not interested , then the simplicity and excellent tooling ( at least on Windows ) of SVN shine .Well , it 's distributed .
Benchmarks indicate that it 's considerably faster ( given its distributed nature , operations like diffs and logs are all local so of course it 's blazingly faster in this case ) , and working folders are smaller ( which still blows my mind ) .
When you 're working on subversion , or any other client/server revision control system , you essentially create working copies on your machine by checking-out revisions .
This represents a snapshot in time of what the repository looks like .
You update your working copy via updates , and you update the repository via commits .
With a distributed version control , you do n't have a snapshot , but rather the entire codebase .
Wan na do a diff with a 3 month old version ?
No problem , the 3 month old version is still on your computer .
This does n't only mean things are way faster , but if you 're disconnected from your central server , you can still do many of the operations you 're used to .
In other words , you do n't just have a snapshot of a given revision , but the entire codebase .
You 'd think that Git would take up a bunch of space on your harddrive , but from a couple benchmarks I 've seen , it actually takes less .
Do n't ask me how .
I mean , it was built by Linus , he knows a thing or two about filesystems I guess .You 'll need to compile with the target CPU set to x86 to force your code to use the 32 bit version of the VFP OLE Db provider .
Microsoft has stated that there are no plans on releasing a 64-bit edition of the Visual FoxPro OLE Db provider .
For what 's worth , Microsoft has also stated that VFP 9 is the final version of Visual FoxPro and support will end in 2015 .
If you need the OLE DB provider for VFP 9 , you can get it here .Are PHP variables passed by value or by reference ?Depends on the version , 4 is by value , 5 is by reference .PHP variables are assigned by value , passed to functions by value , and when containing/representing objects are passed by reference .
You can force variables to pass by reference using an & Assigned by value/reference example : @ code1 @ would output `` var1 : test , var2 : final test , var3 : final test '' .
Passed by value/reference exampe : @ code2 @ would output : `` var1 : foo , var2 BAR '' .
Object variables passed by reference exampe : @ code3 @ Would output : `` FOO '' ( that last example could be better probably ... )Rather than coding this out , I figure I 'll pseudo-code it in steps and explain my logic , so that better programmers can poke holes in my logic if necessary .
On the first step we start out with a list of numbers length n. For each number we need to create a list of length n-1 becuase we are n't adding a number to itself .
By the end we have a list of about n sorted lists that was generated in O ( n^2 ) time .
@ code1 @ In step 2 because the lists were sorted by design ( add a number to each element in a sorted list and the list will still be sorted ) we can simply do a mergesort by merging each list together rather than mergesorting the whole lot .
In the end this should take O ( n^2 ) time .
@ code2 @ The merge method would be then the normal merge step with a check to make sure that there are no duplicate sums .
I wo n't write this out because anyone can look up mergesort .
So there 's my solution .
The entire algorithm is O ( n^2 ) time .
Feel free to point out any mistakes or improvements .It 's by value according to the PHP Documentation .
By default , function arguments are passed by value ( so that if the value of the argument within the function is changed , it does not get changed outside of the function ) .
To allow a function to modify its arguments , they must be passed by reference .
To have an argument to a function always passed by reference , prepend an ampersand ( & ) to the argument name in the function definition .
< ? phpfunction add_some_extra ( & $ string ) { $ string .= 'and something extra .
' ; } $ str = 'This is a string , ' ; add_some_extra ( $ str ) ; echo $ str ; // outputs 'This is a string , and something extra .
' ? >Git and DVCS in general is great for developers doing a lot of coding independently of each other because everyone has their own branch .
If you need a change from someone else , though , she has to commit to her local repo and then she must push that changeset to you or you must pull it from her .
My own reasoning also makes me think DVCS makes things harder for QA and release management if you do things like centralized releases .
Someone has to be responsible for doing that push/pull from everyone else 's repository , resolving any conflicts that would have been resolved at initial commit time before , then doing the build , and then having all the other developers re-sync their repos .
All of this can be addressed with human processes , of course ; DVCS just broke something that was fixed by centralized version control in order to provide some new conveniences .How do you debug PHP scripts ?
I am aware of basic debugging such as using the Error Reporting .
The breakpoint debugging in PHPEclipse is also quite useful .
What is the best ( in terms of fast and easy ) way to debug in phpStorm or any other IDE ?I 've used the Zend Studio ( 5.5 ) , together with Zend Platform .
That gives proper debugging , breakpoints/stepping over the code etc. , although at a price .I follow this article to setup an Eclipse environment that has debugging features like you mentioned .
The ability to step into the code is a much better way to debug then the old method of var_dump and print at various points to see where your flow goes wrong .
When all else fails though and all I have is SSH and vim I still var_dump ( ) /die ( ) to find where the code goes south .print_r ( debug_backtrace ( ) ) ; or something like that : - )It 's all about the ease of use/steps required to do something .
If I 'm developing a single project on my PC/laptop , git is better , because it is far easier to set up and use .
You do n't need a server , and you do n't need to keep typing repository URL 's in when you do merges .
If it were just 2 people , I 'd say git is also easier , because you can just push and pull from eachother .
Once you get beyond that though , I 'd go for subversion , because at that point you need to set up a 'dedicated ' server or location .
You can do this just as well with git as with SVN , but the benefits of git get outweighed by the need to do additional steps to synch with a central server .
In SVN you just commit .
In git you have to git commit , then git push .
The additional step gets annoying simply because you end up doing it so much .
SVN also has the benefit of better GUI tools , however the git ecosystem seems to be catching up quickly , so I would n't worry about this in the long term .If you have the ability to use WScript.Shell then you can just execute pscp.exe from the Putty package .
Obviously this is less then ideal but it will get the job done and let you use SCP/SFTP in classic ASP .I am by no means authoritative , but I believe the only supported path is from 6.5 to 7 .
Certainly that would be the most sane route , then I believe you can migrate from 7 directly to 2005 pretty painlessly .
As for scripting out all the objects - I would advise against it as you will inevitably miss something ( unless you database is truly trivial ) .http : //www.php.net/manual/en/migration5.oop.php In PHP 5 there is a new Object Model .
PHP 's handling of objects has been completely rewritten , allowing for better performance and more features .
In previous versions of PHP , objects were handled like primitive types ( for instance integers and strings ) .
The drawback of this method was that semantically the whole object was copied when a variable was assigned , or passed as a parameter to a method .
In the new approach , objects are referenced by handle , and not by value ( one can think of a handle as an object 's identifier ) .Variables containing primitive types are passed by value in PHP5 .
Variables containing objects are passed by reference .
There 's quite an interesting article from Linux Journal from 2006 which mentions this and other OO differences between 4 and 5. http : //www.linuxjournal.com/article/9170How have you implemented Internationalization ( i18n ) in actual projects you 've worked on ?
I took an interest in making software cross-cultural after I read the famous post by Joel , The Absolute Minimum Every Software Developer Absolutely , Positively Must Know About Unicode and Character Sets ( No Excuses ! ) .
However , I have yet to able to take advantage of this in a real project , besides making sure I used Unicode strings where possible .
But making all your strings Unicode and ensuring you understand what encoding everything you work with is in is just the tip of the i18n iceberg .
Everything I have worked on to date has been for use by a controlled set of US English speaking people , or i18n just was n't something we had time to work on before pushing the project live .
So I am looking for any tips or war stories people have about making software more localized in real world projects .Given a relatively simple CSS : @ code1 @ How do I make it so that the string stays constrained to the width of 150 , and simply wraps to a newline on the hyphen ?We 're upgrading an existing program from Win2k/SQL Server 2k to Windows 2003 and SQL Server 2005 as well as purchasing a new program that also uses 2k3/2k5 .
The vendor says that for us to host both databases we need to get the Enterprise version because the softwares clients use different collation for the connections and only Enterprise supports this .
I can not find anything on MS 's site to support this and , honestly , do n't want to pay the extra for Enterprise if the Standard edition works .
Am I missing some not talked about feature of SQL Server or is this , as I suspect , a vendor trying to upsell me ?I worked on a project for my previous employer that used .NET , and there was a built in .resx format we used .
We basically had a file that had all translations in the .resx file , and then multiple files with different translations .
The consequence of this is that you have to be very diligent about ensuring that all strings visible in the application are stored in the .resx , and anytime one is changed you have to update all languages you support .
If you get lazy and do n't notify the people in charge of translations , or you embed strings without going through your localization system , it will be a nightmare to try and fix it later .
Similarly , if localization is an afterthought , it will be very difficult to put in place .
Bottom line , if you do n't have all visible strings stored externally in a standard place , it will be very difficult to find all that need to be localized .
One other note , very strictly avoid concatenating visible strings directly , such as @ code1 @ Instead , you must use something like @ code2 @ The reason for this is that different languages often order the words differently , and concatenating strings directly will need a new build to fix , but if you used some kind of string replacement mechanism like above , you can modify your .resx file ( or whatever localization files you use ) for the specific language that needs to reorder the words .If you can find a professional or some other super-enterprise version of Visual Studio 6.0 - it came with a copy of MSDE ( Basically the predecessor to SQL Express ) .
I believe MSDE 2000 is still available as a free download from Microsoft , but I do n't know if you can migrate directly from 6.5 to 2000 .
I think in concept , you wo n't likely face any danger .
Years of practice however tell me that you will always miss some object , permission , or other database item that wo n't manifest itself immediately .
If you can script out the entire dump , the better as you will be less likely to miss something - and if you do miss something , it can be easily added to the script and fixed .
I would avoid any manual steps ( other than hitting the enter key once ) like the plague .With Git , you can do practically anything offline , because everybody has their own repository .
Making branches and merging between branches is really easy .
Even if you do n't have commit rights for a project , you can still have your own repository online , and publish `` push requests '' for your patches .
Everybody who likes your patches can pull them into their project , including the official maintainers .
It 's trivial to fork a project , modify it , and still keep merging in the bugfixes from the HEAD branch .
Git works for the Linux kernel developers .
That means it is really fast ( it has to be ) , and scales to thousands of contributors .
Git also uses less space ( up to 30 times less space for the Mozilla repository ) .
Git is very flexible , very TIMTOWTDI ( There is more than one way to do it ) .
You can use whatever workflow you want , and Git will support it .
Finally , there 's GitHub , a great site for hosting your Git repositories .
Drawbacks of Git : it 's much harder to learn , because Git has more concepts and more commands .
revisions do n't have version numbers like in subversion many Git commands are cryptic , and error messages are very user-unfriendly it lacks a good GUI ( such as the great TortoiseSVN )Replace your hyphens with this : & shy ; It 's called a `` soft '' hyphen .As part of CSS3 , it is not yet fully supported , but you can find information on word-wrapping here .
Another option is the wbr tag , & shy ; , and & # 8203 ; none of which are fully supported either .Some fun things : Having a PHP and MySQL Application that works well with German and French , but now needs to support Russian and Chinese .
I think I move this over to .net , as PHP 's Unicode support is - in my opinion - not really good .
Sure , juggling around with utf8_de/encode or the mbstring-functions is fun .
Almost as fun as having Freddy Krüger visit you at night ...
Realizing that some languages are a LOT more Verbose than others .
German is a LOT more verbose than English usually , and seeing how the German Version destroys the User Interface because too little space was allocated was not fun .
Some products gained some fame for their creative ways to work around that , with Oblivion 's `` Schw.Tr.d.Le.En.W . ''
being memorable : - ) Playing around with date formats , woohoo !
Yes , there ARE actually people in the world who use date formats where the day goes in the middle .
Sooooo much fun trying to find out what 07/02/2008 is supposed to mean , just because some users might believe it could be July 2 ...
But then again , you guys over the pond may believe the same about users who put the month in the middle : -P , especially because in English , July 2 sounds a lot better than 2nd of July , something that does not neccessarily apply to other languages ( i.e .
in German , you would never say Juli 2 but always Zweiter Juli ) .
I use 2008-02-07 whenever possible .
It 's clear that it means February 7 and it sorts properly , but dd/mm vs. mm/dd can be a really tricky problem .
Anoter fun thing , Number formats !
10.000,50 vs 10,000.50 vs. 10 000,50 vs. 10'000,50 ...
This is my biggest nightmare right now , having to support a multi-cultural environent but not having any way to reliably know what number format the user will use .
Formal or Informal .
In some language , there are two ways to address people , a formal way and a more informal way .
In English , you just say `` You '' , but in German you have to decide between the formal `` Sie '' and the informal `` Du '' , same for French Tu/Vous .
It 's usually a safe bet to choose the formal way , but this is easily overlooked .
Calendars .
In Europe , the first day of the Week is Monday , whereas in the US it 's Sunday .
Calendar Widgets are nice .
Showing a Calendar with Sunday on the left and Saturday on the right to a European user is not so nice , it confuses them .What 's the simplest way to connect and query a database for a set of records in C # ?@ Bernard : I have to admit , most of your example went straight over my head .
It does compile , and seems to work , though .
Is this safe for SMP systems or SpeedStep ?
That 's a good question ...
I think the code 's ok. From a practical standpoint , we use it in my company every day , and we run on a pretty wide array of boxes , everything from 2-8 cores .
Of course , YMMV , etc , but it seems to be a reliable and low-overhead ( because it does n't make a context switch into system-space ) method of timing .
Generally how it works is : declare the block of code to be assembler ( and volatile , so the optimizer will leave it alone ) .
execute the CPUID instruction .
In addition to getting some CPU information ( which we do n't do anything with ) it synchronizes the CPU 's execution buffer so that the timings are n't affected by out-of-order execution .
execute the rdtsc ( read timestamp ) execution .
This fetches the number of machine cycles executed since the processor was reset .
This is a 64-bit value , so with current CPU speeds it will wrap around every 194 years or so .
Interestingly , in the original Pentium reference , they note it wraps around every 5800 years or so .
the last couple of lines store the values from the registers into the variables hi and lo , and put that into the 64-bit return value .
Specific notes : out-of-order execution can cause incorrect results , so we execute the '' cpuid '' instruction which in addition to giving you some information about the cpu also synchronizes any out-of-order instruction execution .
Most OS 's synchronize the counters on the CPUs when they start , so the answer is good to within a couple of nano-seconds .
The hibernating comment is probably true , but in practice you probably do n't care about timings across hibernation boundaries .
regarding speedstep : Newer Intel CPUs compensate for the speed changes and returns an adjusted count .
I did a quick scan over some of the boxes on our network and found only one box that did n't have it : a Pentium 3 running some old database server .
( these are linux boxes , so I checked with : grep constant_tsc /proc/cpuinfo ) I 'm not sure about the AMD CPUs , we 're primarily an Intel shop , although I know some of our low-level systems gurus did an AMD evaluation .
Hope this satisfies your curiosity , it 's an interesting and ( IMHO ) under-studied area of programming .
You know when Jeff and Joel were talking about whether or not a programmer should know C ?
I was shouting at them , `` hey forget that high-level C stuff ... assembler is what you should learn if you want to know what the computer is doing ! ''Attempting to insert an escape character into a table results in a warning .
For example : @ code1 @ Produces the warning : @ code2 @ ( Using PSQL 8.2 ) Anyone know how to get around this ?Does doubling the \ work ?
insert into EscapeTest ( text ) values ( 'This will be inserted \\n This will not be ' ) ;Partially .
The text is inserted , but the warning is still generated .
I found a discussion that indicated the text needed to be preceded with 'E ' , as such : @ code1 @ This suppressed the warning , but the text was still not being returned correctly .
When I added the additional slash as Michael suggested , it worked .
As such : @ code2 @There is nothing I know of that can do this without you at least writing a little bit of code ... You will need 2 separate library : A CSV Parser Framework An XML Serialization Framework The CSV parser I would recommend ( unless you want to have a little bit of fun to write your own CSV Parser ) is OpenCSV ( A SourceForge Project for parsing CSV Data ) The XML Serialization Framework should be something that can scale in case you want to transform large ( or huge ) CSV file to XML : My recommendation is the Sun Java Streaming XML Parser Framework ( See here ) which allows pull-parsing AND serialization .Very roughly and from memory since I do n't have code on this laptop : @ code1 @All editions of SQL Server 2000/2005/2008 support having multiple databases , each using their own collation sequence .
You do n't need the Enterprise version .
When you have a database that uses a collation sequence that is different from default collation for the database server , you will need to take some extra precautions if you use temporary tables and/or table variables .
Temp tables/variables live in the tempdb database , which uses the collation seqyuence used by by the master databases .
Just remember to use `` COLLATE database_default '' when defining character fields in the temp tables/variables .
I blogged about that not too long ago , if you want some more details .Cool .
I also found the documentation regarding the E : http : //www.postgresql.org/docs/8.3/interactive/sql-syntax-lexical.html # SQL-SYNTAX-STRINGS PostgreSQL also accepts `` escape '' string constants , which are an extension to the SQL standard .
An escape string constant is specified by writing the letter E ( upper or lower case ) just before the opening single quote , e.g .
E'foo ' .
( When continuing an escape string constant across lines , write E only before the first opening quote . )
Within an escape string , a backslash character ( \ ) begins a C-like backslash escape sequence , in which the combination of backslash and following character ( s ) represents a special byte value .
\b is a backspace , \f is a form feed , \n is a newline , \r is a carriage return , \t is a tab .
Also supported are \digits , where digits represents an octal byte value , and \xhexdigits , where hexdigits represents a hexadecimal byte value .
( It is your responsibility that the byte sequences you create are valid characters in the server character set encoding . )
Any other character following a backslash is taken literally .
Thus , to include a backslash character , write two backslashes ( \\ ) .
Also , a single quote can be included in an escape string by writing \ ' , in addition to the normal way of '' .I 'm maintaining a .NET 1.1 application , and one of the things I 've been tasked with is making sure the user does n't see any unfriendly error notifications .
I 've added handlers to Application.ThreadException and AppDomain.CurrentDomain.UnhandledException , which do get called .
My problem is that the standard CLR error dialog is still displayed ( before the exception handler is called ) .
Jeff talks about this problem on his blog here and here .
But there 's no solution .
So what is the standard way in .NET 1.1 to handle uncaught exceptions and display a friendly dialog box ?
Edit : Jeff 's response was marked as the correct answer , because the link he provided has the most complete information on how to do what 's required .I have imported the kernel32 library so I have the createMutex function available but I am not quite sure of the various parameters and return values .
This is classic Visual Basic , not Visual Basic.NET but I can probably work with either language in the form of an answer .You can upgrade 6.5 to SQL Server 2000 .
You may have an easier time getting a hold of SQL Server or the 2000 version of the MSDE .
Microsoft has a page on going from 6.5 to 2000 .
Once you have the database in 2000 format , SQL Server 2005 will have no trouble upgrading it to the 2005 format .
If you do n't have SQL Server 2000 , you can download the MSDE 2000 version directly from Microsoft .@ Goyuix -- that 's excellent for something written from memory .
tested it here -- found the connection was n't opened .
Otherwise very nice .
using System.Data.OleDb ; ... using ( OleDbConnection conn = new OleDbConnection ( ) ) { conn.ConnectionString = `` Provider=sqloledb ; Data Source=yourServername\\yourInstance ; Initial Catalog=databaseName ; Integrated Security=SSPI ; '' ; using ( OleDbCommand cmd = new OleDbCommand ( ) ) { conn.Open ( ) ; cmd.Connection = conn ; cmd.CommandText = `` Select * from yourTable '' ; using ( OleDbDataReader dr = cmd.ExecuteReader ( ) ) { while ( dr.Read ( ) ) { Console.WriteLine ( dr [ `` columnName '' ] ) ; } } } }Encoding flash video is actually very easy with ffmpeg .
You can use one command to convert from just about any video format , ffmpeg is smart enough to figure the rest out , and it 'll use every processor on your machine .
Invoking it is easy : ffmpeg -i input.avi output.flv ffmpeg will guess at the bitrate you want , but if you 'd like to specify one , you can use the -b option , so -b 500000 is 500kbps for example .
There 's a ton of options of course , but I generally get good results without much tinkering .
This is a good place to start if you 're looking for more options : video options .
You do n't need a special web server to show flash video .
I 've done just fine by simply pushing .flv files up to a standard web server , and linking to them with a good swf player , like flowplayer .
WMVs are fine if you can be sure that all of your users will always use [ a recent , up to date version of ] Windows only , but even then , Flash is often a better fit for the web .
The player is even extremely skinnable and can be controlled with javascript .I 've read that it is possible to add a method to an existing object ( e.g .
not in the class definition ) in Python , I think this is called Monkey Patching ( or in some cases Duck Punching ) .
I understand that it 's not always a good decision to do so .
But , how might one do this ?
UPDATE 8/04/2008 00:21:01 EST : That looks like a good answer John Downey , I tried it but it appears that it ends up being not a true method .
Your example defines the new patch function with an argument of self , but if you write actual code that way , the now patched class method asks for an argument named self ( it does n't automagically recognize it as the object to which it is supposed to bind , which is what would happen if defined within the class definition ) , meaning you have to call class.patch ( obj ) instead of just class.patch ( ) if you want the same functionality as a true method .
It looks like Python is n't really treating it as a method , but more just as a variable which happens to be a function ( and as such is callable ) .
Is there any way to attach an actual method to a class ?
Oh , and Ryan , that is n't exactly what I was looking for ( it is n't builtin functionality ) , but it is quite cool nonetheless .The database will do this for you .
Look at `` select ... for update '' , which is designed just for this kind of thing .
It will give you a write lock on the selected rows , which you can then commit or roll back .In Python monkey patching generally works by overwriting a class or functions signature with your own .
Below is an example from the Zope Wiki : from SomeOtherProduct.SomeModule import SomeClassdef speak ( self ) : return `` ook ook eee eee eee !
`` SomeClass.speak = speak That code will overwrite/create a method called speak on the class .
In Jeff Atwood 's recent post on monkey patching .
He shows an example in C # 3.0 which is the current language I use for work .I do n't know Python syntax , but I know Ruby can do it , and it is rather trivial .
Let 's say you want to add a method to Array that prints the length to standard out : class Array def print_length puts length endend If you do n't want to modify the whole class , you can just add the method to a single instance of the array , and no other arrays will have the method : array = [ 1 , 2 , 3 ] def array.print_length puts lengthend Just be aware of the issues involved in using this feature .
Jeff Atwood actually wrote about it not too long ago .is this a console app or a winforms app ?
If it 's a .NET 1.1 console app this is , sadly , by design -- it 's confirmed by a MSFT dev in the second blog post you referenced : BTW , on my 1.1 machine the example from MSDN does have the expected output ; it 's just that the second line does n't show up until after you 've attached a debugger ( or not ) .
In v2 we 've flipped things around so that the UnhandledException event fires before the debugger attaches , which seems to be what most people expect .
Sounds like .NET 2.0 does this better ( thank goodness ) , but honestly , I never had time to go back and check .It 's a WinForms app .
The exceptions that are caught by Application.ThreadException work fine , and I do n't get the ugly .NET exception box ( OK to terminate , cancel to debug ?
who came up with that ? ? ) .
I was getting some exceptions that were n't being caught by that and ended up going to the AppDomain.UnhandledException event that were causing problems .
I think I 've caught most of those exceptions and I am displaying them in our nice error box now .
So I 'll just have to hope there are not some other circumstances that would cause exceptions to not be caught by the Application.ThreadException handler .Often while editing config files , I 'll open one with vi and then when I go to save it realize that I did n't type @ code1 @ Is there any way to give vi sudo privileges to save the file ?
I seem to recall seeing something about this while looking up some stuff about vi a while ago , but now I ca n't find it .A quick Google seems to give this advice : Do n't try to edit if it 's read-only .
You might be able to change the permissions on the file .
( Whether or not it will let you save is up to experimentation . )
If you still edited anyway , save to a temporary file and then move it .
http : //ubuntuforums.org/showthread.php ? t=782136I need to grab the base64-encoded representation of the ViewState .
Obviously this would not be available until fairly late in the request lifecycle , which is OK. For example , if the output of the page includes : @ code1 @ I need a way on the server side to get the value `` /wEPDwUJODU0Njc5MD ... == '' To clarify , I need this value when the page is being rendered , not on PostBack .
e.g .
I need to know the ViewState value that is being sent to the client , not the ViewState I 'm getting back from them .Ryan 's advice is generally good , however , if following step 3 , do n't move the temporary file ; it 'll have the wrong ownership and permissions .
Instead , sudoedit the correct file and read in the contents ( using : r or the like ) of the temporary file .
If following step 2 , use : w !
to force the file to be written .Oh , in WinForms you definitely should be able to get it to work .
The only thing you have to watch out for is things happening on different threads .
I have an old CodeProject article here which should help : http : //www.codeproject.com/KB/exception/ExceptionHandling.aspxThat 's definitely a good way to do it .
But you if you happen to be using a database that supports LINQ to SQL , it can be a lot more fun .
It can look something like this : MyDB db = new MyDB ( `` Data Source= ... '' ) ; var q = from db.MyTable select c ; foreach ( var c in q ) Console.WriteLine ( c.MyField.ToString ( ) ) ;The CCNetArtifactDirectory is passed to the MSBuild by default , so you dont need to worry about it .
MSBuild will place the build output in the `` bin location '' relevant to the working directory that you have specified .
@ code1 @ So in the above example your build output will be put in C : \data\projects\FooSolution [ ProjectName ] \bin\Debug .
Should you want to output to a different location you may want to look at of the tag in CCNET .
@ code2 @ This will allow you to publish your output to a different location .After upgrading a rails 1.2 website to 2.1 , the ExceptionNotifier plugin no longer works , complaining about this error : @ code1 @ What causes it and how do I fix it ?Well , based on the documentation it looks like : Security attributes ( can pass null ) Whether it 's initially owned ( can pass false ) The name of it HTHThis was caused by a change in rails 2.1 which prevents rails from loading views from any arbitrary path for security reasons .
There is now an updated version of the plugin on github , so the solution is to use that .
The old solution here for posterity To work around it , edit init.rb under your vendor/plugins/exception_notification directory , and add the following code to the end @ code1 @ This adds the ExceptionNotifier plugins ' views folder to the list , so it is allowed to load them .The VB code looks something like this : @ code1 @ The first parameter is a pointer to an SECURITY_ATTRIBUTES structure .
If you do n't know what it is , you do n't need it .
Pass NULL ( 0 ) .
The second parameter is TRUE ( non-zero , or 1 ) if the calling thread should take ownership of the mutex .
FALSE otherwise .
The third parameter is the mutex name and may be NULL ( 0 ) , as shown .
If you need a named mutex , pass the name ( anything unique ) in .
Not sure whether the VB wrapper marshals the length-prefixed VB string type ( BSTR ) over to a null-terminated Ascii/Unicode string if not , you 'll need to do that and numerous examples are out there .
Good luck !Does anyone know of a simple way to retrieve the country for a given IP Address ?
Preferably in ISO_3166-1 format ?Here 's a nice free service with a public API : http : //www.hostip.info/use.htmlWhat is the best way to display Flash content in a C # WinForms application ?
I would like to create a user control ( similar to the current PictureBox ) that will be able to display images and flash content .
It would be great to be able to load the flash content from a stream of sorts rather than a file on disk .While I have n't used a flash object inside a windows form application myself , I do know that it 's possible .
In Visual studio on your toolbox , choose to add a new component .
Then in the new window that appears choose the `` COM Components '' tab to get a list in which you can find the `` Shockwave Flash Object '' Once added to the toolbox , simply use the control as you would use any other `` standard '' control from visual studio .
three simple commands are available to interact with the control : AxShockwaveFlash1.Stop ( ) AxShockwaveFlash1.Movie = FilePath & '' \FileName.swf '' AxShockwaveFlash1.Play ( ) which , I think , are all self explanatory .
It would be great to be able to load the flash content from a stream of sorts rather than a file on disk .
I just saw you are also looking for a means to load the content from a stream , and because I 'm not really sure that is possible with the shockwave flash object I will give you another option ( two actually ) .
the first is the one I would advise you to use only when necessary , as it uses the full blown `` webbrowser component '' ( also available as an extra toolbox item ) , which is like trying to shoot a fly with a bazooka .
of course it will work , as the control will act as a real browser window ( actually the internet explorer browser ) , but its not really meant to be used in the way you need it .
the second option is to use something I just discovered while looking for more information about playing flash content inside a windows form .
F-IN-BOX is a commercial solution that will also play content from a given website URL .
( The link provided will direct you to the .NET code you have to use ) .I 'm looking for a way to delete a file which is locked by another process using C # .
I suspect the method must be able to find which process is locking the file ( perhaps by tracking the handles , although I 'm not sure how to do this in C # ) then close that process before being able to complete the file delete using File.Delete ( ) .I 'm looking for a robust , easy-to-use , regular expression evaluator for native C++ , either platform-independent or Windows-specific .
Requirements : Ca n't use Boost or ACE regex libraries ( unfortunately ) Ca n't use .NET Regex ( or any managed code ) The main requirement is that it should be standalone and open .For instance , my query is like the following using SQL Server 2005 : @ code1 @ I have a full text index defined to use the column SearchField which returns results when using : @ code2 @ I believe # is a special letter , so how do I allow FREETEXT to work correctly for the query above ?You can use this program , Handle , to find which process has the lock on your file .
It 's a command-line tool , so I guess you use the output from that ...
I 'm not sure about finding it programmatically .
If deleting the file can wait , you could specify it for deletion when your computer next starts up : Start REGEDT32 ( W2K ) or REGEDIT ( WXP ) and navigate to : @ code1 @ W2K and WXP W2K : EditAdd Value ... Data Type : REG_MULTI_SZValue Name : PendingFileRenameOperationsOK WXP : EditNewMulti-String Valueenter PendingFileRenameOperations In the Data area , enter `` \ ?
? \ '' + filename to be deleted .
LFNs may be entered without being embedded in quotes .
To delete C : \Long Directory Name\Long File Name.exe , enter the following data : @ code2 @ Then press OK .
The `` destination file name '' is a null ( zero ) string .
It is entered as follows : W2K : EditBinaryselect Data Format : Hexclick at the end of the hex stringenter 0000 ( four zeros ) OK WXP : Right-click the valuechoose `` Modify Binary Data '' click at the end of the hex stringenter 0000 ( four zeros ) OK Close REGEDT32/REGEDIT and reboot to delete the file .
( Shamelessly stolen from some random forum , for posterity 's sake . )Killing other processes is not a healthy thing to do .
If your scenario involves something like uninstallation , you could use the MoveFileEx API function to mark the file for deletion upon next reboot .
If it appears that you really need to delete a file in use by another process , I 'd recommend re-considering the actual problem before considering any solutions .Quoting a much-replicated help page about Indexing Service query language : To use specially treated characters such as & , | , ^ , # , @ , $ , ( , ) , in a query , enclose your query in quotation marks ( “ ) .
As far as I know , full text search in MSSQL is also done by the Indexing Service , so this might help .If you want to do it programatically .
I 'm not sure ... and I 'd really recommend against it .
If you 're just troubleshooting stuff on your own machine , SysInternals Process Explorer can help you Run it , use the Find Handle command ( I think it 's either in the find or handle menu ) , and search for the name of your file .
Once the handle ( s ) is found , you can forcibly close them .
You can then delete the file and so on .
Beware , doing this may cause the program which owns the handles to behave strangely , as you 've just pulled the proverbial rug out from under it , but it works well when you are debugging your own errant code , or when visual studio / windows explorer is being crap and not releasing file handles even though you told them to close the file ages ago ... sigh : - )Oh , one big hack I employed years ago , is that Windows wo n't let you delete files , but it does let you move them .
Pseudo-sort-of-code : @ code1 @ When the applications restarted ( note we did n't need to reboot the machine ) , they loaded the new mfc42.dll , and all was well .
That , coupled with PendingFileOperations to delete the old one the next time the whole system restarted , worked pretty well .The typical method is as follows .
You 've said you want to do this in C # so here goes ...
If you do n't know which process has the file locked , you 'll need to examine each process 's handle list , and query each handle to determine if it identifies the locked file .
Doing this in C # will likely require P/Invoke or an intermediary C++/CLI to call the native APIs you 'll need .
Once you 've figured out which process ( es ) have the file locked , you 'll need to safely inject a small native DLL into the process ( you can also inject a managed DLL , but this is messier , as you then have to start or attach to the .NET runtime ) .
That bootstrap DLL then closes the handle using CloseHandle etc .
Essentially : the way to unlock a `` locked '' file is to inject a DLL into the offending process 's address space and close it yourself .
You can do this using native or managed code .
No matter what , you 're going to need a small amount of native code or at least P/Invoke into the same .
Helpful links : http : //www.codeproject.com/KB/threads/winspy.aspx http : //damianblog.com/2008/07/02/net-code-injection/ Good luck !Most mathematicians agree that : eπi + 1 = 0 However , most floating point implementations disagree .
How well can we settle this dispute ?
I 'm keen to hear about different languages and implementations , and various methods to make the result as close to zero as possible .
Be creative !Here 's a short list of implementations and languages I 've tried .
It 's sorted by closeness to zero : Scheme : ( + 1 ( make-polar 1 ( atan 0 -1 ) ) ) ⇒ 0.0+1.2246063538223773e-16i ( Chez Scheme , MIT Scheme ) ⇒ 0.0+1.22460635382238e-16i ( Guile ) ⇒ 0.0+1.22464679914735e-16i ( Chicken with numbers egg ) ⇒ 0.0+1.2246467991473532e-16i ( MzScheme , SISC , Gauche , Gambit ) ⇒ 0.0+1.2246467991473533e-16i ( SCM ) Common Lisp : ( 1+ ( exp ( complex 0 pi ) ) ) ⇒ # C ( 0.0L0 -5.0165576136843360246L-20 ) ( CLISP ) ⇒ # C ( 0.0d0 1.2246063538223773d-16 ) ( CMUCL ) ⇒ # C ( 0.0d0 1.2246467991473532d-16 ) ( SBCL ) Perl : use Math : :Complex ; Math : :Complex- > emake ( 1 , pi ) + 1 ⇒ 1.22464679914735e-16i Python : from cmath import exp , pi ; exp ( complex ( 0 , pi ) ) + 1 ⇒ 1.2246467991473532e-16j ( CPython ) Ruby : require 'complex ' ; Complex : :polar ( 1 , Math : :PI ) + 1 ⇒ Complex ( 0.0 , 1.22464679914735e-16 ) ( MRI ) ⇒ Complex ( 0.0 , 1.2246467991473532e-16 ) ( JRuby ) R : complex ( argument = pi ) + 1 ⇒ 0+1.224606353822377e-16itry libpcre If you 're stuck on windows they have a windows port which should work .
I know e-texteditor uses it , so at least that 's proof it works : - )A lot of people ( including my company ) seem to use MaxMind GeoIP .
They have a free version GeoLite which is not as accurate as the paid version , but if you 're just after something simple , it may be good enough .Is it possible to settle this dispute ?
My first thought is to look to a symbolic language , like Maple .
I do n't think that counts as floating point though .
In fact , how does one represent i ( or j for the engineers ) in a conventional programming language ?
Perhaps a better example is sin ( π ) = 0 ?
( Or have I missed the point again ? )The # char is indexed as punctuation and therefore ignored , so it looks like we 'll remove the letter C from our word indexing ignore lists .
Tested it locally after doing that and rebuilding the indexes and I get results !
Looking at using a different word breaker language on the indexed column , so that those special characters are n't ignored .
EDIT : I also found this information : c # is indexed as c ( if c is not in your noise word list , see more on noise word lists later ) , but C # is indexed as C # ( in SQL 2005 and SQL 2000 running on Win2003 regardless if C or c is in your noise word list ) .
It is not only C # that is stored as C # , but any capital letter followed by # .
Conversely , c++ ( and any other lower-cased letter followed by a ++ ) is indexed as c ( regardless of whether c is in your noise word list ) .The GNU C Library supports regular expressions .
It 's open , and the RE code seems to be easily extractable .I 'd like to display ~100 floating cubes using DirectX or OpenGL .
I 'm looking for either some sample source code , or a description of the technique .
I know this kind of thing is easy for you accomplished '3D ' gurus out there but I have enough trouble getting even one cube to display correctly .
I 've combed the net for a good series of tutorials and although they talk about how to do 3D primitives , what I ca n't find is information on how to do large numbers of 3D primitives - cubes , spheres , pyramids , and so forth .In general , you ca n't change the effective user id of the vi process , but you can do this : : w ! sudo tee myfileSven , you reached the same conclusion as I did : I found the Shockwave Flash Object , all be it from a slightly different route , but was stumped on how to load the files from somewhere other than file on disk/URL .
The F-IN-BOX , although just a wrapper of the Shockwave Flash Object seems to provide much more functionality , which may just help me !
Shooting flys with bazookas may be fun , but an embeded web brower is not the path that I am looking for .
: ) There was a link on Adobe 's site that talked about `` Embedding and Communicating with the Macromedia Flash Player in C # Windows Applications '' but they seem to have removed it : (I 'm working on a multithreaded C++ application that is corrupting the heap .
The usual tools to locate this corruption seem to be inapplicable .
Old builds ( 18 months old ) of the source code exhibit the same behaviour as the most recent release , so this has been around for a long time and just was n't noticed ; on the downside , source deltas ca n't be used to identify when the bug was introduced - there are a lot of code changes in the repository .
The prompt for crashing behaviuor is to generate throughput in this system - socket transfer of data which is munged into an internal representation .
I have a set of test data that will periodically cause the app to exception ( various places , various causes - including heap alloc failing , thus : heap corruption ) .
The behaviour seems related to CPU power or memory bandwidth ; the more of each the machine has , the easier it is to crash .
Disabling a hyper-threading core or a dual-core core reduces the rate of ( but does not eliminate ) corruption .
This suggests a timing related issue .
Now here 's the rub : When it 's run under a lightweight debug environment ( say Visual Studio 98 / AKA MSVC6 ) the heap corruption is reasonably easy to reproduce - ten or fifteen minutes pass before something fails horrendously and exceptions , like an alloc ; when running under a sophisticated debug environment ( Rational Purify , VS2008/MSVC9 or even Microsoft Application Verifier ) the system becomes memory-speed bound and does n't crash ( Memory-bound : CPU is not getting above 50 % , disk light is not on , the program 's going as fast it can , box consuming 1.3G of 2G of RAM ) .
So , I 've got a choice between being able to reproduce the problem ( but not identify the cause ) or being able to idenify the cause or a problem I ca n't reproduce .
My current best guesses as to where to next is : Get an insanely grunty box ( to replace the current dev box : 2Gb RAM in an E6550 Core2 Duo ) ; this will make it possible to repro the crash causing mis-behaviour when running under a powerful debug environment ; or Rewrite operators new and delete to use VirtualAlloc and VirtualProtect to mark memory as read-only as soon as it 's done with .
Run under MSVC6 and have the OS catch the bad-guy who 's writing to freed memory .
Yes , this is a sign of desperation : who the hell rewrites new and delete ? !
I wonder if this is going to make it as slow as under Purify et al .
And , no : Shipping with Purify instrumentation built in is not an option .
A colleague just walked past and asked `` Stack Overflow ?
Are we getting stack overflows now ? ! ? ''
And now , the question : How do I locate the heap corruptor ?
Update : balancing new [ ] and delete [ ] seems to have gotten a long way towards solving the problem .
Instead of 15mins , the app now goes about two hours before crashing .
Not there yet .
Any further suggestions ?
The heap corruption persists .
Update : a release build under Visual Studio 2008 seems dramatically better ; current suspicion rests on the STL implementation that ships with VS98 .
Reproduce the problem .
Dr Watson will produce a dump that might be helpful in further analysis .
I 'll take a note of that , but I 'm concerned that Dr Watson will only be tripped up after the fact , not when the heap is getting stomped on .
Another try might be using WinDebug as a debugging tool which is quite powerful being at the same time also lightweight .
Got that going at the moment , again : not much help until something goes wrong .
I want to catch the vandal in the act .
Maybe these tools will allow you at least to narrow the problem to certain component .
I do n't hold much hope , but desperate times call for ... And are you sure that all the components of the project have correct runtime library settings ( C/C++ tab , Code Generation category in VS 6.0 project settings ) ?
No I 'm not , and I 'll spend a couple of hours tomorrow going through the workspace ( 58 projects in it ) and checking they 're all compiling and linking with the appropriate flags .
Update : This took 30 seconds .
Select all projects in the Settings dialog , unselect until you find the project ( s ) that do n't have the right settings ( they all had the right settings ) .What is the correct way to get the process size on Solaris , HP-UX and AIX ?
Should we use top or ps -o vsz or something else ?What I did when working against a database of unknown performance was to measure turnaround time on my queries .
I kept upping the thread count until turn-around time dropped , and dropping the thread count until turn-around time improved ( well , it was processes in my environment , but whatever ) .
There were moving averages and all sorts of metrics involved , but the take-away lesson was : just adapt to how things are working at the moment .
You never know when the DBAs will improve performance or hardware will be upgraded , or perhaps another process will come along to load down the system while you 're running .
So adapt .
Oh , and another thing : avoid process switches if you can - batch things up .
Oh , I should make this clear : this all happened at run time , not during development .You tried old builds , but is there a reason you ca n't keep going further back in the repository history and seeing exactly when the bug was introduced ?
Otherwise , I would suggest adding simple logging of some kind to help track down the problem , though I am at a loss of what specifically you might want to log .
If you can find out what exactly CAN cause this problem , via google and documentation of the exceptions you are getting , maybe that will give further insight on what to look for in the code .My first choice would be a dedicated heap tool such as pageheap.exe .
Rewriting new and delete might be useful , but that does n't catch the allocs committed by lower-level code .
If this is what you want , better to Detour the low-level alloc APIs using Microsoft Detours .
Also sanity checks such as : verify your run-time libraries match ( release vs. debug , multi-threaded vs. single-threaded , dll vs. static lib ) , look for bad deletes ( eg , delete where delete [ ] should have been used ) , make sure you 're not mixing and matching your allocs .
Also try selectively turning off threads and see when/if the problem goes away .
What does the call stack etc look like at the time of the first exception ?Rex , I suspect a good place to start looking is solutions that compress the ViewState -- they 're grabbing ViewState on the server before it 's sent down to the client and gzipping it .
That 's exactly where you want to be .
Scott Hanselman on ViewState Compression ( 2005 ) ViewState Compression with System.IO.Compression ( 2007 )The trouble with international standards is that pretty much noone uses them .
I try where I can , but I am forced to use dd/mm/yyyy almost everywhere in real life , which means I am so used to it it 's always a conscious process to use ISO-8601 .
For the majority of people who do n't even try to use ISO-8601 it 's even worse .
If you can internationalize where you can , I think it 's a great advantage .Yes , you are right to lock at the VSZ .
`` ps u '' will give you the VSZ and RSS , which are the virtual memory size and resident set size .
The RSS is how much physical memory has been allocated to the process , and the VSZ is the virtual memory size of the process .
If you have several copies of a program running , a lot of the memory in the VSZ will be shared between those processes .I was wondering if there is any good and clean oo implementation of bayesian filtering for spam and text classification ?
For learning purposes .My first action would be as follows : Build the binaries in `` Release '' version but creating debug info file ( you will find this possibility in project settings ) .
Use Dr Watson as a defualt debugger ( DrWtsn32 -I ) on a machine on which you want to reproduce the problem .
Repdroduce the problem .
Dr Watson will produce a dump that might be helpful in further analysis .
Another try might be using WinDebug as a debugging tool which is quite powerful being at the same time also lightweight .
Maybe these tools will allow you at least to narrow the problem to certain component .
And are you sure that all the components of the project have correct runtime library settings ( C/C++ tab , Code Generation category in VS 6.0 project settings ) ?Maybe https : //ci-bayes.dev.java.net/ or http : //www.cs.cmu.edu/~javabayes/Home/node2.html ?
I never played with it either .Just use glTranslatef ( or the DirectX equivalent ) to draw a cube using the same code , but moving the relative point where you draw it .
Maybe there 's a better way to do it though , I 'm fairly new to OpenGL .
Be sure to set your viewpoint so you can see them all .See this blog post where the author describes a method for overriding the default behavior for generating the ViewState and instead shows how to save it on the server Session object .
In ASP.NET 2.0 , ViewState is saved by a descendant of PageStatePersister class .
This class is an abstract class for saving and loading ViewsState and there are two implemented descendants of this class in .Net Framework , named HiddenFieldPageStatePersister and SessionPageStatePersister .
By default HiddenFieldPageStatePersister is used to save/load ViewState information , but we can easily get the SessionPageStatePersister to work and save ViewState in Session object .
Although I did not test his code , it seems to show exactly what you want : a way to gain access to ViewState code while still on the server , before postback .Here is an implementation of Bayesian filtering in C # : A Naive Bayesian Spam Filter for C # ( hosted on CodeProject ) .You say you have enough trouble getting one cube to display ... so I am not sure if you have got one to display or not .
Basically ... put your code for writing a cube in one function , then just call that function 100 times .
void DrawCube ( ) { //code to draw the cube } void DisplayCubes ( ) { for ( int i = 0 ; i < 10 ; ++i ) { for ( int j = 0 ; j < 10 ; ++j ) { glPushMatrix ( ) ; //alter these values depending on the size of your cubes .
//This call makes sure that your cubes are n't drawn overtop of each other glTranslatef ( i*5.0 , j*5.0 , 0 ) ; DrawCube ( ) ; glPopMatrix ( ) ; } } } That is the basic outline for how you could go about doing this .
If you want something more efficient take a look into Display Lists sometime once you have the basics figured out : )Exceptions in C++ do n't need to be caught ( no compile time errors ) by the calling function .
So it 's up to developer 's judgment whether to catch it using try/catch ( unlike in Java ) .
Is there a way one can ensure that the exceptions thrown are always caught using try/catch by the calling function ?Given that indexing is so important as your dataset increases in size , can someone explain how indexing works at a database agnostic level ?
For information on queries to index a field , check out http : //stackoverflow.com/questions/1156/how-do-i-index-a-database-fieldNo .
See A Pragmatic Look at Exception Specifications for reasons why not .
The only way you can `` help '' this is to document the exceptions your function can throw , say as a comment in the header file declaring it .
This is not enforced by the compiler or anything .
Use code reviews for that purpose .AppDomain.UnhandledException is an event , not a global exception handler .
This means , by the time it is raised , your application is already on its way down the drain , and there is nothing you can do about it , except for doing cleanup and error logging .
What happened behind the scenes is this : The framework detected the exception , walked up the call stack to the very top , found no handlers that would recover from the error , so was unable to determine if it was safe to continue execution .
So , it started the shutdown sequence , and fired up this event as a courtesy to you so you can pay your respects to your already-doomed process .
This happens when an exception is left unhandled in the main thread .
There is no single-point solution to this kind of error .
You need to put a real exception handler ( a catch block ) upstream of all places where this error occurs and forward it to ( for example ) a global handler method/class that will determine if it is safe to simply report and continue , based on exception type and/or content .
Edit : It is possible to disable ( =hack ) the error-reporting mechanism built into Windows so the mandatory `` crash and burn '' dialog does not get displayed when your app goes down .
However , this becomes effective for all the applications in the system , not just your own .I 've just set up Sandcastle again .
Try installing it ( the May 2008 release ) and search for SandcastleGui.exe or something similar ( it 's in the examples folder or so ) .
Click Add Assembly and add your Assembly or Assemblies , add any .xml Documentation files ( the ones generated by the compiler if you enabled that option ) and then Build .
It will take some time , but the result will be worth the effort .
It will actually look up stuff from MSDN , so your resulting documentation will also have the Class Inheritance all the way down to System.Object with links to MSDN and stuff .
Sandcastle seems a bit complicated at first , especially when you want to use it in an automated build , but I am absolutely sure it will be worth the effort .
Also have a look at Sandcastle Help File Builder , this is a somewhat more advanced GUI for it .Follow this simple 5 step article and you are pretty much done .
As a bonus you can use H2Viewer to view Html Help 2.x files .Why is it needed ?
When data is stored on disk based storage devices , it is stored as blocks of data .
These blocks are accessed in their entirety , making them the atomic disk access operation .
Disk blocks are structured in much the same way as linked lists ; both contain a section for data , a pointer to the location of the next node ( or block ) , and both need not be stored contiguously .
Due to the fact that a number of records can only be sorted on one field , we can state that searching on a field that isn’t sorted requires a Linear Search which requires N/2 block accesses ( on average ) , where N is the number of blocks that the table spans .
If that field is a non-key field ( i.e .
doesn’t contain unique entries ) then the entire table space must be searched at N block accesses .
Whereas with a sorted field , a Binary Search may be used , this has log2 N block accesses .
Also since the data is sorted given a non-key field , the rest of the table doesn’t need to be searched for duplicate values , once a higher value is found .
Thus the performance increase is substantial .
What is indexing ?
Indexing is a way of sorting a number of records on multiple fields .
Creating an index on a field in a table creates another data structure which holds the field value , and pointer to the record it relates to .
This index structure is then sorted , allowing Binary Searches to be performed on it .
The downside to indexing is that these indexes require additional space on the disk , since the indexes are stored together in a table using the MyISAM engine , this file can quickly reach the size limits of the underlying file system if many fields within the same table are indexed .
How does it work ?
Firstly , let’s outline a sample database table schema ; Field name Data type Size on disk id ( Primary key ) Unsigned INT 4 bytes firstName Char ( 50 ) 50 bytes lastName Char ( 50 ) 50 bytes emailAddress Char ( 100 ) 100 bytes Note : char was used in place of varchar to allow for an accurate size on disk value .
This sample database contains five million rows , and is unindexed .
The performance of several queries will now be analyzed .
These are a query using the id ( a sorted key field ) and one using the firstName ( a non-key unsorted field ) .
Example 1 Given our sample database of r = 5,000,000 records of a fixed size giving a record length of R = 204 bytes and they are stored in a table using the MyISAM engine which is using the default block size B = 1,024 bytes .
The blocking factor of the table would be bfr = ( B/R ) = 1024/204 = 5 records per disk block .
The total number of blocks required to hold the table is N = ( r/bfr ) = 5000000/5 = 1,000,000 blocks .
A linear search on the id field would require an average of N/2 = 500,000 block accesses to find a value given that the id field is a key field .
But since the id field is also sorted a binary search can be conducted requiring an average of log2 1000000 = 19.93 = 20 block accesses .
Instantly we can see this is a drastic improvement .
Now the firstName field is neither sorted nor a key field , so a binary search is impossible , nor are the values unique , and thus the table will require searching to the end for an exact N = 1,000,000 block accesses .
It is this situation that indexing aims to correct .
Given that an index record contains only the indexed field and a pointer to the original record , it stands to reason that it will be smaller than the multi-field record that it points to .
So the index itself requires fewer disk blocks than the original table , which therefore requires fewer block accesses to iterate through .
The schema for an index on the firstName field is outlined below ; Field name Data type Size on disk firstName Char ( 50 ) 50 bytes ( record pointer ) Special 4 bytes Note : Pointers in MySQL are 2 , 3 , 4 or 5 bytes in length depending on the size of the table .
Example 2 Given our sample database of r = 5,000,000 records with an index record length of R = 54 bytes and using the default block size B = 1,024 bytes .
The blocking factor of the index would be bfr = ( B/R ) = 1024/54 = 18 records per disk block .
The total number of blocks required to hold the table is N = ( r/bfr ) = 5000000/18 = 277,778 blocks .
Now a search using the firstName field can utilise the index to increase performance .
This allows for a binary search of the index with an average of log2 277778 = 18.08 = 19 block accesses .
To find the address of the actual record , which requires a further block access to read , bringing the total to 19 + 1 = 20 block accesses , a far cry from the 277,778 block accesses required by the non-indexed table .
When should it be used ?
Given that creating an index requires additional disk space ( 277,778 blocks extra from the above example ) , and that too many indexes can cause issues arising from the file systems size limits , careful thought must be used to select the correct fields to index .
Since indexes are only used to speed up the searching for a matching field within the records , it stands to reason that indexing fields used only for output would be simply a waste of disk space and processing time when doing an insert or delete operation , and thus should be avoided .
Also given the nature of a binary search , the cardinality or uniqueness of the data is important .
Indexing on a field with a cardinality of 2 would split the data in half , whereas a cardinality of 1,000 would return approximately 1,000 records .
With such a low cardinality the effectiveness is reduced to a linear sort , and the query optimizer will avoid using the index if the cardinality is less than 30 % of the record number , effectively making the index a waste of space .Back in the old days , Help was not trivial but possible : generate some funky .rtf file with special tags , run it through a compiler , and you got a WinHelp file ( .hlp ) that actually works really well .
Then , Microsoft decided that WinHelp was not hip and cool anymore and switched to CHM , up to the point they actually axed WinHelp from Vista .
Now , CHM maybe nice , but everyone that tried to open a .chm file on the Network will know the nice `` Navigation to the webpage was canceled '' screen that is caused by security restrictions .
While there are ways to make CHM work off the network , this is hardly a good choice , because when a user presses the Help Button he wants help and not have to make some funky settings .
Bottom Line : I find CHM absolutely unusable .
But with WinHelp not being an option anymore either , I wonder what the alternatives are , especially when it comes to integrate with my Application ( i.e .
for WinHelp and CHM there are functions that allow you to directly jump to a topic ) ?
PDF has the disadvantage of requiring the Adobe Reader ( or one of the more lightweight ones that not many people use ) .
I could live with that seeing as this is kind of standard nowadays , but can you tell it reliably to jump to a given page/anchor ?
HTML files seem to be the best choice , you then just have to deal with different browsers ( CSS and stuff ) .
Edit : I am looking to create my own Help Files .
As I am a fan of the `` No Setup , Just Extract and Run '' Philosophy , i had that problem many times in the past because many of my users will run it off the network , which causes exactly this problem .
So i am looking for a more robust and future-proof way to provide help to my users without having to code a different help system for each application i make .
CHM is a really nice format , but that Security Stuff makes it unusable , as a Help system is supposed to provide help to the user , not to generate even more problems .HTML would be the next best choice , ONLY IF you would serve them from a public web server .
If you tried to bundle it with your app , all the files ( and images ( and stylesheets ( and ... ) ) ) would make CHM look like a gift from gods .
That said , when actually bundled in the installation package , ( instead of being served over the network ) , I found the CHM files to work nicely .
OTOH , another pitfall about CHM files : Even if you try to open a CHM file on a local disk , you may bump into the security block if you initially downloaded it from somewhere , because the file could be marked as `` came from external source '' when it was obtained .Is the question how to generate your own help files , or what is the best help file format ?
Personally , I find CHM to be excellent .
One of the first things I do when setting up a machine is to download the PHP Manual in CHM format ( http : //www.php.net/download-docs.php ) and add a hotkey to it in Crimson Editor .
So when I press F1 it loads the CHM and performs a search for the word my cursor is on ( great for quick function reference ) .Our software is both distributed locally to the clients and served from a network share .
We opted for generating both a CHM file and a set of HTML files for serving from the network .
Users starting the program locally use the CHM file , and users getting their program served from a network share has to use the HTML files .
We use Help and Manual and can thus easily produce both types of output from the same source project .
The HTML files also contain searching capabilities and does n't require a web server , so though it is n't an optimal solution , works fine .
So far all the single-file types for Windows seems broken in one way or another : WinHelp - obsoleted HtmlHelp ( CHM ) - obsoleted on Vista , does n't work from network share , other than that works really nice Microsoft Help 2 ( HXS ) - this seems to work right up until the point when it does n't , corrupted indexes or similar , this is used by Visual Studio 2005 and above , as an exampleIn French , but you should be able to find the download link : ) PHP Naive Bayesian FilterI 've tried to write a string replace function in C , which works on a char * , which has been allocated using malloc ( ) .
It 's a little different in that it will find and replace strings , rather than characters in the starting string .
It 's trivial to do if the search and replace strings are the same length ( or the replace string is shorter than the search string ) , since I have enough space allocated .
If I try to use realloc ( ) , I get an error that tells me I am doing a double free - which I do n't see how I am , since I am only using realloc ( ) .
Perhaps a little code will help : @ code1 @ The program works , until I try to realloc ( ) in an instance where the replaced string will be longer than the initial string .
( It still kind of works , it just spits out errors as well as the result ) .
If it helps , the calling code looks like : @ code2 @Anyone know of any services that will match IP to geographic location Well , lucky for you that answer can be found on our very own stackoverflow website : http : //stackoverflow.com/questions/1033/ip-to-country spoiler : http : //www.hostip.info/use.htmlJust a shot in the dark because I have n't tried it yet but when you realloc it returns the pointer much like malloc .
Because realloc can move the pointer if needed you are most likely operating on an invalid pointer if you do n't do the following : input = realloc ( input , strlen ( input ) + delta ) ;Note , try to edit your code to get rid of the html escape codes .
Well , though it has been a while since I used C/C++ , realloc that grows only reuses the memory pointer value if there is room in memory after your original block .
For instance , consider this : ( xxxxxxxxxx ... ... ... . ) If your pointer points to the first x , and .
means free memory location , and you grow the memory size pointed to by your variable by 5 bytes , it 'll succeed .
This is of course a simplified example as blocks are rounded up to a certain size for alignment , but anyway .
However , if you subsequently try to grow it by another 10 bytes , and there is only 5 available , it will need to move the block in memory and update your pointer .
However , in your example you are passing the function a pointer to the character , not a pointer to your variable , and thus while the strrep function internally might be able to adjust the variable in use , it is a local variable to the strrep function and your calling code will be left with the original pointer variable value .
This pointer value , however , has been freed .
In your case , input is the culprit .
However , I would make another suggestion .
In your case it looks like the input variable is indeed input , and if it is , it should n't be modified , at all .
I would thus try to find another way to do what you want to do , without changing input , as side-effects like this can be hard to track down .As a general rule , you should never do a free or realloc on a user provided buffer .
You do n't know where the user allocated the space ( in your module , in another DLL ) so you can not use any of the allocation functions on a user buffer .
Provided that you now can not do any reallocation within your function , you should change a little its behavior like doing only one replacement , so the user will be able to compute the resulting string max length and provide you with a buffer long enough for this one replacement to occur .
Then you could create another function to do the multiple replacements , but you will have to allocate the whole space for the resulting string and copy the user input string .
Then you must provide a way to delete the string you allocated .
Resulting in : void strrep ( char *input , char *search , char *replace ) ; char* strrepm ( char *input , char *search , char *replace ) ; void strrepmfree ( char *input ) ;My quick hints .
Instead of : void strrep ( char *input , char *search , char *replace ) try : void strrep ( char * & input , char *search , char *replace ) and than in the body : input = realloc ( input , strlen ( input ) + delta ) ; Generally read about passing function arguments as values/reference and realloc ( ) description : ) .Hopefully , I can get answers for each database server .
For an outline of how indexing works check out : http : //stackoverflow.com/questions/1108/how-does-database-indexing-workThe following is SQL92 standard so should be supported by the majority of RDMBS that use SQL : CREATE INDEX [ index name ] ON [ table name ] ( [ column name ] )I 've tried using input = realloc ( ... ) , but that appears to just case some other problems , which , considering this is a slightly `` non-recommended '' practice of altering strings that I do n't know where they 've come from , I will take lassevk 's advice of doing it another way ...
... Since this is n't really production code ( it 's for learning , only ) , and the specification said `` you can limit input to 80 characters '' , and the only increasing replace is `` Noel '' - > `` Christmas '' , I can just initially allocate 161 characters , and leave it be .I am using CCNET on a sample project with SVN as my source control .
CCNET is configured to create a build on every check in .
CCNET uses MSBuild to build the source code .
I would like to use the latest revision number to generate AssemblyInfo.cs while compiling .
How can I retrieve the latest revision from subversion and use the value in CCNET ?
Edit : I 'm not using NAnt - only MSBuild .This seems to work ; char *strrep ( char *string , const char *search , const char *replace ) { char *p = strstr ( string , search ) ; if ( p ) { int occurrence = p - string ; int stringlength = strlen ( string ) ; int searchlength = strlen ( search ) ; int replacelength = strlen ( replace ) ; if ( replacelength > searchlength ) { string = ( char * ) realloc ( string , strlen ( string ) + replacelength - searchlength + 1 ) ; } if ( replacelength ! = searchlength ) { memmove ( string + occurrence + replacelength , string + occurrence + searchlength , stringlength - occurrence - searchlength + 1 ) ; } strncpy ( string + occurrence , replace , replacelength ) ; } return string ; } Sigh , is there anyway to post code without it sucking ?I am currently `` manually '' doing it through a prebuild-exec Task , using my cmdnetsvnrev tool , but if someone knows a better ccnet-integrated way of doing it , i 'd be happy to hear : - )I found this project on google code .
This is CCNET plugin to generate the label in CCNET .
The DLL is tested with CCNET 1.3 but it works with CCNET 1.4 for me .
I 'm successfully using this plugin to label my build .
Now onto passing it to MSBuild ...You have basically two options .
Either you write a simple script that will start and parse output from svn.exe info -- revision HEAD to obtain revision number ( then generating AssemblyInfo.cs is pretty much straight forward ) or just use plugin for CCNET .
Here it is : SVN Revision Labeller is a plugin for CruiseControl.NET that allows you to generate CruiseControl labels for your builds , based upon the revision number of your Subversion working copy .
This can be customised with a prefix and/or major/minor version numbers .
http : //code.google.com/p/svnrevisionlabeller/ I prefer the first option because it 's only roughly 20 lines of code : @ code1 @I need to be able to manipulate a large ( 10^7 nodes ) graph in python .
The data corresponding to each node/edge is minimal , say , a small number of strings .
What is the most efficient , in terms of memory and speed , way of doing this ?
A dict of dicts is more flexible and simpler to implement , but I intuitively expect a list of lists to be faster .
The list option would also require that I keep the data separate from the structure , while dicts would allow for something of the sort : @ code1 @ What would you suggest ?
Yes , I should have been a bit clearer on what I mean by efficiency .
In this particular case I mean it in terms of random access retrieval .
Loading the data in to memory is n't a huge problem .
That 's done once and for all .
The time consuming part is visiting the nodes so I can extract the information and measure the metrics I 'm interested in .
I had n't considered making each node a class ( properties are the same for all nodes ) but it seems like that would add an extra layer of overhead ?
I was hoping someone would have some direct experience with a similar case that they could share .
After all , graphs are one of the most common abstractions in CS .If you prefer doing it on the MSBuild side over the CCNet config , looks like the MSBuild Community Tasks extension 's SvnVersion task might do the trick .A dictionary may also contain overhead , depending on the actual implementation .
A hashtable usually contain some prime number of available nodes to begin with , even though you might only use a couple of the nodes .
Judging by your example , `` Property '' , would you be better of with a class approach for the final level and real properties ?
Or is the names of the properties changing a lot from node to node ?
I 'd say that what `` efficient '' means depends on a lot of things , like : speed of updates ( insert , update , delete ) speed of random access retrieval speed of sequential retrieval memory used I think that you 'll find that a data structure that is speedy will generally consume more memory than one that is slow .
This is n't always the case , but most data structures seems to follow this .
A dictionary might be easy to use , and give you relatively uniformly fast access , it will most likely use more memory than , as you suggest , lists .
Lists , however , generally tend to contain more overhead when you insert data into it , unless they preallocate X nodes , in which they will again use more memory .
My suggestion , in general , would be to just use the method that seems the most natural to you , and then do a `` stress test '' of the system , adding a substantial amount of data to it and see if it becomes a problem .
You might also consider adding a layer of abstraction to your system , so that you do n't have to change the programming interface if you later on need to change the internal data structure .If you are doing `` just extract and run '' , you are going to run in security issues .
This is especially true if you are users are running Vista ( or later ) .
is there a reason why you wanted to avoid packaging your applications inside an installer ?
Using an installer would alleviate the `` external source '' problem .
You would be able to use .chm files without any problems .
We use InstallAware to create our install packages .
It 's not cheap , but is very good .
If cost is your concern , WIX is open source and pretty robust .
WIX does have a learning curve , but it 's easy to work with .I have just recently started to study Ruby , and in lieu of Jeff 's advice over the weekend ... Stop theorizing .
Write lots of software .
Learn from your mistakes .
... I was interested in honing my skills while helping out the Open Source Community the process so I thought I 'd ask if anyone have any suggestions for cool/interesting Open Source Projects written in Ruby that you know of or are involved in .I am looking to allow users to control of subdomain of an app I am toying with , much like Basecamp where it is customusername.seework.com .
What is required on the DNS end to allow these to be created dynamically and be available instantly .
And how do you recommend dealing with this in the logic of the site ?
Htaccess rule to lookup the subdomain in the DB ?The GNU C library regular expressions facility ( regcomp ( ) , regexec ( ) and friends ) is broken .
Use libetre instead ; the function signatures match the ones provided by glibc .
http : //laurikari.net/tre/The trick to that is to use URL rewriting so that name.domain.com transparently maps to something like domain.com/users/name on your server .
Once you start down that path , it 's fairly trivial to implement .Well , you did n't specify Rails , so I 'm going to throw Shoes out there .
First , building shoes apps is probably the best way to learn Ruby ( Rails is great , but I find mastering Ruby far more fun/useful ) .
Secondly , while I certainly do n't think building crossplatform UI components is trivial , shoes is relatively new , and relatively small .
There are no doubt countless additions that could be made .Do n't worry about DNS and URL rewriting Your DNS record will be static , something like : *.YOURDOMAIN.COM A 123.123.123.123 Ask your DNS provider to do it for you ( if it 's not done already ) or do it by yourself if you have control over your DNS records .
This will automatically point all your subdomains ( current and future ones ) into the same HTTP server .
Once it 's done , you will only need to parse HOST header on every single http request to detect what hostname was used to access your server-side scripts on your http server .
Assuming you 're using ASP.NET , this is kind of silly example I came up with but works and demonstrates simplicity of this approach : < % @ Language= '' C # '' % > < % string subDomain = Request.Url.Host.Split ( ' .
' ) [ 0 ] .ToUpper ( ) ; if ( subDomain == `` CLIENTXXX '' ) Response.Write ( `` Hello CLIENTXXX , your secret number is 33 '' ) ; else if ( subDomain == `` CLIENTYYY '' ) Response.Write ( `` Hello CLIENTYYY , your secret number is 44 '' ) ; else Response.Write ( subDomain+ '' does n't exist '' ) ; % >One of the sites I maintain relies heavily on use of ViewState ( it is n't my code ) .
However , on certain pages where the ViewState is extra-bloated , Safari throws a `` Validation of viewstate MAC failed '' error .
This appears to only happen in Safari .
Firefox , IE and Opera all load successfully in the same scenario .I 've been doing a little research into this and whilst I 'm not entirely sure its the cause I believe it is because Safari is not returning the full result set ( hence cropping it ) .
I have been in dicussion with another developer and found the following post on Channel 9 as well which recommends making use of the SQL State service to store the viewstate avoiding the postback issue and also page size .
http : //channel9.msdn.com/forums/TechOff/250549-ASPNET-ViewState-flawed-architecture/ ? CommentID=270477 # 263702 Does this seem like the best solution ?Making a class-based structure would probably have more overhead than the dict-based structure , since in python classes actually use dicts when they are implemented .My first port of call would be to go through the elements on the page and see which controls : Will still work when I switch ViewState off Can be moved out of the page and into an AJAX call to be loaded when required Failing that , and here 's the disclaimer - I 've never used this solution on a web-facing site - but in the past where I 've wanted to eliminate massive ViewStates in limited-audience applications I have stored the ViewState in the Session .
It has worked for me because the hit to memory is n't significant for the number of users , but if you 're running a fairly popular site I would n't recommend this approach .
However , if the Session solution works for Safari you could always detect the user agent and fudge appropriately .Customizing csproj files to autogenerate AssemblyInfo.cs http : //www.codeproject.com/KB/dotnet/Customizing_csproj_files.aspx Every time we create a new C # project , Visual Studio puts there the AssemblyInfo.cs file for us .
The file defines the assembly meta-data like its version , configuration , or producer .
Found the above technique to auto-gen AssemblyInfo.cs using MSBuild .
Will post sample shortly .C and C++ compilers will generate a warning when you compare signed and unsigned types ; in your example code , you could n't make your loop variable unsigned and have the compiler generate code without warnings ( assuming said warnings were turned on ) .
Naturally , you 're compiling with warnings turned all the way up , right ?
And , have you considered compiling with `` treat warnings as errors '' to take it that one step further ?
The downside with using signed numbers is that there 's a temptation to overload them so that , for example , the values 0- > n are the menu selection , and -1 means nothing 's selected - rather than creating a class that has two variables , one to indicate if something is selected and another to store what that selection is .
Before you know it , you 're testing for negative one all over the place and the compiler is complaining about how you 're wanting to compare the menu selection against the number of menu selections you have - but that 's dangerous because they 're different types .
So do n't do that .I want to link to a specific slide in an online PowerPoint file , ( e.g .
http : //www.example.com/hello.ppt ) but what I want is that when people click on my link , it goes straight to the nth slide .
Is this possible ?The way we do this is to have a 'catch all ' for our domain name registered in DNS so that anything.ourdomain.com will point to our server .
With Apache you can set up a similar catch-all for your vhosts .
The ServerName must be a single static name but the ServerAlias directive can contain a pattern .
Servername www.ourdomain.com ServerAlias *.ourdomain.com Now all of the domains will trigger the vhost for our project .
The final part is to decode the domain name actually used so that you can work out the username in your code , something like ( PHP ) : list ( $ username ) = explode ( `` .
`` , $ _SERVER [ `` HTTP_HOST '' ] ) ; or a RewriteRule as already suggested that silently maps user.ourdomain.com/foo/bar to www.ourdomain.com/foo/bar ? user=user or whatever you prefer .I have written a NAnt build file that handles parsing SVN information and creating properties .
I then use those property values for a variety of build tasks , including setting the label on the build .
I use this target combined with the SVN Revision Labeller mentioned by lubos hasko with great results .
< target name= '' svninfo '' description= '' get the svn checkout information '' > < property name= '' svn.infotempfile '' value= '' $ { build.directory } \svninfo.txt '' / > < exec program= '' $ { svn.executable } '' output= '' $ { svn.infotempfile } '' > < arg value= '' info '' / > < /exec > < loadfile file= '' $ { svn.infotempfile } '' property= '' svn.info '' / > < delete file= '' $ { svn.infotempfile } '' / > < property name= '' match '' value= '' '' / > < regex pattern= '' URL : ( ? 'match ' .
* ) '' input= '' $ { svn.info } '' / > < property name= '' svn.info.url '' value= '' $ { match } '' / > < regex pattern= '' Repository Root : ( ? 'match ' .
* ) '' input= '' $ { svn.info } '' / > < property name= '' svn.info.repositoryroot '' value= '' $ { match } '' / > < regex pattern= '' Revision : ( ?
'match'\d+ ) '' input= '' $ { svn.info } '' / > < property name= '' svn.info.revision '' value= '' $ { match } '' / > < regex pattern= '' Last Changed Author : ( ?
'match'\w+ ) '' input= '' $ { svn.info } '' / > < property name= '' svn.info.lastchangedauthor '' value= '' $ { match } '' / > < echo message= '' URL : $ { svn.info.url } '' / > < echo message= '' Repository Root : $ { svn.info.repositoryroot } '' / > < echo message= '' Revision : $ { svn.info.revision } '' / > < echo message= '' Last Changed Author : $ { svn.info.lastchangedauthor } '' / > < /target >While I second the Channel 9 solution , also be aware that in some hosted environments Safari is not considered an up-level browser .
You may need to add it to your application 's browscap in order to make use of some ASP.Net features .
That was the root cause of some headaches we had for a client 's site that used the ASP Menu control .I have a file in the following format : Data Data Data [ Start ] Data i want [ End ] Data I 'd like to grab the Data i want from between the [ Start ] and [ End ] tags using a RegEx .
Can anyone show me how this might be done ?If you do n't want to use an installer and you do n't want the user to perform any extra steps to allow CHM files over the network , why not fall back to WinHelp ?
Vista does not include WinHlp32.exe out of the box , but it is freely available as a download for both Vista and Server 2008 .\ [ start\ ] ( . * ?
) \ [ end\ ] which 'll put the text in the middle within a captureI have a client-server app where the client is on a Windows Mobile 6 device , written in C++ and the server is on full Windows and written in C # .
Originally , I only needed it to send messages from the client to the server , with the server only ever sending back an acknowledgment that it received the message .
Now , I would like to update it so that the server can actually send a message to the client to request data .
As I currently have it set up so the client is only in receive mode after it sends data to the server , this does n't allow for the server to send a request at any time .
I would have to wait for client data .
My first thought would be to create another thread on the client with a separate open socket , listening for server requests ... just like the server already has in respect the the client .
Is there a way , within the same thread and using the same socket , to all the server to send requests at any time ?
Can you use something to the affect of WaitForMultipleObjects ( ) and pass it a receive buffer and an event that tells it there is data to be sent ?Link to http : //www.whatever.com/hello.ppt # 4 for slide 4 to be opened initially .
Reference\ [ start\ ] \s* ( ( ( ? ! \ [ start\ ] |\ [ end\ ] ) .
) + ) \s*\ [ end\ ] This should hopefully drop the [ start ] and [ end ] markers as well .Jeff , your code is nice but could be clearer with constants ( as suggested in Code Complete ) .
@ code1 @With Perl you can surround the data you want with ( ) 's and pull it out later , perhaps other languages have a similar feature .
@ code1 @I do n't know how accurate that hostip.info site is .
I just visted that site , and it reported that my country is Canada .
I 'm in the US and the ISP that my office uses only operates from the US .
It does allow you to correct it 's guess , but if you are using this service to track web site vistors by country , you 'll have no way of knowing if the data is correct .
Of course , I 'm just one data point .
I downloaded the GeoLite Country database , which is just a .csv file , and my IP address was correctly identified as US .
Another benefit of the MaxMind product line ( paid or free ) is that you have the data , you do n't incur the performance hit of making a web service call to another system .i found the answer .
all you have to do is add # n after .ppt .
e.g .
http : //www.whatever.com/hello.ppt # 4 will take you straight to the 4th slide .My company is using CVS as our de-facto standard for source control .
However , I 've heard a lot of people say that SVN is better .
I know SVN is newer , but other than that , I 'm unfamiliar with its benefits .
What I 'm looking for is a good , succinct comparison of the two systems , noting any advantages or disadvantages of each in a Java/Eclipse development environment .All memory leaks are resolved by program termination .
Leak enough memory and the Operating System may decide to resolve the problem on your behalf .One of the many comparisons : http : //wiki.scummvm.org/index.php/CVS_vs_SVN Now this is very specific to that project , but a lot of stuff apllies in general .
Pro Subversion : Support for versioned renames/moves ( impossible with CVS ) : Fingolfin , Ender Supports directories natively : It 's possible to remove them , and they are versioned : Fingolfin , Ender File properties are versioned ; no more `` executable bit '' hell : Fingolfin Overall revision number makes build versioning and regression testing much easier : Ender , Fingolfin Atomic commits : Fingolfin Intuitive ( directory-based ) branching and tagging : Fingolfin Easier hook scripts ( pre/post commit , etc ) : SumthinWicked ( I use it for Doxygen after commits ) Prevents accidental committing of conflicted files : Salty-horse , Fingolfin Support for custom 'diff ' command : Fingolfin Offline diffs , and they 're instant : sevI 'll just post here since some of the other posts are slightly inaccurate in relation to C # .
Correct : int is an alias for System.Int32 .
Wrong : float is not an alias for System.Float , but for System.Single Basically , int is a reserved keyword in the C # programming language , and is an alias for the System.Int32 value type .
float and Float is not the same however , as the right system type for ''float '' is System.Single .
There are some types like this that has reserved keywords that does n't seem to match the type names directly .
In C # there is no difference between ''int '' and ''System.Int32 '' , or any of the other pairs or keywords/system types , except for when defining enums .
With enums you can specify the storage size to use and in this case you can only use the reserved keyword , and not the system runtime type name .
Wether the value in the int will be stored on the stack , in memory , or as a referenced heap object depends on the context and how you use it .
This declaration in a method : @ code1 @ defines a variable i of type System.Int32 , living in a register or on the stack , depending on optimizations .
The same declaration in a type ( struct or class ) defines a member field .
The same declaration in a method argument list defines a parameter , with the same storage options as for a local variable .
( note that this paragraph is not valid if you start pulling iterator methods into the mix , these are different beasts altogether ) To get a heap object , you can use boxing : @ code2 @ this will create a boxed copy of the contents of i on the heap .
In IL you can access methods on the heap object directly , but in C # you need to cast it back to an int , which will create another copy .
Thus , the object on the heap can not easily be changed in C # without creating a new boxed copy of a new int value .
( Ugh , this paragraph does n't read all that easily . )The Subversion book has an appendix that details important differences from CVS , which may help you make your decision .
The two approaches are more or less the same idea but SVN was specifically designed to fix long standing flaws in CVS so , in theory at least , SVN will always be the better choice .CVS only tracks modification on a file-by-file basis , while SVN tracks a whole commit as a new revision , which means that it is easier to follow the history of your project .
Add the fact that all modern source control software use the concept of revision so it is far easier to migrate from SVN than it is from CVS .
There is also the atomic commit problem .
While I only encountered it once , it is possible that 2 people committing together in CVS can conflict each other , losing some data and putting your client in an inconsistent state .
When detected early , these problems are not major because your data is still out there somewhere , but it can be a pain in a stressful environment .
And finally , not many tools are developed around CVS anymore .
While the new and shiny-new tools like Git or Mercurial definitely lack tools yet , SVN has a pretty large application base on any system .
EDIT 2015 : Seriously , this answer is 7 years old now .
Forget SVN , go use Git like everyone else !When I needed to write an application with a client-server model where the clients could leave and enter whenever they want , ( I assume that 's also the case for your application as you use mobile devices ) I made sure that the clients send an online message to the server , indicating they were connected and ready to do whatever they needed doing .
at that time the server could send messages back to the client trough the same open connection .
Also , but I do n't know if that is applicable for you , I had some sort of heartbeat the clients sent to the server , letting it know it was still online .
That way the server knows when a client was forcibly disconnected from the network and it could mark that client back as offline .At what point does a MySQL database start to lose performance ?
Does physical database size matter ?
Do number of records matter ?
Is any performance degradation linear or exponential ?
I have what I believe to be a large database , with roughly 15M records which take up almost 2GB .
Based on these numbers , is there any incentive for me to clean the data out , or am I safe to allow it to continue scaling for a few more years ?SVN has 3 main advantages over CVS it 's faster supports versioning of binary files and adds transactional commit ( all or nothing )You can use the artifact directory variable inside the MSBuild script itself .
Here 's an example of how I 'm running FxCop right now from my CC.Net MSBuild script ( this script is what CC.Net points to - there is also a `` Build '' target in the script that includes an MSBuild task against the SLN to do the actual compilation ) : @ code1 @I am looking for a tool that can detect malicious requests ( such as obvious SQL injection gets or posts ) and will immediately ban the IP address of the requester/add to a blacklist .
I am aware that in an ideal world our code should be able to handle such requests and treat them accordingly , but there is a lot of value in such a tool even when the site is safe from these kinds of attacks , as it can lead to saving bandwidth , preventing bloat of analytics , etc .
Ideally , I 'm looking for a cross-platform ( LAMP/.NET ) solution that sits at a higher level than the technology stack ; perhaps at the web-server or hardware level .
I 'm not sure if this exists , though .
Either way , I 'd like to hear the community 's feedback so that I can see what my options might be with regard to implementation and approach .Wine is actually using gettimeofday ( ) to implement QueryPerformanceCounter ( ) and it is known to make many Windows games work on Linux and Mac .
Starts http : //source.winehq.org/source/dlls/kernel32/cpu.c # L312 leads to http : //source.winehq.org/source/dlls/ntdll/time.c # L448I have a Queue < T > object that I have initialised to a capacity of 2 , but obviously that is just the capacity and it keeps expanding as I add items .
Is there already an object that automatically dequeues an item when the limit is reached , or is the best solution to create my own inherited class ?Why would n't you just use an array with a size of 2 ?
A Queue is supposed to be able to dynamically grow and shrink .
Or create a wrapper class around an instance of Queue < T > instance and each time one enqueues a < T > object , check the size of the queue .
If larger than 2 , dequeue the first item .I am using MSBuild to build my stuff .
I want to use CruiseControl.net as by Build Server .
Now , CCNET refers nAnt a lot , but it looks as if ccnet can do most of the stuff nant could do through the project configuration and msbuild .
Also , nAnt seems a bit unsupported , with a Beta release that is almost a year old now .
In short : I am actually quite happy with MSBuild ( especially since it 's the `` official '' compiler front end ) and a bit uncomfortable with nAnt , but I do not want to judge prematurely .
What would be reasons to use nAnt over MSBuild ?
Especially with ccnet , which seems to overlap a bit with nant in terms of features ( and adding the automated build related stuff )You should create your own class , a ringbuffer would probably fit your needs .
The data structures in .NET that allows you to specify capacity , except for array , uses this to build the internal data structure used to hold the internal data .
For instance , for a list , capacity is used to size an internal array .
When you start adding elements to the list , it 'll start filling this array from index 0 and up , and when it reaches your capacity , it increases the capacity to a new higher capacity , and continues filling it up .Is there any way to check whether a file is locked without using a try/catch block ?
Right now , the only way I know of is to just open the file and catch any System.IO.IOException .I 've knocked up a basic version of what I 'm looking for , it 's not perfect but it 'll do the job until something better comes along .
public class LimitedQueue < T > : Queue < T > { private int limit = -1 ; public int Limit { get { return limit ; } set { limit = value ; } } public LimitedQueue ( int limit ) : base ( limit ) { this.Limit = limit ; } public new void Enqueue ( T item ) { if ( this.Count > = this.Limit ) { this.Dequeue ( ) ; } base.Enqueue ( item ) ; } }I 'd like to create a progress bar to indicate the status of an a batch job in Ruby .
I 've read some tutorials / libraries on using ( n ) curses , none of which were particularly helpful in explaining how to create an `` animated '' progress bar in the terminal or using curses with Ruby .
I 'm already aware of using a separate thread to monitor the progress of a given job , I 'm just not sure how to proceed with drawing a progress bar .
Update ProgressBar class was incredibly straight-forward , perfectly solved my problem .No , unfortunately , and if you think about it , that information would be worthless anyway since the file could become locked the very next second ( read : short timespan ) .
Why specifically do you need to know if the file is locked anyway ?
Knowing that might give us some other way of giving you good advice .
If your code would look like this : @ code1 @ Then between the two lines , another process could easily lock the file , giving you the same problem you were trying to avoid to begin with : exceptions .How do you specify a callback for rb- > pcm_play_data ( ) ?I was just listening to a Podcast from Scott Hanselman this morning , where he talks about internationalization , especially the really tricky things , like Turkish ( with it 's four i 's ) and Thai .
Also , Jeff Atwood had a post :Original Question I am looking for a function that attempts to quantify how `` distant '' ( or distinct ) two colors are .
This question is really in two parts : What color space best represents human vision ?
What distance metric in that space best represents human vision ( euclidean ? )We have been using CruiseControl for quite a while with NUnit and NAnt .
For a recent project we decided to use the testing framework that comes with Visual Studio , which so far has been adequate .
I 'm attempting to get the solution running in CruiseControl .
I 've finally got the build itself to work ; however , I have been unable to get any tests to show up in the CruiseControl interface despite adding custom build tasks and components designed to do just that .
Does anyone have a definitive link out there to instructions on getting this set up ?The problem with a generic tool is that it is very difficult to come up with a set of rules that will only match against a genuine attack .
SQL keywords are all English words and do n't forget that the string DROP TABLE users ; is perfectly valid in a form field that , for example , contains an answer to a programming question .
The only sensible option is to sanitise the input before ever passing it to your database but pass it on nonetheless .
Otherwise lots of perfectly normal , non-malicious users are going to get banned from your site .You might be able to get some implementation ideas from the Ruby/ProgressBar library , which generates text progress bars .
I stumbled across it a couple of months back but have n't made any use of it .Not sure if that helps ( i found the ccnet Documentation somewhat unhelpful at times ) : Using CruiseControl.NET with MSTestIn my opinion it is more a question of personal preference .
nAnt is a great framework and MSBuild is almost as capable .
With the ability to easily develop custom tasks ( in both frameworks ) you can accomplish almost anything that you need to do .
I can not answer the `` still supported '' portion of your questions , but I would say if you are already comfortable with nAnt then it 's probably viable .
If you ( or someone in your group ) is familiar with MSBuild then that is a fine way to go as well .May look like spam but no , this link is really interesting for color spaces : ) http : //www.compuphase.com/cmetric.htmThe easiest distance would of course be to just consider the colors as 3d vectors originating from the same origin , and taking the distance between their end points .
If you need to consider such factors that green is more prominent in judging intensity , you can weigh the values .
ImageMagic provides the following scales : red : 0.3 green : 0.6 blue : 0.1 Of course , values like this would only be meaningful in relation to other values for other colors , not as something that would be meaningful to humans , so all you could use the values for would be similiarity ordering .Original Question If you are given N maximally distant colors ( and some associated distance metric ) , can you come up with a way to sort those colors into some order such that the first M are also reasonably close to being a maximally distinct set ?
In other words , given a bunch of distinct colors , come up with an ordering so I can use as many colors as I need starting at the beginning and be reasonably assured that they are all distinct and that nearby colors are also very distinct ( e.g. , bluish red is n't next to reddish blue ) .
Randomizing is OK but certainly not optimal .
Clarification : Given some large and visually distinct set of colors ( say 256 , or 1024 ) , I want to sort them such that when I use the first , say , 16 of them that I get a relatively visually distinct subset of colors .
This is equivalent , roughly , to saying I want to sort this list of 1024 so that the closer individual colors are visually , the farther apart they are on the list .HSL and HSV are better for human color perception .
According to Wikipedia : It is sometimes preferable in working with art materials , digitized images , or other media , to use the HSV or HSL color model over alternative models such as RGB or CMYK , because of differences in the ways the models emulate how humans perceive color .
RGB and CMYK are additive and subtractive models , respectively , modelling the way that primary color lights or pigments ( respectively ) combine to form new colors when mixed .One method that might work for some cases would be to take the sql string that would run if you naively used the form data and pass it to some code that counts the number of statements that would actually be executed .
If it is greater than the number expected , then there is a decent chance that an injection was attempted , especially for fields that are unlikely to include control characters such as username .
Something like a normal text box would be a bit harder since this method would be a lot more likely to return false positives , but this would be a start , at least .I need to copy hundreds of gigs of random files around on my computer and am pretty leery of using the vanilla file copy built into Windows .
I do n't want it to hang on a `` Are you sure ?
`` , `` Are you really sure ?
`` , `` Even zip files ?
`` , `` Surely not read-only files too ! ''
loop as soon as I step away .
I do n't want it to work for hours and then stop unexpectedly : `` Someone once opened this file and so I wo n't copy it ! ''
and then cancel the whole copy or just quit with no indication of what was done and what work remains .
What file management programs do you have experience with ?
Which do you recommend ?
This question is related to my other question : How can I use an old PATA hard disk drive on my newer SATA-only computer ?Use Robocopy ( Robust File Copy ) .
NOTE : In Windows Vista and Server 2008 when you type : xcopy / ?
you get : NOTE : Xcopy is now deprecated , please use Robocopy .
So start getting used to robocopy : )Why not use LINQ : @ code1 @ This would also allow for great flexibility in that you can select the top 10 , 20 10 % etc .
Or if you are using your word frequency index for type-ahead , you could also include StartsWith clause as well .One little thing to keep in mind : In some countries ( i.e .
most of Europe ) , people do not have static IP Addresses , so blacklisting should not be forever .You can try TeraCopy or RoboCopy .It sounds like a backup-style tool may be what you 're looking for .
I 've been using SyncBack ( one of the versions is free ) .
You could also try out MS SyncToy which tries to make moving , copying , syncing , etc .
easy .
If you really do copy just random files at random times , you could try Total Copy which has the added benefit of working well over a network ( pause , resume , etc .
) .Now that I think about it , a Bayesian filter similar to the ones used to block spam might work decently too .
If you got together a set of normal text for each field and a set of sql injections , you might be able to train it to flag injection attacks .You really need to use a file Sync tool , like SyncBackSE , MS SyncToy , or even something like WinMerge will do the trick .
I prefer SyncBack as it allows you to set up very explicit rules for just about every possible case and conflict , at least more so than the other two .
With any of these you wo n't have to keep clicking all the pop-ups and you can verify , without a doubt , that the destination is exactly the same as the source .The physical database size does n't matter .
The number of records do n't matter .
In my experience the biggest problem that you are going to run in to is not size , but the number of queries you can handle at a time .
Most likely you are going to have to move to a master/slave configuration so that the read queries can run against the slaves and the write queries run against the master .
However if you are not ready for this yet , you can always tweak your indexes for the queries you are running to speed up the response times .
Also there is a lot of tweaking you can do to the network stack and kernel in Linux that will help .
I have had mine get up to 10GB , with only a moderate number of connections and it handled the requests just fine .
I would focus first on your indexes , then have a server admin look at your OS , and if all that does n't help it might be time to implement a master/slave configuration .How about good old Command-Line Xcopy ?
With S : being the source and T : the target : @ code1 @ /K Copies attributes .
Normal Xcopy will reset read-only attributes .
/R Overwrites read-only files .
/E Copies directories and subdirectories , including empty ones .
/I If destination does not exist and copying more than one file , assumes that destination must be a directory .
/S Copies directories and subdirectories except empty ones .
/C Continues copying even if errors occur .
/H Copies hidden and system files also .
/Y Suppresses prompting to confirm you want to overwrite an existing destination file .
/G Allows the copying of encrypted files to destination that does not support encryption .
/X Copies file audit settings ( implies /O ) .
( Edit : Added /G and /X which are new since a few years )Do you mean that from a set of N colors , you need to pick M colors , where M < N , such that M is the best representation of the N colors in the M space ?
As a better example , reduce a true-color ( 24 bit color space ) to a 8-bit mapped color space ( GIF ? ) .
There are quantization algorithms for this , like the Adaptive Spatial Subdivision algorithm used by ImageMagic .
These algorithms usually do n't just pick existing colors from the source space but creates new colors in the target space that most closely resemble the source colors .
As a simplified example , if you have 3 colors in the original image where two are red ( with different intensity or bluish tints etc . )
and the third is blue , and need to reduce to two colors , the target image could have a red color that is some kind of average of the original two red + the blue color from the original image .
If you need something else then I did n't understand your question : )You can try SuperCopier , it replaces the standard Windows copy mechanism while loaded .
It can retry failed files at the end , resume a canceled copy ( even a copy canceled by Windows ) , accepts `` All '' for every answers .
You can even answer the annoying questions ( file already exists , error copying file ) before they occur .You can split them in to RGB HEX format so that you can compare the R with R 's of a different color , same with the G and B .
Same format as HTML XX XX XXRR GG BB00 00 00 = blackff ff ff = whiteff 00 00 = red00 ff 00 = green00 00 ff = blue So the only thing you would need to decide is how close you want the colors and what is an acceptable difference for the segments to be considered different .Honestly it depends on what fits in to your environment better .
If you are using a lot of Non-Microsoft tools , nunit , ccnet , ncover .
You will probably find better support with nant .
Alternatively if you are using MSTest , TFSBuild , you will probably find MSBuild a better environment .
I would learn both and use which every fits more smoothly with your environment .Well , as a first point of call , I 'd say of the common metrics HSV ( Hue , Saturation and Value ) or HSL are better representative of how humans perceive colour than say RGB or CYMK .
See HSL , HSV on Wikipedia .
I suppose naively I would plot the points in the HSL space for the two colours and calculate the magnitude of the difference vector .
However this would mean that bright yellow and bright green would be considered just as different as green to dark green .
But then many consider red and pink two different colours .
Moreover , difference vectors in the same direction in this parameter space are not equal .
For instance , the human eye picks up green much better than other colours .
A shift in hue from green by the same amount as a shift from red may seem greater .
Also a shift in saturation from a small amount to zero is the difference between grey and pink , elsewhere the shift would be the difference between two shades of red .
From a programmers point of view , you would need to plot the difference vectors but modified by a proportionality matrix that would adjust the lengths accordingly in various regions of the HSL space - this would be fairly arbitrary and would be based on various colour theory ideas but be tweaked fairly arbitrarily depending on what you wanted to apply this to .
Even better , you could see if anyone has already done such a thing online ...This also sounds to me like some kind of resistance graph where you try to map out the path of least resistance .
If you inverse the requirements , path of maximum resistance , it could perhaps be used to produce a set that from the start produces maximum difference as you go , and towards the end starts to go back to values closer to the others .
For instance , here 's one way to perhaps do what you want .
Calculate the distance ( ref your other post ) from each color to all other colors Sum the distances for each color , this gives you an indication for how far away this color is from all other colors in total Order the list by distance , going down This would , it seems , produce a list that starts with the color that is farthest away from all other colors , and then go down , colors towards the end of the list would be closer to other colors in general .
Edit : Reading your reply to my first post , about the spatial subdivision , would not exactly fit the above description , since colors close to other colors would fall to the bottom of the list , but let 's say you have a cluster of colors somewhere , at least one of the colors from that cluster would be located near the start of the list , and it would be the one that generally was farthest away from all other colors in total .
If that makes sense .Xcopy keeps the Date Modified , only the Date Created and Date Accessed will change .
( tested on XP Pro , try it on a small folder to check if you 're using Vista as I did not test it under Vista ) Edit : You MAY want to redirect the Output though : xcopy /K /R ... ... .
s : \* .
* t : \ > c : \xcopy.log 2 > & 1 That way , if files fail to copy you can check the log ( i.e .
System Volume Information will generate an error , but that folder does not matter anyway for what you 're trying to do )Your almost looking at it the wrong way , no 3party tool that is not aware of your application methods/naming/data/domain is going to going to be able to perfectly protect you .
Something like SQL injection prevention is something that has to be in the code , and best written by the people that wrote the SQL , because they are the ones that will know what should/shouldnt be in those fields ( unless your project has very good docs ) Your right , this all has been done before .
You dont quite have to reinvent the wheel , but you do have to carve a new one because of a differences in everyone 's axle diameters .
This is not a drop-in and run problem , you really do have to be familiar with what exactly SQL injection is before you can prevent it .
It is a sneaky problem , so it takes equally sneaky protections .
These 2 links taught me far more then the basics on the subject to get started , and helped me better phrase my future lookups on specific questions that were n't answered .
SQL injection SQL Injection Attacks by Example And while this one isnt quite a 100 % finder , it will `` show you the light '' on existing problem in your existing code , but like with webstandards , dont stop coding once you pass this test .
Exploit-MeThe CC.Net interface is generated via an XSL transform on your XML files put together as specified in the ccnet.config file for your projects .
The XSL is already written for things like FxCop - check your server 's CC xsl directory for examples - should n't be too hard to write your own to add in the info - just remember to add the XML output from your tests into the main log .If you 've already got a bunch of custom tasks you use with nAnt , stick with it - you do n't gain much with MSBuild .
That said , there does n't seem to be anything that nAnt can do that MSBuild ca n't at its core .
Both can call external tools , both can run .Net-based custom tasks , and both have a bunch of community tasks out there .
We 're using MSBuild here for the same reason you are - it 's the default build system for VS now , and we did n't have any nAnt-specific stuff to worry about .
The MSBuildCommunityTasks are a good third-party task base to start with , and covers most of the custom stuff I ever did in nAnt , including VSS and Subversion support .On windows , curses works out of the box , ncurses does n't , and for a progress bar curses should be sufficient .
So , use curses instead of ncurses .
Also , both curses and ncurses are wafer-thin wrappers around the c library - that means you do n't really need Ruby-specific tutorials .
However , on the site for the PickAxe you can download all the code examples for the book .
The file `` ex1423.rb '' contains a curses demo which plays Pong - that should give you plenty of material to get you going .I need the name of the current logged in user in my Air/Flex application .
The application will only be deployed on Windows machines .
I think I could attain this by regexing the User directory , but am open to other ways .Personally I think curses is overkill in this case .
While the curses lib is nice ( and I frequently use it myself ) it 's a PITA to relearn every time I have n't needed it for 12 months which has to be the sign of a bad interface design .
If for some reason you ca n't get on with the progress bar lib Joey suggested roll your own and release it under a pretty free licence for instant kudos : )Here is a solution that works in XP / Vista , but is definitely expandable to OSX , linux , I 'd still be interested in another way .
public static function GetCurrentOSUser ( ) : String { // XP & Vista only .
var userDirectory : String = File.userDirectory.resolvePath ( `` '' ) .nativePath ; var startIndex : Number = userDirectory.lastIndexOf ( `` \\ '' ) + 1 var stopIndex : Number = userDirectory.length ; var user = userDirectory.substring ( startIndex , stopIndex ) ; return user ; }Use Robocopy , it has the ability to copy files in `` restartable mode '' , plus it should respect the file attributes .
And it comes with Vista and Server 2008 , and you can download it for older OS 's .
Plus you can set it to retry on failed copies , to pick up files that are temporarily in use by another process .I saw many questions asking 'how ' to unit test in a specific language , but no question asking 'what ' , 'why ' , and 'when ' .
What is it ?
What does it do for me ?
Why should I use it ?
When should I use it ( also when not ) ?
What are some common pitfalls and misconceptionsWhat is the concensus on when to use one of these tools adversed to the other ?
I find Subsonic very useful in terms of getting things done quickly , but on large projects it tends not to scale , and its ties your domain model to your database model .
That is where Nhibernate comes in as it gives you lightweight POCOs that are unrelated to your database model , but the setup time is much longer .This is an open-ended question .
What approaches should I consider ?I 'm setting up a dedicated SQL Server 2005 box on Windows Server 2008 this week , and would like to pare it down to be as barebones as possible while still being fully functional .
To that end , the `` Server Core '' option sounds appealing , but I 'm not clear about whether or not I can run SQL Server on that SKU .
Several services are addressed on the Microsoft website , but I do n't see any indication about SQL Server .
Does anyone know definitively ?Unit testing is , roughly speaking , testing bits of your code in isolation with test code .
The immediate advantages that come to mind are : Running the tests becomes automate-able and repeatable You can test at a much more granular level than point-and-click testing via a GUI Note that if your test code writes to a file , opens a database connection or does something over the network , it 's more appropriately categorized as an integration test .
Integration tests are a good thing , but should not be confused with unit tests .
Unit test code should be short , sweet and quick to execute .
Another way to look at unit testing is that you write the tests first .
This is known as Test-Driven Development ( TDD for short ) .
TDD brings additional advantages : You do n't write speculative `` I might need this in the future '' code -- just enough to make the tests pass The code you 've written is always covered by tests By writing the test first , you 're forced into thinking about how you want to call the code , which usually improves the design of the code in the long run .
If you 're not doing unit testing now , I recommend you get started on it .
Get a good book , practically any xUnit-book will do because the concepts are very much transferable between them .
Sometimes writing unit tests can be painful .
When it gets that way , try to find someone to help you , and resist the temptation to `` just write the damn code '' .
Unit testing is a lot like washing the dishes .
It 's not always pleasant , but it keeps your metaphorical kitchen clean , and you really want it to be clean .
: ) Edit : One misconception comes to mind , although I 'm not sure if it 's so common .
I 've heard a project manager say that unit tests made the team write all the code twice .
If it looks and feels that way , well , you 're doing it wrong .
Not only does writing the tests usually speed up development , but it also gives you a convenient `` now I 'm done '' indicator that you would n't have otherwise .Not sure how credible this source is , but : The Windows Server 2008 Core edition can : Run the file server role .
Run the Hyper-V virtualization server role .
Run the Directory Services role .
Run the DHCP server role .
Run the IIS Web server role .
Run the DNS server role .
Run Active Directory Lightweight Directory Services .
Run the print server role .
The Windows Server 2008 Core edition can not : Run a SQL Server .
Run an Exchange Server .
Run Internet Explorer .
Run Windows Explorer .
Host a remote desktop session .
Run MMC snap-in consoles locally .Also I would try : File.userDirectory.name But I do n't have Air installed so I ca n't really test this ...I do n't disagree with Dan ( although a better choice may just be not to answer ) ... but ... Unit testing is the process of writing code to test the behavior and functionality of your system .
Obviously tests improve the quality of your code , but that 's just a superficial benefit of unit testing .
The real benefit are to Make it easier to change the technical implementation while making sure you do n't change the behavior ( refactoring ) .
Properly unit tested code can be aggressively refactored/cleaned up with little chance of breaking anything without noticing it .
Give developers confidence when adding behavior or making fixes .
Document your code Indicate areas of your code that are tightly coupled .
It 's hard to unit test code that 's tightly coupled Provide a means to use your API and look for difficulties early on Indicates methods and classes that are n't very cohesive You should unit test because its in your interest to deliver a maintainable and quality product to your client .
I 'd suggest you use it for any system , or part of a system , which models real-world behavior .
In other words , it 's particularly well suited for enterprise development .
I would not use it for throw-away/utility programs .
I would not use it for parts of a system that are problematic to test ( UI is a common example , but that is n't always the case ) The greatest pitfall is that developers test too large a unit , or they consider a method a unit .
This is particularly true if you do n't understand Inversion of Control - in which case your unit tests will always turn into end-to-end integration testing .
Unit test should test individual behaviors - and most methods have many behaviors .
The greatest misconception is that programmers should n't test .
Only bad or lazy programmers believe that .
Should the guy building your roof not test it ?
Should the doctor replacement a heart valve not test the new valve ?
Only a programmer can test that his code does what he intended it to do ( QA can test edge cases - how code behaves when its told to do things the programmer did n't intend , and the client can do acceptance test - does the code do what what the client paid for it to do )There are some parallel extensions to .NET that are currently in testing and available at Microsoft 's Parallel Computing Developer Center .
They have a few interesting items that you would expect like Parallel foreach and a parallel version of LINQ called PLINQ .
Some of the best information about the extensions is on Channel 9 .I have a custom validation function in JavaScript in a user control on a .Net 2.0 web site which checks to see that the fee paid is not in excess of the fee amount due .
I 've placed the validator code in the ascx file , and I have also tried using Page.ClientScript.RegisterClientScriptBlock ( ) and in both cases the validation fires , but can not find the JavaScript function .
The output in Firefox 's error console is `` feeAmountCheck is not defined '' .
Here is the function ( this was taken directly from firefox- > view source ) @ code1 @ Any ideas as to why the function is n't being found ?
How can I remedy this without having to add the function to my master page or consuming page ?For some reason Jeff 's code did n't seem simple enough .
To me this seems simpler and easier to understand : @ code1 @ However , this assumes you are looking for the western idea of age and not using East Asian reckoning .When you 're using .Net 2.0 and Ajax - you should use : @ code1 @ It will work better in Ajax environments then the old Page.ClientScript versionIs it possible to configure xampp to serve up a file outside of the htdocs directory ?
For instance , say I have a file located as follows : C : \projects\transitCalculator\trunk\TransitCalculator.php and my xampp files are normally served out from : C : \xampp\htdocs\ ( because that 's the default configuration ) Is there some way to make Apache recognize and serve up my TransitCalculator.php file without moving it under htdocs ?
Preferably I 'd like Apache to serve up/have access to the entire contents of the projects directory , and I do n't want to move the projects directory under htdocs .
edit : edited to add Apache to the question title to make Q/A more `` searchable ''I am writing an application that downloads large files in the background .
All clients are logged in locally , or through a VPN .
When they are logged in locally , I do not want to throttle downloads .
However , I would like to limit downloads to 10 KBps when the user is connected via VPN .
I can differentiate between these users by IP Address range .
Since this is an AIR Application , I figure I will throttle via server-side since I can do it from either the server itself ( IIS 6 ) or the web service ( asp.net / C # ) .
Throttling through IIS 6 seems to work fine , but it seems like it has to be done across the entire web site .
Is there anyway to do this via IP ?
Or will I have to rig this up in .NET ?You can set Apache to serve pages from anywhere with any restrictions but it 's normally distributed in a more secure form .
Editing your apache files ( http.conf is one of the more common names ) will allow you to set any folder so it appears in your webroot .
EDIT : alias myapp c : \myapp\ I 've edited my answer to include the format for creating an alias in the http.conf file which is sort of like a shortcut in windows or a symlink under un*x where Apache 'pretends ' a folder is in the webroot .
This is probably going to be more useful to you in the long term .You can relocate it by editing the DocumentRoot setting in XAMPP\apache\conf\httpd.conf .
It should currently be : C : /xampp/htdocs Change it to : C : /projects/transitCalculator/trunkI would look into IMAP IMAP , POP3 and NNTPFirst , let 's get the security considerations out of the way .
I 'm using simple authentication under Apache for a one-off , internal use only , non-internet connected lan , php web app .
How can get I the HTTP authenticated user name in PHP ?Try changing the argument names to `` sender '' and `` args '' .
And , after you have it working , switch the call over to ScriptManager.RegisterClientScriptBlock , regardless of AJAX use .I 'm sure you could write a small app that takes an XSD file and parses it into a SQL script .
I 've never seen code out there to do it though , but that 's not saying it does n't exist .Ok , per pix0r 's , Sparks ' and Dave 's answers it looks like there are three ways to do this : Virtual Hosts Open C : \xampp\apache\conf\extra\httpd-vhosts.conf .
Un-comment line 19 ( NameVirtualHost *:80 ) .
Add your virtual host ( ~line 36 ) : @ code1 @ Open your hosts file ( C : \Windows\System32\drivers\etc\hosts ) .
Add @ code2 @ to the end of the file ( before the Spybot - Search & Destroy stuff if you have that installed ) .
Save ( You might have to save it to the desktop , change the permissions on the old hosts file ( right click > properties ) , and copy the new one into the directory over the old one ( or rename the old one ) if you are using Vista and have trouble ) .
Restart Apache .
Now you can access that directory by browsing to http : //transitcalculator.localhost/ .
Make an Alias Starting ~line 200 of your http.conf file , copy everything between < Directory `` C : /xampp/htdocs '' > and < /Directory > ( ~line 232 ) and paste it immediately below with C : /xampp/htdocs replaced with your desired directory ( in this case C : /Projects ) to give your server the correct permissions for the new directory .
Find the < IfModule alias_module > < /IfModule > section ( ~line 300 ) and add @ code3 @ ( or whatever is relevant to your desires ) below the Alias comment block , inside the module tags .
Change your document root Edit ~line 176 in C : \xampp\apache\conf\httpd.conf ; change DocumentRoot `` C : /xampp/htdocs '' to # DocumentRoot `` C : /Projects '' ( or whatever you want ) .
Edit ~line 203 to match your new location ( in this case C : /Projects ) .
Notes : You have to use forward slashes `` / '' instead of back slashes `` \ '' .
Do n't include the trailing `` / '' at the end .
restart your server .I think that you are after this $ username = $ _SERVER [ 'PHP_AUTH_USER ' ] ; $ password = $ _SERVER [ 'PHP_AUTH_PW ' ] ;I think you pretty much nailed it .
Subsonic generates code , so your business objects will be reflective of your database structure .
nHibernate uses mapping files which map your business objects to the database so your objects can be structured however you like .
How large of a project is this ?
Will there be long term support needed ?
Is the cost effectiveness of Subsonic going to offset any potential scaling issues ?I think if you are attempting to learn a new language do something fun in that language .
I learned python by writing lots of web spiders and little toys and for ruby I 'd take exactly the same path .
Instead of finding a project that needs input do a couple of little personal projects to get a feel for the language .
You learn more by doing something then by reading lots of examples and other peoples code in those first few months .
A language like Ruby is structured in such a way you can do something productive straight out of the box without much support so jump right in and do something fun rather than think you have to do something for somebody else right at the beginning of your experimentation with a new language .
have fun with Ruby , it 's on my short list of things to play with : )How about running IE with the HTML favorites file as a parameter ?
IExplore file : //\windows\fav.htmI think we could also include non-.NET-specific approaches to parallel processing if those are among the best options to consider .Or you could start throwing critical exceptions .
Surely , an access violation exception will catch your users ' attention .Using SMO/DMO , it is n't too difficult to generate a script of your schema .
Data is a little more fun , but still doable .
In general , I take `` Script It '' approach , but you might want to consider something along these lines : Distinguish between Development and Staging , such that you can Develop with a subset of data ... this I would create a tool to simply pull down some production data , or generate fake data where security is concerned .
For team development , each change to the database will have to be coordinated amongst your team members .
Schema and data changes can be intermingled , but a single script should enable a given feature .
Once all your features are ready , you bundle these up in a single SQL file and run that against a restore of production .
Once your staging has cleared acceptance , you run the single SQL file again on the production machine .
I have used the Red Gate tools and they are great tools , but if you ca n't afford it , building the tools and working this way is n't too far from the ideal .@ Larsenal If you want to branch outside of .NET there has been a lot of discussion about Intel 's Threading Building Blocks which is a parallel library for C++ .I often encounter the following scenario where I need to offer many different types of permissions .
I primarily use ASP.NET / VB.NET with SQL Server 2000 .
Scenario I want to offer a dynamic permission system that can work on different parameters .
Let 's say that I want to give either a department or just a specific person access to an application .
And pretend that we have a number of applications that keeps growing .
In the past , I have chosen one of the following two ways that I know to do this .
1 ) Use a single permission table with special columns that are used for determining a how to apply the parameters .
The special columns in this example are TypeID and TypeAuxID .
The SQL would look something like this .
@ code1 @ 2 ) Use a mapping table for each type of permission , then joining them all together .
@ code2 @ My Thoughts I hope that the examples make sense .
I cobbled them together .
The first example requires less work , but neither of them feel like the best answer .
Is there a better way to handle this ?I 'm trying to do this ( which produces an unexpected T_VARIABLE error ) : @ code1 @ I do n't want to put a magic number in there for weight , since the object I am using has a `` defaultWeight '' parameter that all new shipments get if you do n't specify a weight .
I ca n't put the defaultWeight in the shipment itself , because it changes from shipment group to shipment group .
Is there a better way to do it than the following ?
@ code2 @Outside the scope of your question so I debated not posting this but in Java there are actually 2 types of exceptions , checked and unchecked .
The basic difference is that , much like in c [ ++ ] , you dont have to catch an unchecked exception .
For a good reference try thisThis is n't much better : public function createShipment ( $ startZip , $ endZip , $ weight=null ) { $ weight = ! $ weight ?
$ this- > getDefaultWeight ( ) : $ weight ; } // or ... public function createShipment ( $ startZip , $ endZip , $ weight=null ) { if ( ! $ weight ) $ weight = $ this- > getDefaultWeight ( ) ; }I 'm writing an AJAX app , but as the user moves through the app , I 'd like the URL in the address bar to update despite the lack of page reloads .
Basically , I 'd like for them to be able to bookmark at any point and thereby return to the current state .
How are people handling maintaining RESTfulness in AJAX apps ?The way I typically go about coding permission systems is having 6 tables .
Users - this is pretty straight forward it is your typical users table Groups - this would be synonymous to your departments Roles - this is a table with all permissions generally also including a human readable name and a description Users_have_Groups - this is a many-to-many table of what groups a user belongs to Users_have_Roles - another many-to-many table of what roles are assigned to an individual user Groups_have_Roles - the final many-to-many table of what roles each group has At the beginning of a users session you would run some logic that pulls out every role they have assigned , either directory or through a group .
Then you code against those roles as your security permissions .
Like I said this is what I typically do but your millage may vary .It has been a while , so this is not comprehensive .
Character Sets Unicode is great , but you ca n't get away with ignoring other character sets .
The default character set on Windows XP ( English ) is Cp1252 .
On the web , you do n't know what a browser will send you ( though hopefully your container will handle most of this ) .
And do n't be surprised when there are bugs in whatever implementation you are using .
Character sets can have interesting interactions with filenames when they move to between machines .
Translating Strings Translators are , generally speaking , not coders .
If you send a source file to a translator , they will break it .
Strings should be extracted to resource files ( e.g .
properties files in Java or resource DLLs in Visual C++ ) .
Translators should be given files that are difficult to break and tools that do n't let them break them .
Translators do not know where strings come from in a product .
It is difficult to translate a string without context .
If you do not provide guidance , the quality of the translation will suffer .
While on the subject of context , you may see the same string `` foo '' crop up in multiple times and think it would be more efficient to have all instances in the UI point to the same resource .
This is a bad idea .
Words may be very context-sensitive in some languages .
Translating strings costs money .
If you release a new version of a product , it makes sense to recover the old versions .
Have tools to recover strings from your old resource files .
String concatenation and manual manipulation of strings should be minimized .
Use the format functions where applicable .
Translators need to be able to modify hotkeys .
Ctrl+P is print in English ; the Germans use Ctrl+D .
If you have a translation process that requires someone to manually cut and paste strings at any time , you are asking for trouble .
Dates , Times , Calendars , Currency , Number Formats , Time Zones These can all vary from country to country .
A comma may be used to denote decimal places .
Times may be in 24hour notation .
Not everyone uses the Gregorian calendar .
You need to be unambiguous , too .
If you take care to display dates as MM/DD/YYYY for the USA and DD/MM/YYYY for the UK on your website , the dates are ambiguous unless the user knows you 've done it .
Especially Currency The Locale functions provided in the class libraries will give you the local currency symbol , but you ca n't just stick a pound ( sterling ) or euro symbol in front of a value that gives a price in dollars .
User Interfaces Layout should be dynamic .
Not only are strings likely to double in length on translation , the entire UI may need to be inverted ( Hebrew ; Arabic ) so that the controls run from right to left .
And that is before we get to Asia .
Testing Prior To Translation Use static analysis of your code to locate problems .
At a bare minimum , leverage the tools built into your IDE .
( Eclipse users can go to Window > Preferences > Java > Compiler > Errors/Warnings and check for non-externalised strings . )
Smoke test by simulating translation .
It is n't difficult to parse a resource file and replace strings with a pseudo-translated version that doubles the length and inserts funky characters .
You do n't have to speak a language to use a foreign operating system .
Modern systems should let you log in as a foreign user with translated strings and foreign locale .
If you are familiar with your OS , you can figure out what does what without knowing a single word of the language .
Keyboard maps and character set references are very useful .
Virtualisation would be very useful here .
Non-technical Issues Sometimes you have to be sensitive to cultural differences ( offence or incomprehension may result ) .
A mistake you often see is the use of flags as a visual cue choosing a website language or geography .
Unless you want your software to declare sides in global politics , this is a bad idea .
If you were French and offered the option for English with St. George 's flag ( the flag of England is a red cross on a white field ) , this might result in confusion for many English speakers - assume similar issues will arise with foreign languages and countries .
Icons need to be vetted for cultural relevance .
What does a thumbs-up or a green tick mean ?
Language should be relatively neutral - addressing users in a particular manner may be acceptable in one region , but considered rude in another .
Resources C++ and Java programmers may find the ICU website useful : http : //www.icu-project.org/Like Rob Allen , I use SQL Compare / Data Compare by Redgate .
I also use the Database publishing wizard by Microsoft .
I also have a console app I wrote in C # that takes a sql script and runs it on a server .
This way you can run large scripts with 'GO ' commands in it from a command line or in a batch script .
I use Microsoft.SqlServer.BatchParser.dll and Microsoft.SqlServer.ConnectionInfo.dll libraries in the console application .This is similar to what Kevin said .
You can have your client state as some javascript object , and when you want to save the state , you serialize the object ( using JSON and base64 encoding ) .
You can then set the fragment of the href to this string .
var encodedState = base64 ( json ( state ) ) ; var newLocation = oldLocationWithoutFragment + `` # '' + encodedState ; document.location = newLocation ; // adds new entry in browser historydocument.location.replace ( newLocation ) ; // replaces current entry in browser history The first way will treat the new state as a new location ( so the back button will take them to the previous location ) .
The latter does not .An approach I 've used in various applications is to have a generic PermissionToken class which has a changeable Value property .
Then you query the requested application , it tells you which PermissionTokens are needed in order to use it .
For example , the Shipping application might tell you it needs : new PermissionToken ( ) { Target = PermissionTokenTarget.Application , Action = PermissionTokenAction.View , Value = `` ShippingApp '' } ; This can obviously be extended to Create , Edit , Delete etc and , because of the custom Value property , any application , module or widget can define its own required permissions .
YMMV , but this has always been an efficient method for me which I have found to scale well .The way to do this is to manipulate location.hash when AJAX updates result in a state change that you 'd like to have a discreet URL .
For example , if your page 's url is : http : //example.com/ If a client side function executed this code : @ code1 @ Then , the URL displayed in the browser would be updated to : http : //example.com/ # foo This allows users to bookmark the `` foo '' state of the page , and use the browser history to navigate between states .
With this mechanism in place , you 'll then need to parse out the hash portion of the URL on the client side using JavaScript to create and display the appropriate initial state , as fragment identifiers ( the part after the # ) are not sent to the server .
Ben Alman 's hashchange plugin makes the latter a breeze if you 're using jQuery .Is there a way one can ensure that the exceptions thrown are always caught using try/catch by the calling function ?
I find it rather funny , that the Java crowd - including myself - is trying to avoid checked Exceptions .
They are trying to work their way around being forced to catch Exceptions by using RuntimeExceptions .How do you express an integer as a binary number with Python literals ?
I was easily able to find the answer for hex : @ code1 @ and , octal : @ code2 @ How do you use literals to express binary in Python ?
Summary of Answers Python 2.5 and earlier : can express binary using int ( '01010101111',2 ) but not with a literal .
Python 2.5 and earlier : there is no way to express binary literals .
Python 2.6 beta : You can do like so : 0b1100111 or 0B1100111 .
Python 2.6 beta : will also allow 0o27 or 0O27 ( second character is the letter O ) to represent an octal .
Python 3.0 beta : Same as 2.6 , but will no longer allow the older 027 syntax for octals .I agree with John Downey .
Personally , I sometimes use a flagged enumeration of permissions .
This way you can use AND , OR , NOT and XOR bitwise operations on the enumeration 's items .
`` [ Flags ] public enum Permission { VIEWUSERS = 1 , // 2^0 // 0000 0001 EDITUSERS = 2 , // 2^1 // 0000 0010 VIEWPRODUCTS = 4 , // 2^2 // 0000 0100 EDITPRODUCTS = 8 , // 2^3 // 0000 1000 VIEWCLIENTS = 16 , // 2^4 // 0001 0000 EDITCLIENTS = 32 , // 2^5 // 0010 0000 DELETECLIENTS = 64 , // 2^6 // 0100 0000 } '' Then , you can combine several permissions using the AND bitwise operator .
For example , if a user can view & edit users , the binary result of the operation is 0000 0011 which converted to decimal is 3 .
You can then store the permission of one user into a single column of your DataBase ( in our case it would be 3 ) .
Inside your application , you just need another bitwise operation ( OR ) to verify if a user has a particular permission or not .As far as I can tell Python , up through 2.5 , only supports hexadecimal & octal literals .
I did find some discussions about adding binary to future versions but nothing definite .I am pretty sure this is one of the things due to change in Python 3.0 with perhaps bin ( ) to go with hex ( ) and oct ( ) .
EDIT : lbrandy 's answer is correct in all cases .@ code1 @ Another way .In addition to John Downey and jdecuyper 's solutions , I 've also added an `` Explicit Deny '' bit at the end/beginning of the bitfield , so that you can perform additive permissions by group , role membership , and then subtract permissions based upon explicit deny entries , much like NTFS works , permission-wise .In general this is a very subtle issue and note trivial whatsoever .
I encourage you to read mysqlperformanceblog.com and High Performance MySQL .
I really think there is no general answer for this .
I 'm working on a project which has a MySQL database with almost 1TB of data .
The most important scalability factor is RAM .
If the indexes of your tables fit into memory and your queries are highly optimized , you can serve a reasonable amount of requests with a average machine .
The number of records do matter , depending of how your tables look like .
It 's a difference to have a lot of varchar fields or only a couple of ints or longs .
The physical size of the database matters as well : think of backups , for instance .
Depending on your engine , your physical db files on grow , but do n't shrink , for instance with innodb .
So deleting a lot of rows , does n't help to shrink your physical files .
There 's a lot to this issues and as in a lot of cases the devil is in the details .
FDX5BX3I have to agree with the OP 'wrong ' dates really jar with my DD/MM/YYYY upbringing and I find ISO 8601 dates and times extremely easy to work with .
For once the standard got it right and engtech has the obvious answer that does n't require localisation .
I was going to report the birthday input form on stack overflow as a bug because of how much of a sore thumb it is to the majority of the world .For those of us that like to use the graphical version of Vim or Emacs , instead of the console version , which version do you recommend ?
For Vim , there 's Mac OS X Vim , MacVim , Vim-Cocoa .
For Emacs , CarbonEmacs , XEmacs , and Aquamacs .
Are there more ?
Which of these are ready for prime-time ?
If it 's a tough call , what are the trade-offs ?
Are all of these still being maintained ?
No discussion of Vim vs. Emacs , if you do n't mind , or comparisons with other editors .I 'm looking into using Visual Studio 2008 's built in unit test projects instead of NUnit and I was wondering if anyone has any experience in trying to integrate this type of unit test project with Cruise Control.Net .Also watch out for complex joins .
Transaction complexity can be a big factor in addition to transaction volume .
Refactoring heavy queries sometimes offers a big performance boost .How do I set the icon that appears on the iPhone for the web sites I create ?From the Apple Developer Connection Safari Web Content Guide for iPhone page Specifying a Webpage Icon for Web Clip ...
The user can add a web application or webpage link to the Home screen .
These links , represented by an icon , are called web clips .
Follow these simple steps to specify an icon to represent your web application or webpage on iPhone .
To specify an icon for the entire website ( every page on the website ) , place an icon file in PNG format in the root document folder called apple-touch-icon.png .
To specify an icon for a single webpage , or replace the website icon with a webpage-specific icon , add a link element to the webpage as in : < link rel= '' apple-touch-icon '' href= '' /custom_icon.png '' / > In the above example , replace custom_icon.png with your icon filename .
See `` Create an Icon for Your Web Application or Webpage '' in iPhone Human Interface Guidelines in iPhone Human Interface Guidelines for webpage icon metrics .
Note : The web clip feature is available in iPhone 1.1.3 and later .My office has a central Source Safe 2005 install that we use for source control .
I ca n't change what the office uses on the server .
I develop on a laptop and would like to have a different local source control repository that can sync with the central server ( when available ) regardless of the what that central provider is .
The reason for the request is so I can maintain a local stable branch/build for client presentations while continuing to develop without having to jump through flaming hoops .
Also , as a consultant , my clients may request that I use their source control provider and flexibility here would make life easier .
Can any of the existing distributed source control clients handle that ?And for the sake of completeness , a link to Scott Hanselman 's posting , which contains some additional tips as well : Add Home Screen iPhone Icons and Adjust the ViewPortFrom some of the initial research it does n't appear to be a super simple solution .
It appears that doing this involves having Visual Studio 2008 actually installed on the continuous integration server , which could be a deal breaker .
Then configure the MSTest.exe to run in the tasks list , but first you 'll have to make a batch file to delete the results files from previous passes as this file 's existence causes an error .
Then create a xslt to format the results and put it into the dashboard.config file .
The code project article I found has a lot more detail .
Integrating Visual Studio Team System 2008 Unit Tests with CruiseControl.NETYou should be able to check out the current version of the code and then create a git repository around it .
Updating that and committing it to your local git repository should be painless .
As should cloning it .
The only catch is that you need to have them both ignore each other ( I 've done something similar with SVN ) by messing with the appropriate ignore files .
I 'm presuming SourceSafe let 's you ignore things .
And you 'll need to do certain operations twice ( like telling both that you are deleting a file ) .MacVim works well and certainly looks more mature than Vim-Cocoa , moreover there is a Cocoa plugin architecture in the pipeline for MacVim ( and someone is already working on a TextMate style file browser tray plugin which is a huge ++ IMHO ) .
There was also a Carbon version of Vim , but this did n't offer a great deal over the Terminal version .
i.e .
only allowed one window open , not very OSX in appearance ... Aquamacs is very usable and looks pretty good .
Supports both traditional Mac OS style keyboard shortcuts ( command-O , command-S ) and the Control/Meta shortcuts for those raised on traditional Emacs .
It is definitely more Mac-like than Carbon Emacs .
It seems stable and fast , but I am not an Emacs guru so I do n't stress it all that much when I use it .
I ca n't speak to the extensiveness of the included elisp packages , either .
Someone syncs Carbon Emacs with the upstream tree quarterly I think .
Aquamacs has a more irregular schedule , but it 's seen some pretty major updates over the last year .
GNU Emacs for OSX can be found at emacsformacosx.com .
In addition to the latest stable release , there are also pre-release test builds and nightly builds , and Atom feeds are provided for tracking all three release types .I 've made a suggestion that Stack Overflow implement an apple-touch-icon : Add an apple-touch-icon for Safari on iPhoneA short form is convenient and helps avoid spelling mistakes .
Localize as applicable , but be sure to display the expected format ( do not leave the user blind ) .
Provide a date-picker control as an optional aide to filling in the field .
As an extra , on-the-fly parsing and display of the date in long form might help too .Well ... KernelTrap has something on this .
Looks like you can use vss2svn to pipe the Source Safe repo into a Subversion repository , then use the very nice git-svn to pull into a local git repo .
I would assume the commits back to VSS would not be a smooth , automatic process using this method .While I would still like an answer to why my JS was n't being recognized , the solution I found in the meantime ( and should have done in the first place ) is to use an Asp : CompareValidator instead of an Asp : CustomValidator .I 'm looking for some way to effectively hide inherited members .
I have a library of classes which inherit from common base classes .
Some of the more recent descendant classes inherit dependency properties which have become vestigial and can be a little confusing when using IntelliSense or using the classes in a visual designer .
These classes are all controls that are written to be compiled for either WPF or Silverlight 2.0 .
I know about ICustomTypeDescriptor and ICustomPropertyProvider , but I 'm pretty certain those ca n't be used in Silverlight .
It 's not as much a functional issue as a usability issue .
What should I do ?
Update Some of the properties that I would really like to hide come from ancestors that are not my own and because of a specific tool I 'm designing for , I ca n't do member hiding with the new operator .
( I know , it 's ridiculous )While you can not prevent usage of those inherited members to my knowledge , you should be able to hide them from IntelliSense using the EditorBrowsableAttribute : @ code1 @ Edit : Just saw this in the documentation comments , which makes it kinda useless for this purpose : There is a prominent note that states that this attribute `` does not suppress members from a class in the same assembly '' .
That is true but not complete .
Actually , the attribute does not suppress members from a class in the same solution .I think you 're best least hackish way is to consider composition as opposed to inheritance .
Or , you could create an interface that has the members you want , have your derived class implement that interface , and program against the interface .I 've never been completely happy with the way exception handling works , there 's a lot exceptions and try/catch brings to the table ( stack unwinding , etc .
) , but it seems to break a lot of the OO model in the process .
Anyway , here 's the problem : Let 's say you have some class which wraps or includes networked file IO operations ( e.g .
reading and writing to some file at some particular UNC path somewhere ) .
For various reasons you do n't want those IO operations to fail , so if you detect that they fail you retry them and you keep retrying them until they succeed or you reach a timeout .
I already have a convenient RetryTimer class which I can instantiate and use to sleep the current thread between retries and determine when the timeout period has elapsed , etc .
The problem is that you have a bunch of IO operations in several methods of this class , and you need to wrap each of them in try-catch / retry logic .
Here 's an example code snippet : RetryTimer fileIORetryTimer = new RetryTimer ( TimeSpan.FromHours ( 10 ) ) ; bool success = false ; while ( ! success ) { try { // do some file IO which may succeed or fail success = true ; } catch ( IOException e ) { if ( fileIORetryTimer.HasExceededRetryTimeout ) { throw e ; } fileIORetryTimer.SleepUntilNextRetry ( ) ; } } So , how do you avoid duplicating most of this code for every file IO operation throughout the class ?
My solution was to use anonymous delegate blocks and a single method in the class which executed the delegate block passed to it .
This allowed me to do things like this in other methods : this.RetryFileIO ( delegate ( ) { // some code block } ) ; I like this somewhat , but it leaves a lot to be desired .
I 'd like to hear how other people would solve this sort of problem .One potential thing you can do is contain the object rather than extend from the other class .
This will give you the most flexibility in terms of exposing what you want to expose , but if you absolutely need the object to be of that type it is not the ideal solution ( however you could expose the object from a getter ) .
Thus : public class MyClass : BaseClass { // Your stuff here } Becomes : public class MyClass { private BaseClass baseClass ; public void ExposeThisMethod ( ) { baseClass.ExposeThisMethod ( ) ; } } Or : public class MyClass { private BaseClass baseClass ; public BaseClass BaseClass { get { return baseClass ; } } }If you use Visual Studio you can use Visual C++ 2008 Feature Pack Release , this implements some of TR1 , and includes regular expression parsing .
Get itSome time ago I wrote a quick and dirty exe that would update the version # 's in an assemblyinfo .
{ cs/vb } - I also have used rxfind.exe ( a simple and powerful regex-based search replace tool ) to do the update from a command line as part of the build process .
A couple of other helpfule hints : separate the assemblyinfo into product parts ( company name , version , etc . )
and assembly specific parts ( assembly name etc . ) .
See here Also - i use subversion , so I found it helpful to set the build number to subversion revision number thereby making it really easy to always get back to the codebase that generated the assembly ( e.g .
1.4.100.1502 was built from revision 1502 ) .Just wondering , what do you feel your method leaves to be desired ?
You could replace the anonymous delegate with a.. named ?
delegate , something like public delegate void IoOperation ( params string [ ] parameters ) ; public void FileDeleteOperation ( params string [ ] fileName ) { File.Delete ( fileName [ 0 ] ) ; } public void FileCopyOperation ( params string [ ] fileNames ) { File.Copy ( fileNames [ 0 ] , fileNames [ 1 ] ) ; } public void RetryFileIO ( IoOperation operation , params string [ ] parameters ) { RetryTimer fileIORetryTimer = new RetryTimer ( TimeSpan.FromHours ( 10 ) ) ; bool success = false ; while ( ! success ) { try { operation ( parameters ) ; success = true ; } catch ( IOException e ) { if ( fileIORetryTimer.HasExceededRetryTimeout ) { throw ; } fileIORetryTimer.SleepUntilNextRetry ( ) ; } } } public void Foo ( ) { this.RetryFileIO ( FileDeleteOperation , `` L : \file.to.delete '' ) ; this.RetryFileIO ( FileCopyOperation , `` L : \file.to.copy.source '' , `` L : \file.to.copy.destination '' ) ; }Override them like Michael Suggests above and to prevent folks from using the overridden ( sp ? )
methods , mark them as obsolete : [ Obsolete ( `` These are not supported in this class .
`` , true ) ] public override void dontcallmeanymore ( ) { } If the second parm is set to true , a compiler error will be generated if anyone tries to call that method and the string in the first parm is the message .
If parm2 is false only a compiler warning will be generated .I 've tried Aquamacs and it 's very usable and looks pretty good .
Supports both traditional Mac OS style keyboard shortcuts ( command-O , command-S ) and the Control/Meta shortcuts for those raised on traditional Emacs .
It is definitely more Mac-like than Carbon Emacs .
It seems stable and fast , but I am not an Emacs guru so I do n't stress it all that much when I use it .
I ca n't speak to the extensiveness of the included elisp packages , either .
Someone syncs Carbon Emacs with the upstream tree quarterly I think .
Aquamacs has a more irregular schedule , but it 's seen some pretty major updates over the last year .We 've finally moved our websites to a decent host , and for the first time we have shell access .
I know very little about using Linux .
I can navigate through the file system , read files with Vim and I 'm aware of the man command , and I have been able to work out solutions to problems as they show up ( eventually ) , but I know I 'm unaware of a lot .
We currently only use the host to hold our live sites , I 'm sure that we use it more effectively , but I 'm not sure where to start .
So with web development in mind : What are the essential commands that every Linux user should know about ?
What are the most useful commands that I should look into ?If you only have shell access to your host , a number of issues are already taken care of for you , ( you do n't have to maintain the system yourself ) .
The useful commands depend on what you primarily want to do , such as interacting with your source control system via command line ( you do use source control , do n't you ? )
You already know how to use vim and navigate through the filesystem using cd and ls , so that is a great start .
Most useful commands : ls list files in current directory ( like Windows dir ) cd change directory cp copying file ( s ) example : $ > cp { file1 } { file2 } $ > cp /home/jms/file1.txt /home/jms/file1-copy.txt mv moving or renaming file ( s ) example - rename file1.txt : @ code1 @ example - move file1.txt : @ code2 @ man see the manual pages for a command example : $ > man woman $ > Segmentation fault ( core dumped ) find search through directories recursively ( and optionally perform some action for each match ) grep search for pattern matches wc word count / character count / line count example : counting the files in a the current directory ( uses ls and wc ) @ code3 @ example : count the files that contain .txt in your home directory ( uses find , grep , and wc ) @ code4 @ less lightweight file viewer head see the first few lines of a file tail see the last few lines of a file ( useful for realtime logging ) example : monitor a logfile as logging occurs while an application is running @ code5 @ passwd change your password example : will act on current user and prompt for old/new password @ code6 @ example : will change password for the user named someuser @ code7 @ ssh secure shell for logging into remote systems touch set file `` last modified '' time to now ( creates a new file if none exists ) rm remove a file can also remove files and directories recursively mkdir / rmdir create or remove a directory df check free disk space on volumes du check used disk space on a directory ( recursively ) ln make a new file/directory that is a `` link '' to another ( such as a symbolic link ) example ( symbolic link ) : $ > ln -s /path/to/destination kill kill/stop a running process chmod , chown change permissions / ownership for files .
sudo run a command with superuser ( ie `` root '' ) privileges your web host may not give you permission to do this vi a text editor included with every linux installation A number of these items you will have an easier time learning by experimentation .
A very comprehensive guide to bash scripting might also be of use .Find out the distribution they run ( for example , Ubuntu ) and focus on that first .
Use VirtualBox or another VM to play around , especially networking and the daemons that your host is using .less [ file name ] - shows contents of file screen by screen .
I 'd recommend that over reading the file through Vim .
Also , Pico and Emacs are a little more friendly than Vim for noobs .Let 's see ...
The basics : cd , ls , mv , cp , mkdir , rm , cat , and man .
The good ones : ps , chmod , chgrp , sudo ( or su ) , pwd , grep , head , tail , xargs , less , find , awk , sed , ssh/scp , ping , touch , locate/updatedb , ln , kill , echo , wc , passwd , tar , diff , df , du , free , and reboot And the operators : | , > , > > , & & , and || ... and all the useful associated command line options therein .
I could probably come up with another 20 if I thought about it more .This has already been answered for Java , here 's the C # answer : '' Integer '' is not a valid type name in C # and `` int '' is just an alias for System.Int32 .
Also , unlike in Java ( or C++ ) there are n't any special primitive types in C # , every instance of a type in C # ( including int ) is an object .
Here 's some demonstrative code : void DoStuff ( ) { System.Console.WriteLine ( SomeMethod ( ( int ) 5 ) ) ; System.Console.WriteLine ( GetTypeName < int > ( ) ) ; } string SomeMethod ( object someParameter ) { return string.Format ( `` Some text { 0 } '' , someParameter.ToString ( ) ) ; } string GetTypeName < T > ( ) { return ( typeof ( T ) ) .FullName ; }As a LAMP developer considering moving to a .Net IIS platform , one of my concerns is the loss of productivity due to lack of shell ... Has anyone else had this experience ?
Is there possibly a Linux shell equivalent for Windows ?I assume you do n't mean cygwin , right ?
How about powershell , then ?more/less : view a file in your command prompt .
more myfile.txt grep : find keywords using regular expressions in a file , group of files or recursively in all files grep hello myfile.txtgrep hello *.txtgrep -r hello .
understand pipe ( the '| ' character ) ... it lets you chain things together nicely .
grep -r hello .
| grep txt : ( will recursively grep for hello , then grep those results for `` txt : '' which if you look at grep output , you will see the above means it was found in a file ending in txt ) su will let you `` switch user '' , with no argument will be to root , with an argument will be to another user sudo will let you execute a command as root , if you are on the sudo list sudo more /path/to/root/only/accessible.file Those are the first few I can think of off the top of my head that should prove really helpful , but I 'm sure there are many more that are useful .Xdebug , by Derick Rethans , is very good .
I used it some time ago and found it was not so easy to install .
Once you 're done , you wo n't understand how you managed without it : - ) There is a good article on Zend Developer Zone ( installing on Linux does n't seem any easier ) and even a Firefox plugin , which I never used .Are you asking about Linux shell as in an environment to work in ?
For that CygWin I think has been around the longest and is pretty robust : http : //www.cygwin.com/ A while ago I found a windows port of all the popular linux commands I use ( ls , grep , diff ) and I simply unzip those to a file , add it to my PATH environment and then can run from there : http : //unxutils.sourceforge.net/ Or are you talking about executing shell commands from within your code ?
If you 're in the .net sphere , there is the Process.Start ( ) method that will give you a lot of options .
Hope this helps !Treebeard 's Unix Cheat SheetThis article on linuxguide.it is a fairly comprehensive list of commands but here are a few of the ones I find I use most frequently : ls -la ( lists the contents of the current directory , including hidden files ) tar cvzf output_file.tar.gz directory/ ( tars and gzips all of the files in directory/ into output_file.tar.gz ) tar cvzf file.tar.gz ( unzips and untars the file , restoring the original directory structure and keeping all permissions intact ) man binary ( displays the man page for binary ) chmod ( changes the permissions on a specified file , use man chmod for more details ) Hope that helped at least a little bit , check out the link above for a better list .Do n't forget you can pipe things to the more command which will give you a single page of output and prompt you to continue .
I use this most often with ls -la | more ( emulates the standard dir command ) and I also sometimes pipe ( | ) grep searches to more .If you 're referring to simply accessing your IIS server from a remote location , remote desktop generally solves that problem .
Assuming your server has a static IP address or a host name you can access from the internet , remote desktop is a simple and relatively secure solution .
Is there a problem with this answer ?
Now I have negative reputation ...To capatilise it in , say , C - use the ascii codes ( http : //www.asciitable.com/ ) to find the integer value of the char and subtract 32 from it .
This is a poor solution if you ever plan to accept characters beyond a-z and A-Z .
For instance : ASCII 134 : å , ASCII 143 : Å .
Using arithmetic gets you : ASCII 102 : f Use library calls , do n't assume you can use integer arithmetic on your characters to get back something useful .
Unicode is tricky .Seeing that this seems to be a web host , you will probably want to know : How to start/stop/restart the webserver ( Distro/server dependent .
Try /etc/init.d/apache restart to restart . )
How to check the logs ( Distro/server dependent .
Try less /var/www/apache.log ) How to access MySQL directly ( mysql -u myusername -p and mysqladmin ) How to upload/download files ( Probably using SFTP on the client end ) How to edit the webserver configuration ( Probably nano /etc/apache2/httpd.conf ) How to check/edit UNIX permissions and ownership ( ls -l to check and chmod XXXX files and chown newowner.newgroup files to change ) These are all using complete guesses for file locations etc .
and assume that you are using Apache .
A bit of investigation is probably needed for your particular setup .
A special mention goes to permissions .
This causes much of the headaches with running CGI 's and security .
Below is a rough guide to what permissions should be set .
PHP files : readable by web-server ( chmod 640 filename.php ) CGI scripts : executable by webserver ( chmod 750 filename.cgi ) Static web files : readable by webserver ( chmod 640 filename.html ) Directories : executable by webserver ( chmod 750 directoryname ) These settings assume that the webserver process is running as a user who belongs to the same group as the webserver files ( this is likely the setup on a managed host ) .
A final headache that may cause trouble is that Linux is case sensitive .
When serving static files from Apache , by default you will need to include any weird capitalisation .
It 's generally a good idea to stick to lower-case and underscores/hyphens for naming directories and files .I 'm not sure what the correct size should be .
Many sites seem to repeat that the apple-touch-icon should be 57x57 pixels but cite a broken link as their source .
Hanselman 's and playgroundblues 's comments suggest different sizes including 163x163 and 60x60 .
Apple 's own apple.com icon is 129x129 !
See my related question : How do I give my web sites an icon for iPhone ?I do n't think there is a `` correct size '' .
Since the iPhone really is running OSX , the icon rendering system is pretty robust .
As long as you give it a high-quality image with the right aspect ratio and a resolution at least as high as the actual output will be , the OS will downscale very cleanly .
My site uses a 158x158 and the icon looks pixel-perfect on the iPhone screen .I always create a new empty database , after that backup and restore of the existing database into it , but is this really the best way ?
As it seems very error prone and over complicated for me .Fosswire Unix/Linux Command Reference My advice would be to install a distribution ( or use a LiveCD ) on one of your own machines and play around with it first , and then , once you 're confident enough to move on fiddling with the production server .In all honesty , a combination of print and print_r ( ) to print out the variables .
I know that many prefer to use other more advanced methods but I find this the easiest to use .
I will say that I did n't fully appreciate this until I did some Microprocessor programming at Uni and was not able to use even this .This is a little more broad range & general of a suggestion but I found Unix System V : A Practical Guide ( Mark G. Sobell ) was a great ... I repeat ... great book to learn the basics from ( albeit it was my academic textbook ) .
Like I said , you might be more interested in something a little more distro-specific , but for the general Unix goods I ca n't recommend this reference enough .What are the best methods for tracking and/or automating DB schema changes ?
Our team uses Subversion for version control and we 've been able to automate some of our tasks this way ( pushing builds up to a staging server , deploying tested code to a production server ) but we 're still doing database updates manually .
I would like to find or create a solution that allows us to work efficiently across servers with different environments while continuing to use Subversion as a backend through which code and DB updates are pushed around to various servers .
Many popular software packages include auto-update scripts which detect DB version and apply the necessary changes .
Is this the best way to do this even on a larger scale ( across multiple projects and sometimes multiple environments and languages ) ?
If so , is there any existing code out there that simplifies the process or is it best just to roll our own solution ?
Has anyone implemented something similar before and integrated it into Subversion post-commit hooks , or is this a bad idea ?
While a solution that supports multiple platforms would be preferable , we definitely need to support the Linux/Apache/MySQL/PHP stack as the majority of our work is on that platform .The best way I can think of would be to use Cygwin over an OpenSSH connection .
Here 's a document that explains how to do just that : http : //www.ucl.ac.uk/cert/openssh_rdp_vnc.pdfDepends on how much detail you want it to have , it needs to have the aspect ratio of 1:1 ( basically - it needs to be square ) I would go with the Apple 's own 129*129If I 'm adding a column to a table in Microsoft SQL Server , can I control where the column is displayed logically in queries ?
I do n't want to mess with the physical layout of columns on disk , but I would like to logically group columns together when possible so that tools like SQL Server Management Studio list the contents of the table in a convenient way .
I know that I can do this through SQL Management Studio by going into their `` design '' mode for tables and dragging the order of columns around , but I 'd like to be able to do it in raw SQL so that I can perform the ordering scripted from the command line .I would check both IP and User Agent to see if they change if ( $ _SESSION [ 'user_agent ' ] ! = $ _SERVER [ 'HTTP_USER_AGENT ' ] || $ _SESSION [ 'user_ip ' ] ! = $ _SERVER [ 'REMOTE_ADDR ' ] ) { //Something fishy is going on here ? }The .XFDL file extension identifies XFDL Formatted Document files .
These belong to the XML-based document and template formatting standard .
This format is exactly like the XML file format however , contains a level of encryption for use in secure communications .
I know how to view XFDL files using a file viewer I found here .
I can also modify and save these files by doing File : Save/Save As .
I 'd like , however , to modify these files on the fly .
Any suggestions ?
Is this even possible ?
Update # 1 : I have now successfully decoded and unziped a .xfdl into an XML file which I can then edit .
Now , I am looking for a way to re-encode the modified XML file back into base64-gzip ( using Ruby or the command line )It is possible to skip the step of creating the empty database .
You can create the new database as part of the restore process .
This is actually the easiest and best way I know of to clone a database .
You can eliminate errors by scripting the backup and restore process rather than running it through the SQL Server Management Studio There are two other options you could explore : Detach the database , copy the .mdf file and re-attach .
Use SQL Server Integration Services ( SSIS ) to copy all the objects over I suggest sticking with backup and restore and automating if necessary .MySQLdb is what I have used before .
If you host is using Python version 2.5 or higher , support for sqlite3 databases is built in ( sqlite allows you to have a relational database that is simply a file in your filesystem ) .
But buyer beware , sqlite is not suited for production , so it may depend what you are trying to do with it .
Another option may be to call your host and complain , or change hosts .
Honestly these days , any self respecting web host that supports python and mysql ought to have MySQLdb pre installed .@ Mark Harrison : SQL Server does not support that syntax ( SELECT ... FOR UPDATE ) .
The SQL Server equivalent is the SELECT statement hint UPDLOCK .
See SQL Server Books Online for more information .You can not do this programatically ( in a safe way that is ) without creating a new table .
What Enterprise Manager does when you commit a reordering is to create a new table , move the data and then delete the old table and rename the new table to the existing name .
If you want your columns in a particular order/grouping without altering their physical order , you can create a view which can be whatever you desire .When spliting a solution in to logical layers , when is it best to use a separate project over just grouping by a folder ?When Management Studio does it , it 's creating a temporary table , copying everything across , dropping your original table and renaming the temporary table .
There 's no simple equivalent T-SQL statement .
If you do n't fancy doing that , you could always create a view of the table with the columns in the order you 'd like and use that ?
Edit : beaten !What software is recommended for working with and editing large XML schemas ?
I 'm looking for both Windows and Linux software ( does n't have to be cross platform , just want suggestions for both ) that help with dealing with huge XML files .I may be old fashioned , but I prefer my text editor .
I use emacs , and it has a fairly decent xml mode .
Most good text editors will have decent syntax hi-lighting and tag matching facilities .
Your IDE might already do it ( IntelliJ idea does , and I believe Eclipse does as well ) .
Good text editors will be able to deal with huge files , but some text editors may not be able to handle them .
How big are we talking about ?If the encoding is base64 then this is the solution I 've stumbled upon on the web link : '' Decoding XDFL files saved with 'encoding=base64 ' .
Files saved with : application/vnd.xfdl ; content-encoding= '' base64-gzip '' are simple base64-encoded gzip files .
They can be easily restored to XML by first decoding and then unzipping them .
This can be done as follows on Ubuntu : sudo apt-get install uudeview uudeview -i yourform.xfdl gunzip -S `` '' < UNKNOWN.001 > yourform-unpacked.xfdl The first command will install uudeview , a package that can decode base64 , among others .
You can skip this step once it is installed .
Assuming your form is saved as 'yourform.xfdl ' , the uudeview command will decode the contents as 'UNKNOWN.001 ' , since the xfdl file does n't contain a file name .
The '-i ' option makes uudeview uninteractive , remove that option for more control .
The last command gunzips the decoded file into a file named 'yourform-unpacked.xfdl'. ''
Another possible solution - here Side Note : Block quoted < code > does n't work for long strings of codeI usually do a project for the GUI a project for the business logic a project for data access and a project for unit tests .
But sometimes it is prudent to have separation based upon services ( if you are using a service oriented architecture ) Such as Authentication , Sales , etc .
I guess the rule of thumb that I work off of is that if you can see it as a component that has a clear separation of concerns then a different project could be prudent .
But I would think that folders versus projects could just be a preference or philosophy .
I personally feel that if reusable code is split into projects it is simpler to use other places than if it is just in folders .Yes , Podcasts , those nice little Audiobooks I can listen to on the way to work .
With the current amount of Podcasts , it 's like searching a needle in a haystack , except that the haystack happens to be the Internet and is filled with too many of these `` Hot new Gadgets '' stuff : ( Now , even though I am mainly a .NET developer nowadays , maybe anyone knows some good Podcasts from people regarding the whole software lifecycle ?
Unit Testing , Continous Integration , Documentation , Deployment ...
So - what are you guys and gals listening to ?
Please note that the categorizations are somewhat subjective and may not be 100 % accurate as many podcasts cover several areas .
Categorization is made against what is considered the `` main '' area .
General Software Engineering / Productivity Stack Overflow ( inactive , but still a good listen ) TekPub ( Requires Paid Subscription ) Software Engineering Radio 43 Folders Perspectives Dr. Dobb 's ( now a video feed ) The Pragmatic Podcast ( Inactive ) IT Matters Agile Toolkit Podcast The Stack Trace ( Inactive ) Parleys Techzing The Startup Success Podcast Berkeley CS class lectures FLOSS Weekly This Developer 's Life .NET / Visual Studio / Microsoft Herding Code Hanselminutes .NET Rocks !
Deep Fried Bytes Alt.Net Podcast ( inactive ) Polymorphic Podcast ( inconsistent ) Sparkling Client ( The Silverlight Podcast ) dnrTV !
Spaghetti Code ASP.NET Podcast Channel 9 Radio TFS PowerScripting Podcast The Thirsty Developer Elegant Code ( inactive ) ConnectedShow Crafty Coders Coding QA jQuery yayQuery The official jQuery podcast Java / Groovy The Java Posse Grails Podcast Java Technology Insider Basement Coders Ruby / Rails Railscasts Rails Envy The Ruby on Rails Podcast Rubiverse Ruby5 Web Design / JavaScript / Ajax WebDevRadio Boagworld The Rissington podcast Ajaxian YUI Theater Unix / Linux / Mac / iPhone Mac Developer Network Hacker Public Radio Linux Outlaws Mac OS Ken LugRadio Linux radio show ( Inactive ) The Linux Action Show !
Linux Kernel Mailing List ( LKML ) Summary Podcast Stanford 's iPhone programming class Advanced iPhone Development Course - Madison Area Technical College WWDC 2010 Session Videos ( requires Apple Developer registration ) System Administration , Security or Infrastructure RunAs Radio Security Now !
Crypto-Gram Security Podcast Hak5 VMWare VMTN Windows Weekly PaulDotCom Security The Register - Semi-Coherent Computing FeatherCast General Tech / Business Tekzilla This Week in Tech The Guardian Tech Weekly PCMag Radio Podcast ( Inactive ) Entrepreneurship Corner Manager Tools Other / Misc .
/ Podcast Networks IT Conversations Retrobits Podcast No Agenda Netcast Cranky Geeks The Command Line Freelance Radio IBM developerWorks The Register - Open Season Drunk and Retired Technometria Sod This Radio4Nerds Hacker MedleyHere 's a dynamic sql script I 've used in the past .
It can be further modified but it will give you the basics .
I prefer scripting it to avoid the mistakes you can make using the Management Studio : Declare @ OldDB varchar ( 100 ) Declare @ NewDB varchar ( 100 ) Declare @ vchBackupPath varchar ( 255 ) Declare @ query varchar ( 8000 ) /*Test code to implement Select @ OldDB = 'Pubs'Select @ NewDB = 'Pubs2'Select @ vchBackupPath = '\\dbserver\C $ \Program Files\Microsoft SQL Server\MSSQL.1\MSSQL\Backup\pubs.bak'*/SET NOCOUNT ON ; Select @ query = 'Create Database ' + @ NewDBexec ( @ query ) Select @ query = 'Declare @ vBAKPath varchar ( 256 ) declare @ oldMDFName varchar ( 100 ) declare @ oldLDFName varchar ( 100 ) declare @ newMDFPath varchar ( 100 ) declare @ newLDFPath varchar ( 100 ) declare @ restQuery varchar ( 800 ) select @ vBAKPath = ' '' + @ vchBackupPath + ' '' select @ oldLDFName = name from ' + @ OldDB +'.dbo.sysfiles where filename like '' % .ldf % ''select @ oldMDFName = name from ' + @ OldDB +'.dbo.sysfiles where filename like '' % .mdf % ''select @ newMDFPath = physical_name from ' + @ NewDB +'.sys.database_files where type_desc = ''ROWS '' select @ newLDFPath = physical_name from ' + @ NewDB +'.sys.database_files where type_desc = ''LOG '' select @ restQuery = ''RESTORE DATABASE ' + @ NewDB + ' FROM DISK = N '' + ' '' ' '' '' + @ vBAKpath + ' '' ' '' '' + '' WITH MOVE N '' + ' '' ' '' '' + @ oldMDFName + ' '' ' '' '' + '' TO N '' + ' '' ' '' '' + @ newMDFPath + ' '' ' '' '' + '' , MOVE N '' + ' '' ' '' '' + @ oldLDFName + ' '' ' '' '' + '' TO N '' + ' '' ' '' '' + @ newLDFPath + ' '' ' '' '' + '' , NOUNLOAD , REPLACE , STATS = 10 '' exec ( @ restQuery ) -- print @ restQuery'exec ( @ query )The only answer I can think of right now is - read the manual for uudeview .
As much as I would like to help you , I am not an expert in this area , so you 'll have to wait for someone more knowledgable to come down here and help you .
Meanwhile I can give you links to some documents that might help you : UUDeview Home Page Using XDFLengine Gettting started with the XDFL Engine Sorry if this does n't help you .If you are into web design and website creation then I recommend Boagworld and also The Rissington podcast even if you are not .I agree that your text editor is probably your best bet .
I do know some people who swear by XMLSpy , if you need something that 's tailored specifically for dealing with XML files in a visual way .
I bet you could find some F/OSS work-alikse but I 'm not aware of any .The oXygen XML Editor a great IDE for Windows , bit expensive tho .In the Stack Overflow podcast SE-radio was mentioned .
It 's pretty in depth .
Also if you are an aspiring JavaScript developer , the Douglas Crockford `` The JavaScript Programming Language '' and `` Advanced JavaScript '' talks on YUI Developer Theatre are excellent .
There are a few other gems on the podcast too .Open source XML editors examined - it is a little bit outdated though .It 's kinda low tech , and there might be a better solution out there , but you could just store your schema in an SQL script which can be run to create the database .
I think you can execute a command to generate this script , but I do n't know the command unfortunately .
Then , commit the script into source control along with the code that works on it .
When you need to change the schema along with the code , the script can be checked in along with the code that requires the changed schema .
Then , diffs on the script will indicate diffs on schema changes .
With this script , you could integrate it with DBUnit or some kind of build script , so it seems it could fit in with your already automated processes .I work a lot with XML , and have found Oxygen to be a great editor .
It 's cross-platform and has a graphical schema editor , but since I use DTDs and not schemas , I ca n't vouch for the schema editor 's quality .
The rest of the editing package ( such as the XML editor and XSLT debugger ) is solid , so it could be worth a try .I like General Software Stackoverflow ( perhaps too obvious ) Deep Fried Bytes Hanselminutes Software Engineering Radio ( via Brenden ) Herding Code Dot Net Alt.NET Podcast Polymorphic Podcast Productivity 43 FoldersThe Google Developer Podcast is good .I do n't want to get into a religious war here , but regardless of your eventual text editor preference pico and nano are the easiest to jump into because the essential keyboard commands are all displayed for you at the bottom of the screen .
I 've long been told that vi would be worth using but for when you just need to change one line in a config file its nice not to have to spend 15 minutes trying to remember how to save and quit .
That said , if you do find yourself in vi you can press escape to enter 'command mode ' ( or whatever it is called ) and : wq will save and quit .
I only mention this because of all things Linux related trying to exit vi is the thing that frustrated me the most until I learnt how !denny wrote : I personally feel that if reusable code is split into projects it is simpler to use other places than if it is just in folders .
I really agree with this - if you can reuse it , it should be in a separate project .
With that said , it 's also very difficult to reuse effectively : ) Here at SO , we 've tried to be very simple with three projects : MVC Web project ( which does a nice job of separating your layers into folders by default ) Database project for source control of our DB Unit tests against MVC models/controllers I ca n't speak for everyone , but I 'm happy with how simple we 've kept it - really speeds the builds along !In the Rails world , there 's the concept of migrations , scripts in which changes to the database are made in Ruby rather than a database-specific flavour of SQL .
Your Ruby migration code ends up being converted into the DDL specific to your current database ; this makes switching database platforms very easy .
For every change you make to the database , you write a new migration .
Migrations typically have two methods : an `` up '' method in which the changes are applied and a `` down '' method in which the changes are undone .
A single command brings the database up to date , and can also be used to bring the database to a specific version of the schema .
In Rails , migrations are kept in their own directory in the project directory and get checked into version control just like any other project code .
This Oracle guide to Rails migrations covers migrations quite well .
Developers using other languages have looked at migrations and have implemented their own language-specific versions .
I know of Ruckusing , a PHP migrations system that is modelled after Rails ' migrations ; it might be what you 're looking for .By default , always just create new folder within the same project You will get single assembly ( without additional ILMerge gymnastic ) Easier to obfuscate ( because you will have less public types and methods , ideally none at all ) Separating your source code into multiple projects makes only sense if you ... Have some portions of the source code that are part of the project but not deployable by default or at all ( unit tests , extra plugins etc . )
More developers involved and you want to treat their work as consumable black box .
( not very recommended ) If you can clearly separate your project into isolated layers/modules and you want to make sure that they ca n't cross-consume internal members .
( also not recommended because you will need to decide which aspect is the most important ) If you think that some portions of your source code could be reusable , still do n't create it as a new project .
Just wait until you will really want to reuse it in another solution and isolate it out of original project as needed .
Programming is not a lego , reusing is usually very difficult and often wo n't happen as planned .Preferred languages : C/C++ , Java , and Ruby .
I am looking for some helpful books/tutorials on how to write your own compiler simply for educational purposes .
I am most familiar with C/C++ , Java , and Ruby , so I prefer resources that involve one of those three , but any good resource is acceptable .If you are using C # , have a look at Subsonic , a very useful ORM tool , but is also generates sql script to recreated your scheme and\or data .
These scripts can then be put into source control .
http : //subsonicproject.com/Big List of Resources : A Nanopass Framework for Compiler Education ¶ Advanced Compiler Design and Implementation $ An Incremental Approach to Compiler Construction ¶ ANTLR 3.x Video Tutorial Basics of Compiler Design Building a Parrot Compiler Compiler Basics Compiler Construction $ Compiler Design and Construction $ Crafting a Compiler with C $ Compiler Design in C ¶ Dragon Book $ — Widely considered `` the book '' for compiler writing .
Engineering a Compiler $ Essentials of Programming Languages Flipcode Article Archive ( look for `` Implementing A Scripting Engine by Jan Niestadt '' ) Game Scripting Mastery $ How to build a virtual machine from scratch in C # ¶ Implementing Functional Languages Implementing Programming Languages ( with BNFC ) Implementing Programming Languages using C # 4.0 Interpreter pattern ( described in Design Patterns $ ) specifies a way to evaluate sentences in a language Language Implementation Patterns : Create Your Own Domain-Specific and General Programming Languages Let 's Build a Compiler — The PDF ¶ version Linkers and Loaders $ ( Google Books ) Lisp in Small Pieces ( LiSP ) $ LLVM Tutorial Modern Compiler Implementation in ML $ — There is a Java $ and C $ version as well - widely considered a very good book Object-Oriented Compiler Construction $ Parsing Techniques - A Practical Guide Project Oberon ¶ - Look at chapter 13 Programming a Personal Computer $ Programing Languages : Application and Interpretation Rabbit : A Compiler for Scheme¶ Reflections on Trusting Trust — A quick guide Roll Your Own Compiler for the .NET framework — A quick tutorial from MSDN Structure and Interpretation of Computer Programs Types and Programming Languages Want to Write a Compiler ?
- a quick guide Writing a Compiler in Ruby Bottom Up Legend : ¶ Link to a PDF file $ Link to a printed bookIf you are quite happy with MSBuild , then I would stick with MSBuild .
This may be one of those cases where the tool you learn first is the one you will prefer .
I started with NAnt and ca n't quite get used to MSBuild .
I 'm sure they will both be around for quite some time .
There are some fundamental differences between the two , probably best highlighted by this conversation between some NAnt fans and a Microsoftie .
Interestingly , Jeremy Miller asked the exact opposite question on his blog last year .`` Let 's Build a Compiler '' is awesome , but it 's a bit outdated .
( I 'm not saying it makes it even a little bit less valid . )I 'm writing an app to help facilitate some research , and part of this involves doing some statistical calculations .
Right now , the researchers are using a program called SPSS .
Part of the output that they care about looks like this : They 're really only concerned about the F and Sig .
values .
My problem is that I have no background in statistics , and I ca n't figure out what the tests are called , or how to calculate them .
I thought the F value might be the result of the F-test , but after following the steps given on Wikipedia , I got a result that was different from what SPSS gives .I am looking for guidance regarding the best practice around the use of the Profile feature in ASP.NET .
How do you decide what should be kept in the built-in user Profile , or if you should create your own DB Table and add a column for the desired fields ?
For example , a user has a ZIP code , should I save the ZIP code in my own table , or should I add it to the web.config xml profile and access it via the user profile ASP.NET mechanize ?
The pros/cons I can think of are that since I do n't know the profile very well ( it is a bit of a Matrix right now ) , I probably can do whatever I want if I go the table route ( e.g. , SQL to get all the users in the same ZIP code as the current user ) ; I do n't know if I can do the same if I use the ASP.NET profile .I 'm not necessarily looking for the fastest but a nice balance , some of the server that this code is being developed for are fairly slow , the script that hashes and stores the password is taking 5-6 seconds to run , and I 've narrowed it down to the hashing ( if I comment the hashing out it runs , in 1-2 seconds ) .
It does n't have to be the MOST secure , I 'm not codding for a bank ( right now ) but I certainly WILL NOT store the passwords as plain-text .I concur with the Dragon Book reference ; IMO , it is the definitive guide to compiler construction .
Get ready for some hardcore theory , though .
If you want a book that is lighter on theory , Game Scripting Mastery might be a better book for you .
If you are a total newbie at compiler theory , it provides a gentler introduction .
It does n't cover more practical parsing methods ( opting for non-predictive recursive descent without discussing LL or LR parsing ) , and as I recall , it does n't even discuss any sort of optimization theory .
Plus , instead of compiling to machine code , it compiles to a bytecode that is supposed to run on a VM that you also write .
It 's still a decent read , particularly if you can pick it up for cheap on Amazon .
If you only want an easy introduction into compilers , Game Scripting Mastery is not a bad way to go .
If you want to go hardcore up front , then you should settle for nothing less than the Dragon Book .In my experience its best to keep an the info in the profile to a bare minimum , only put the essentials in there that are directly needed for authentication .
Other information such as addresses should be saved in your own database by your own application logic , this approach is more extensible and maintainable .I think that depends on how many fields you need .
To my knowledge , Profiles are essentially a long string that gets split at the given field sizes , which means that they do not scale very well if you have many fields and users .
On the other hand , they are built in , so it 's an easy and standardized way , which means there is not a big learning curve and you can use it in future apps as well without needing to tweak it to a new table structure .
Rolling your own thing allows you to put it in a properly normalized database , which drastically improves performance , but you have to write pretty much all the profile managing code yourself .
Edit : Also , Profiles are not cached , so every access to a profile goes to the database first ( it 's then cached for that request , but the next request will get it from the database again ) If you 're thinking about writing your own thing , maybe a custom Profile Provider gives you the best of both worlds - seamless integration , yet the custom stuff you want to do .If you 're looking to use powerful , higher level tools rather than building everything yourself , going through the projects and readings for this course is a pretty good option .
It 's a languages course by the author of the Java parser engine ANTLR .
You can get the book for the course as a PDF from the Pragmatic Programmers .
The course goes over the standard compiler compiler stuff that you 'd see elsewhere : parsing , types and type checking , polymorphism , symbol tables , and code generation .
Pretty much the only thing that is n't covered is optimizations .
The final project is a program that compiles a subset of C. Because you use tools like ANTLR and LLVM , it 's feasible to write the entire compiler in a single day ( I have an existence proof of this , though I do mean ~24 hours ) .
It 's heavy on practical engineering using modern tools , a bit lighter on theory .
LLVM , by the way , is simply fantastic .
Many situations where you might normally compile down to assembly , you 'd be much better off compiling to LLVM 's Intermediate Representation instead .
It 's higher level , cross platform , and LLVM is quite good at generating optimized assembly from it .The active projects on Rubyforge are a great place to start .
What would be a good starter project is to pick one that is pretty popular but not a lot of developers .
If you are interested in Ruby on Rails , I 'm working on Redmine right now .
It 's been one of the most active projects and only has 5 developers .
Open Source Rails also has a good collection of projects .
I 've found doing a Refactotum a great way to get started on a project .
Use the fact that you are new to your advantage , most people who have been on a project forget about simple things like gem dependencies and documentationThis website might help you out a bit more .
Also this one .
I 'm working from a fairly rusty memory of a statistics course , but here goes nothing : When you 're doing analysis of variance ( ANOVA ) , you actually calculate the F statistic as the ratio from the mean-square variances `` between the groups '' and the mean-square variances `` within the groups '' .
The second link above seems pretty good for this calculation .
This makes the F statistic measure exactly how powerful your model is , because the `` between the groups '' variance is explanatory power , and `` within the groups '' variance is random error .
High F implies a highly significant model .
As in many statistical operations , you back-determine Sig .
using the F statistic .
Here 's where your Wikipedia information comes in slightly handy .
What you want to do is - using the degrees of freedom given to you by SPSS - find the proper P value at which an F table will give you the F statistic you calculated .
The P value where this happens [ F ( table ) = F ( calculated ) ] is the significance .
Conceptually , a lower significance value shows a very strong ability to reject the null hypothesis ( which for these purposes means to determine your model has explanatory power ) .
Sorry to any math folks if any of this is wrong .
I 'll be checking back to make edits ! ! !
Good luck to you .
Stats is fun , just maybe not this part .
= )My dilemma is , basically , how to share an enumeration between two applications .
The users upload documents through a front-end application that is on the web .
This application calls a web service of the back-end application and passes the document to it .
The back-end app saves the document and inserts a row in the Document table .
The document type ( 7 possible document types : Invoice , Contract etc . )
is passed as a parameter to the web service 's UploadDocument method .
The question is , what should the type ( and possible values ) of this parameter be ?
Since you need to hardcode these values in both applications , I think it is O.K .
to use a descriptive string ( Invoice , Contract , WorkOrder , SignedWorkOrder ) .
Is it maybe a better approach to create a DocumentTypes enumeration in the first application , and to reproduce it also in the second application , and then pass the corresponding integer value to the web service between them ?If you could go back in time and tell yourself to read a specific book at the beginning of your career as a developer , which book would it be ?
I expect this list to be varied and to cover a wide range of things .
To search : Use the search box in the upper-right corner .
To search the answers of the current question , use inquestion : this .
For example : @ code1 @Code Complete ( 2nd edition ) by Steve McConnell The Pragmatic Programmer Structure and Interpretation of Computer Programs The C Programming Language by Kernighan and Ritchie Introduction to Algorithms by Cormen , Leiserson , Rivest & Stein Design Patterns by the Gang of Four Refactoring : Improving the Design of Existing Code The Mythical Man Month The Art of Computer Programming by Donald Knuth Compilers : Principles , Techniques and Tools by Alfred V. Aho , Ravi Sethi and Jeffrey D. Ullman Gödel , Escher , Bach by Douglas Hofstadter Clean Code : A Handbook of Agile Software Craftsmanship by Robert C. Martin Effective C++ More Effective C++ CODE by Charles Petzold Programming Pearls by Jon Bentley Working Effectively with Legacy Code by Michael C. Feathers Peopleware by Demarco and Lister Coders at Work by Peter Seibel Surely You 're Joking , Mr. Feynman !
Effective Java 2nd edition Patterns of Enterprise Application Architecture by Martin Fowler The Little Schemer The Seasoned Schemer Why 's ( Poignant ) Guide to Ruby The Inmates Are Running The Asylum : Why High Tech Products Drive Us Crazy and How to Restore the Sanity The Art of Unix Programming Test-Driven Development : By Example by Kent Beck Practices of an Agile Developer Do n't Make Me Think Agile Software Development , Principles , Patterns , and Practices by Robert C. Martin Domain Driven Designs by Eric Evans The Design of Everyday Things by Donald Norman Modern C++ Design by Andrei Alexandrescu Best Software Writing I by Joel Spolsky The Practice of Programming by Kernighan and Pike Pragmatic Thinking and Learning : Refactor Your Wetware by Andy Hunt Software Estimation : Demystifying the Black Art by Steve McConnel The Passionate Programmer ( My Job Went To India ) by Chad Fowler Hackers : Heroes of the Computer Revolution Algorithms + Data Structures = Programs Writing Solid Code JavaScript - The Good Parts Getting Real by 37 Signals Foundations of Programming by Karl Seguin Computer Graphics : Principles and Practice in C ( 2nd Edition ) Thinking in Java by Bruce Eckel The Elements of Computing Systems Refactoring to Patterns by Joshua Kerievsky Modern Operating Systems by Andrew S. Tanenbaum The Annotated Turing Things That Make Us Smart by Donald Norman The Timeless Way of Building by Christopher Alexander The Deadline : A Novel About Project Management by Tom DeMarco The C++ Programming Language ( 3rd edition ) by Stroustrup Patterns of Enterprise Application Architecture Computer Systems - A Programmer 's Perspective Agile Principles , Patterns , and Practices in C # by Robert C. Martin Growing Object-Oriented Software , Guided by Tests Framework Design Guidelines by Brad Abrams Object Thinking by Dr. David West Advanced Programming in the UNIX Environment by W. Richard Stevens Hackers and Painters : Big Ideas from the Computer Age The Soul of a New Machine by Tracy Kidder CLR via C # by Jeffrey Richter The Timeless Way of Building by Christopher Alexander Design Patterns in C # by Steve Metsker Alice in Wonderland by Lewis Carol Zen and the Art of Motorcycle Maintenance by Robert M. Pirsig About Face - The Essentials of Interaction Design Here Comes Everybody : The Power of Organizing Without Organizations by Clay Shirky The Tao of Programming Computational Beauty of Nature Writing Solid Code by Steve Maguire Philip and Alex 's Guide to Web Publishing Object-Oriented Analysis and Design with Applications by Grady Booch Effective Java by Joshua Bloch Computability by N. J. Cutland Masterminds of Programming The Tao Te Ching The Productive Programmer The Art of Deception by Kevin Mitnick The Career Programmer : Guerilla Tactics for an Imperfect World by Christopher Duncan Paradigms of Artificial Intelligence Programming : Case studies in Common Lisp Masters of Doom Pragmatic Unit Testing in C # with NUnit by Andy Hunt and Dave Thomas with Matt Hargett How To Solve It by George Polya The Alchemist by Paulo Coelho Smalltalk-80 : The Language and its Implementation Writing Secure Code ( 2nd Edition ) by Michael Howard Introduction to Functional Programming by Philip Wadler and Richard Bird No Bugs !
by David Thielen Rework by Jason Freid and DHH JUnit in ActionThis is very easy to do in java , and as stated above you can reach into the java.util.zip libraries from C # .
For references see : java.util.zip javadocs sample code I used this a while ago to do a deep ( recursive ) zip of a folder structure , but I do n't think I ever used the unzipping .
If I 'm so motivated I may pull that code out and edit it into here later .This is my take on it .
I would say unit testing is the practice of writing software tests to verify that your real software does what it is meant to .
This started with jUnit in the Java world and has become a best practice in PHP as well with SimpleTest and phpUnit .
It 's a core practice of Extreme Programming and helps you to be sure that your software still works as intended after editing .
If you have sufficient test coverage , you can do major refactoring , bug fixing or add features rapidly with much less fear of introducing other problems .
It 's most effective when all unit tests can be run automatically .
Unit testing is generally associated with OO development .
The basic idea is to create a script which sets up the environment for your code and then exercises it ; you write assertions , specify the intended output that you should receive and then execute your test script using a framework such as those mentioned above .
The framework will run all the tests against your code and then report back success or failure of each test .
phpUnit is run from the Linux command line by default , though there are HTTP interfaces available for it .
SimpleTest is web-based by nature and is much easier to get up and running , IMO .
In combination with xDebug , phpUnit can give you automated statistics for code coverage which some people find very useful .
Some teams write hooks from their subversion repository so that unit tests are run automatically whenever you commit changes .
It 's good practice to keep your unit tests in the same repository as your application .I 'd suggest against passing an integer between them , simply for purposes of readability and debugging .
Say you 're going through your logs and you see a bunch of 500 errors for DocumentType=4 .
Now you 've got to go look up which DocumentType is 4 .
Or if one of the applications refers to a number that does n't exist in the other , perhaps due to mismatched versions .
It 's a bit more code , and it rubs the static typing part of the brain a bit raw , but in protocols on top of HTTP the received wisdom is to side with legible strings over opaque enumerations .I can only speak about .net , but if you have an ASP.net Webservice , you should be able to add an enumeration directly to it .
When you then use the `` Add Web Reference '' in your Client Application , the resulting Class should include that enum But this is from the top of my head , i 'm pretty sure i 've done it in the past , but I ca n't say for sure .I would still use enumeration internally but would expect consumers to pass me only the name , not the numeric value itself .
just some silly example to illustrate : public enum DocumentType { Invoice , Contract , WorkOrder , SignedWorkOrder } [ WebMethod ] public void UploadDocument ( string type , byte [ ] data ) { DocumentType docType = ( DocumentType ) Enum.Parse ( typeof ( DocumentType ) , type ) ; }I was just looking through some information about Google 's protocol buffers data interchange format .
Has anyone played around with the code or even created a project around it ?
I 'm currently using XML in a Python project for structured content created by hand in a text editor , and I was wondering what the general opinion was on Protocol Buffers as a user-facing input format .
The speed and brevity benefits definitely seem to be there , but there are so many factors when it comes to actually generating and processing the data .I am trying to grab the capital letters of a couple of words and wrap them in span tags .
I am using preg_replace for extract and wrapping purposes , but it 's not outputting anything .
@ code1 @You need to put the match in parentheses , like this : @ code1 @I thought I 'd give this a shot using classes and polymorphism .
I had a previous iteration which used sub-classing which ended up having way too much overhead .
I 've switched to a more flexible delegate / public property object model which is significantly better .
My code is very slightly more accurate , I wish I could come up with a better way to generate `` months ago '' that did n't seem too over-engineered .
I think I 'd still stick with Jeff 's if-then cascade because it 's less code and it 's simpler ( it 's definitely easier to ensure it 'll work as expected ) .
For the below code PrintRelativeTime.GetRelativeTimeMessage ( TimeSpan ago ) returns the relative time message ( e.g .
`` yesterday '' ) .
public class RelativeTimeRange : IComparable { public TimeSpan UpperBound { get ; set ; } public delegate string RelativeTimeTextDelegate ( TimeSpan timeDelta ) ; public RelativeTimeTextDelegate MessageCreator { get ; set ; } public int CompareTo ( object obj ) { if ( !
( obj is RelativeTimeRange ) ) { return 1 ; } // note that this sorts in reverse order to the way you 'd expect , // this saves having to reverse a list later return ( obj as RelativeTimeRange ) .UpperBound.CompareTo ( UpperBound ) ; } } public class PrintRelativeTime { private static List < RelativeTimeRange > timeRanges ; static PrintRelativeTime ( ) { timeRanges = new List < RelativeTimeRange > { new RelativeTimeRange { UpperBound = TimeSpan.FromSeconds ( 1 ) , MessageCreator = ( delta ) = > { return `` one second ago '' ; } } , new RelativeTimeRange { UpperBound = TimeSpan.FromSeconds ( 60 ) , MessageCreator = ( delta ) = > { return delta.Seconds + `` seconds ago '' ; } } , new RelativeTimeRange { UpperBound = TimeSpan.FromMinutes ( 2 ) , MessageCreator = ( delta ) = > { return `` one minute ago '' ; } } , new RelativeTimeRange { UpperBound = TimeSpan.FromMinutes ( 60 ) , MessageCreator = ( delta ) = > { return delta.Minutes + `` minutes ago '' ; } } , new RelativeTimeRange { UpperBound = TimeSpan.FromHours ( 2 ) , MessageCreator = ( delta ) = > { return `` one hour ago '' ; } } , new RelativeTimeRange { UpperBound = TimeSpan.FromHours ( 24 ) , MessageCreator = ( delta ) = > { return delta.Hours + `` hours ago '' ; } } , new RelativeTimeRange { UpperBound = TimeSpan.FromDays ( 2 ) , MessageCreator = ( delta ) = > { return `` yesterday '' ; } } , new RelativeTimeRange { UpperBound = DateTime.Now.Subtract ( DateTime.Now.AddMonths ( -1 ) ) , MessageCreator = ( delta ) = > { return delta.Days + `` days ago '' ; } } , new RelativeTimeRange { UpperBound = DateTime.Now.Subtract ( DateTime.Now.AddMonths ( -2 ) ) , MessageCreator = ( delta ) = > { return `` one month ago '' ; } } , new RelativeTimeRange { UpperBound = DateTime.Now.Subtract ( DateTime.Now.AddYears ( -1 ) ) , MessageCreator = ( delta ) = > { return ( int ) Math.Floor ( delta.TotalDays / 30 ) + `` months ago '' ; } } , new RelativeTimeRange { UpperBound = DateTime.Now.Subtract ( DateTime.Now.AddYears ( -2 ) ) , MessageCreator = ( delta ) = > { return `` one year ago '' ; } } , new RelativeTimeRange { UpperBound = TimeSpan.MaxValue , MessageCreator = ( delta ) = > { return ( int ) Math.Floor ( delta.TotalDays / 365.24D ) + `` years ago '' ; } } } ; timeRanges.Sort ( ) ; } public static string GetRelativeTimeMessage ( TimeSpan ago ) { RelativeTimeRange postRelativeDateRange = timeRanges [ 0 ] ; foreach ( var timeRange in timeRanges ) { if ( ago.CompareTo ( timeRange.UpperBound ) < = 0 ) { postRelativeDateRange = timeRange ; } } return postRelativeDateRange.MessageCreator ( ago ) ; } }+1 for print_r ( ) .
Use it to dump out the contents of an object or variable .
To make it more readable , do it with a pre tag so you do n't need to view source .
echo ' < pre > ' ; print_r ( $ arrayOrObject ) ; Also var_dump ( $ thing ) - this is very useful to see the type of subthingsGetting back into a bit more .NET after a few-years of not using it full-time , and wondering what the good unit testing packages are these days .
I 'm familiar with NUnit ( a few years ago ) , and have played briefly around with IronRuby , with the goal of getting something like rspec going , but do n't know much beyond that .
I realize I could google for this and call it a day but I believe I 'm likely to get a better and more informed response from asking a question here : - ) Suggestions ?I am trying to set a flag to show or hide a page element , but it always displays even when the expression is false .
$ canMerge = ( $ condition1 & & $ condition2 ) ?
'true ' : 'false ' ; ... < ? php if ( $ canMerge ) { ? > Stuff < ? php } ? > What 's up ?The value of 'false ' is true .
You need to remove the quotes : $ canMerge = ( $ condition1 & & $ condition2 ) ?
true : false ;I like MbUnit , er , Gallio .
Most importantly to me is having good tools support inside Visual Studio .
For that I use Resharper , which has an MbUnit test runner .
A lot of folks seem to like TestDriven.NET as their test runner as well .This is broken because 'false ' as a string will evaluate to true as a boolean .
However , this is an unneeded ternary expression , because the resulting values are simple true and false .
This would be equivalent : $ canMerge = ( $ condition1 & & $ condition2 ) ;Baby steps ...
Most of my command line stuff is integrating with our code .
svn up , svn commit , svn merge , and so on .
Personally , what I 've found very useful is a basic knowledge of bash scripting , ruby scripting , and ( absurdly ) colordiff Command line stuff requires a pile of typing , but that 's one of the main reasons why scripting exists .
For example , I ca n't for the life of me read svn diff output on the commandline .
Piping it to the colordiff utility makes a world of difference .
It 's then one more step to create a bash function to save you typing svn diff | colordiff | less -R all the time .
If you read man colordiff you 'll see they define a function in your ~/.bashrc file , mine goes something like this function sdiff ( ) { svn diff $ @ | colordiff | less -R ; } The $ @ means 'All arguments ' , otherwise arguments are $ 1 , $ 2 , etc , and the rest should be fairly self explanatory .
You can then apply this knowledge you now have to other repetitive actions , and before you know it , you 'll be a guru .
Sometimes bash gets a bit hairy ( like when you want to do anything more complicated than piping a few programs together ) and in those cases I 've found just writing simple ruby scripts to be the way out .
Ruby is awesome .
Good Luck !
: - )The exact definitions of `` vsize , '' `` rss , '' `` rprvt , '' `` rshrd , '' and other obscure-looking abbreviations vary from OS to OS .
The manual pages for the `` top '' and `` ps '' commands will have some sort of description , but all such descriptions are simplified greatly ( or are based on long-extinct kernel implementations ) .
`` Process size '' as a concept is fiendishly difficult to pin down in the general case .
Answers in specific instances depend heavily on the actual memory management implementation in the OS , and are rarely as satisfying as the tidy `` process size '' concept that exists in the minds of most users ( and most developers ) .
For example , none of those numbers ( nor , likely , any combination of them ) can be used to tell you exactly how many such processes can run at once in a given amount of free memory .
But really , your best bet is to come at it from that end : why do you want this number , and what will you use it for ?
Given that information , I think you 'll get more useful answers .Oh , I also use less to look at log files .
It 's a WHOLE LOT BETTER if you know some shortcuts ( these also work in Vim ) /regex - Search for the next text which matches regex n - Jump to the next match for the regex you just typed N - Jump to previous match ESC - Cancel searches and get yourself out of trouble g - Jump to start of file G - Jump to end of file 72g - Jump to line 72The Publish to Provider functionality has worked great for me .
See Scott Gu 's Blog Entry .
If you need something really robust look at redgate software 's tools here ... if you are doing much SQL at all , these are worth the $ $ .Wild guess here : If you do n't want to modify the third party 's tables , Can you create a view and then put a trigger on that view ?If you are looking for user facing interaction , stick with xml .
It has more support , understanding , and general acceptance currently .
If it 's internal , I would say that protocol buffers are a great idea .
Maybe in a few years as more tools come out to support protocol buffers , then start looking towards that for a public facing api .
Until then ... JSON ?@ Peter Coulton -- you do n't read Knuth , you study it .
For me , and my work ... Purely Functional Data Structures is great for thinking and developing with functional languages in mind .In .NET , enumeration values are ( by default ) serialized into xml with the name .
For instances where you can have multiple values ( flags ) , then it puts a space between the values .
This works because the enumeration does n't contain spaces , so you can get the value again by splitting the string ( ie .
`` Invoice Contract SignedWorkOrder '' , using lubos 's example ) .
You can control the serialization of values of in asp.net web services using the XmlEnumAttribute , or using the EnumMember attribute when using WCF .Is it possible to create `` federated '' Subversion servers ?
As in one server at location A and another at location B that sync up their local versions of the repository automatically .
That way when someone at either location interacts with the repository they are accessing their respective local server and therefore has faster response times .Backup and Restore is the most straight-forward way I know .
You have to be careful between servers as security credentials do n't come with the restored database .Sounds like you might like Git .
There 's a Google Talk explaining all about it .If you are consuming your Web service from a .NET page/application , you should be able to access the enumeration after you add your Web reference to the project that is consuming the service .From the preg_replace documentation on php.net : replacement may contain references of the form \n or ( since PHP 4.0.4 ) $ n , with the latter form being the preferred one .
Every such reference will be replaced by the text captured by the n'th parenthesized pattern .
See Flubba 's example .Its probably not exactly what your looking for , but you may be able to implement OS level clustering .@ Dillie-O : Your answer put me in the right direction ( I should have expected it to just be a registry change ) and I got this working .
But I 'm going to mark this as the answer because I 'm going to put some additional information that I found while working on this .
The solution to this question really does n't matter what programming language you 're using , as long as there 's some way to modify Windows registry settings .
Finally , here 's the answer : To associate a program with the mailto protocol for all users on a computer , change the HKEY_CLASSES_ROOT\mailto\shell\open\command Default value to : '' Your program 's executable '' `` % 1 '' To associate a program with the mailto protocol for the current user , change the HKEY_CURRENT_USER\Software\Classes\mailto\shell\open\command Default value to : '' Your program 's executable '' `` % 1 '' The % 1 will be replaced with the entire mailto URL .
For example , given the link : @ code1 @ The following will be executed : '' Your program 's executable '' `` mailto : user @ example.com ''I like TestDriven.NET ( even though I use ReSharper ) and I 'm pretty happy with XUnit.net .
It uses Facts instead of Tests which many people dislike but I like the difference in terminology .
It 's useful to think of a collection of automatically provable Facts about your software and see which ones you violate when you make a change .
Be aware that Visual Studio 2008 Professional ( and above ) now comes with integrated Unit Testing ( it used to be available only with the Team System Editions ) and may be suitable for your needs .There are so many it 's crazy .
Crazy good , I guess .
For the conservative types ( me ) , NUnit is still available and still more than capable .
For the Microsoft-types , MSTest is adequate , but slow and clunky compared to Nunit .
It also lacks code coverage without paying the big bucks for the pricey versions of Visual Studio .
There 's also MbUnit .
It 's like NUnit , but has nifty features like RowTest ( run the same test with different parameters ) and Rollback ( put the database back like you found it after a test ) And finally , xUnit.net is the trendy option with some attitude .
Oh , and TestDriven.NET will give you IDE integration for both Nunit and MBunit .
I 'm sure they 're all just fine .
I 'd steer away from MSTest though , unless you just enjoy the convenience of having everything in one IDE out of the box .
Scott Hanselman has a podcast on this very topic .This one 's not specifically about development , but Security Now from Steve Gibson and Leo Laporte is an excellent discussion of security issues .
I think it 's a must-listen for just about any computer user who 's concerned about security , and especially for web developers who are responsible both for the security of their site and at least partially responsible for their users ' security .Is your customer using Exchange 2007 ?
If so , I 'd have a look at Exchange Web Services .
If not , as hairy as it can be , I think WebDAV is your best bet .
Personally I do n't like using the Outlook.Application COM object route , as its security prompts ( `` An application is attempting to access your contacts .
Allow this ?
`` , etc . )
can cause problems on a server .
I also think it would be difficult to accomplish your impersonation-like tasks using Outlook , such as sending mail as a given user .I 've got a menu in Python .
That part was easy .
I 'm using raw_input ( ) to get the selection from the user .
The problem is that raw_input ( and input ) require the user to press Enter after they make a selection .
Is there any way to make the program act immediately upon a keystroke ?
Here 's what I 've got so far : @ code1 @ It would be great to have something like @ code2 @OK .
This is a bit of a vanity app , but I had a situation today at work where I was in a training class and the machine was set to lock every 10 minutes .
Well , if the trainers got excited about talking - as opposed to changing slides - the machine would lock up .
I 'd like to write a teeny app that has nothing but a taskbar icon that does nothing but move the mouse by 1 pixel every 4 minutes .
I can do that in 3 ways with Delphi ( my strong language ) but I 'm moving to C # for work and I 'd like to know the path of least resistance there .On Linux : set raw mode select and read the keystroke restore normal settings import sys import select import termios import tty def getkey ( ) : old_settings = termios.tcgetattr ( sys.stdin ) tty.setraw ( sys.stdin.fileno ( ) ) select.select ( [ sys.stdin ] , [ ] , [ ] , 0 ) answer = sys.stdin.read ( 1 ) termios.tcsetattr ( sys.stdin , termios.TCSADRAIN , old_settings ) return answer print `` '' '' Menu 1 ) Say Foo 2 ) Say Bar '' '' '' answer=getkey ( ) if `` 1 '' in answer : print `` foo '' elif `` 2 '' in answer : print `` bar ''for C # 3.5 without notifyicon therefore you will need to terminate this application in task manager manually using System ; using System.Drawing ; using System.Windows.Forms ; static class Program { static void Main ( ) { Timer timer = new Timer ( ) ; // timer.Interval = 4 minutes timer.Interval = ( int ) ( TimeSpan.TicksPerMinute * 4 / TimeSpan.TicksPerMillisecond ) ; timer.Tick += ( sender , args ) = > { Cursor.Position = new Point ( Cursor.Position.X + 1 , Cursor.Position.Y + 1 ) ; } ; timer.Start ( ) ; Application.Run ( ) ; } }I am currently working on a project and my goal is to locate text in an image .
OCR'ing the text is not my intention as of yet .
I want to basically obtain the bounds of text within an image .
I am using the AForge.Net imaging component for manipulation .
Any assistance in some sense or another ?
Update 2/5/09 : I 've since went along another route in my project .
However I did attempt to obtain text using MODI ( Microsoft Office Document Imaging ) .
It allows you to OCR an image and pull text from it with some ease .Something like this should work ( though , you will want to change the interval ) .
@ code1 @On Windows : @ code1 @What do I need to look at to see if I 'm on Windows , Unix , etc ?This is an active area of research .
There are literally oodles of academic papers on the subject .
It 's going to be difficult to give you assistance especially w/o more deatails .
Are you looking for specific types of text ?
Fonts ?
English-only ?
Are you familiar with the academic literature ?
`` Text detection '' is a standard problem in any OCR ( optical character recognition ) system and consequently there are lots of bits of code on the interwebs that deal with it .
I could start listing piles of links from google but I suggest you just do a search for `` text detection '' and start reading : ) .
There is ample example code available as well .@ code1 @ See : platform — Access to underlying platform’s identifying dataOn the project that I am working on I have a couple of databases .
Each table and each column in the database has a description set ( as an extended property in SQL 2005 ) .
As a part of the documentation going to the client we need to produce a data dictionary showing all of the tables and columns along with a collection of meta data ( data-type , optionality , constraints ) .
Is anyone using a tool to automatically create this kind of document ?
If so , which tools do you use ?
I have used Data Dictionary Creator which is awesome but it does n't seem to do data types or optionality ( unless you want to add in custom fields and fill them in yourself ) .Wow , that took forever .
Ok , here 's what I 've ended up with # ! C : \python25\python.exeimport msvcrtprint `` '' '' Menu1 ) Say Foo 2 ) Say Bar '' '' '' while 1 : char = msvcrt.getch ( ) if char == chr ( 27 ) : # escape break if char == `` 1 '' : print `` foo '' break if char == `` 2 '' : print `` Bar '' break It fails hard using IDLE , the python ... thing ... that comes with python .
But once I tried it in DOS ( er , CMD.exe ) , as a real program , then it ran fine .
No one try it in IDLE , unless you have Task Manager handy .
I 've already forgotten how I lived with menus that ar n't super-instant responsive .Dang -- lbrandy beat me to the punch , but that does n't mean I ca n't provide you with the system results for Vista !
@ code1 @How do I delimit a Javascript databound string parameter in an anchor OnClick event ?
I have an anchor tag in an ASP.NET Repeater control .
The OnClick event of the anchor contains a call to a Javascript function .
The Javascript function takes a string for its input parameter .
The string parameter is populated with a databound value from the Repeater .
I need the 'double quotes ' for the Container.DataItem .
I need the 'single quotes ' for the OnClick .
And I still need one more delimiter ( triple quotes ? )
for the input string parameter of the Javascript function call .
Since I ca n't use 'single quotes ' again , how do I ensure the Javascript function knows the input parameter is a string and not an integer ?
Without the extra quotes around the input string parameter , the Javascript function thinks I 'm passing in an integer .
Cheers in advance for any knowledge you can drop .
The anchor : @ code1 @ and here is the Javascript : @ code2 @Without the extra quotes around the input string parameter , the Javascript function thinks I 'm passing in an integer .
Can you do some rudimentary string function to force JavaScript into changing it into a string ?
Like value = value + `` ''For the record here 's the results on Mac : @ code1 @Try putting the extra text inside the server-side script block and concatenating .
onclick= ' < % # `` ToggleDisplay ( `` '' '' & DataBinder.Eval ( Container.DataItem , `` JobCode '' ) & `` '' '' ) ; '' % > ' Edit : I 'm pretty sure you could just use double quotes outside the script block as well .The reason msvcrt fails in IDLE is because IDLE is not accessing the library that runs msvcrt .
Whereas when you run the program natively in cmd.exe it works nicely .
For the same reason that your program blows up on Mac and Linux terminals .
But I guess if you 're going to be using this specifically for windows , more power to ya .I had recently similar problem and the only way to solve it was to use plain old HTML codes for single ( & # 39 ; ) and double quotes ( & # 34 ; ) .
Source code was total mess of course but it worked .
Try @ code1 @ or @ code2 @I create folders named after the build versions and put upgrade and downgrade scripts in there .
For example , you could have the following folders : 1.0.0 , 1.0.1 and 1.0.2 .
Each one contains the script that allows you to upgrade or downgrade your database between versions .
Should a client or customer call you with a problem with version 1.0.1 and you are using 1.0.2 , bringing the database back to his version will not be a problem .
In your database , create a table called `` schema '' where you put in the current version of the database .
Then writing a program that can upgrade or downgrade your database for you is easy .
Just like Joey said , if you are in a Rails world , use Migrations .
: )I realize this is a newbie question , but I 'm looking for a simple solution - it seems like there should be one .
What 's the best way to import a CSV file into a strongly-typed data structure ?
Again simple = better .Check out FileHelpers Open Source Library .Extending on what Mike Powell has to say , I am actually a big fan of almost all of the podcasts at http : //www.twit.tv .
Most of the content is watered down a bit , but some of the speakers are top notch thinkers - especially on `` This Week in Tech '' , the flagship program .
Oh - and Car Talk on NPR but those guys hardly EVER get into the SDLC !A good simple way to do it is to open the file , and read each line into an array , linked list , data-structure-of-your-choice .
Be careful about handling the first line though .
This may be over your head , but there seems to be a direct way to access them as well using a connection string .
Why not try using python instead of c # or vb ?
It has a nice CSV module to import that does all the heavy lifting for you .
EDIT : @ NotMyself - just because we commented poorly on your trolling question , please do n't vote down our answers .
sasb and i appreciate it .I have a bunch of latitude/longitude pairs that map to known x/y coordinates on a ( geographically distorted ) map .
Then I have one more latitude/longitude pair .
I want to plot it on the map as best is possible .
How do I go about doing this ?
At first I decided to create a system of linear equations for the three nearest lat/long points and compute a transformation from these , but this does n't work well at all .
Since that 's a linear system , I ca n't use more nearby points either .
You ca n't assume North is up : all you have is the existing lat/long- > x/y mappings .
EDIT : it 's not a Mercator projection , or anything like that .
It 's arbitrarily distorted for readability ( think subway map ) .
I want to use only the nearest 5 to 10 mappings so that distortion on other parts of the map does n't affect the mapping I 'm trying to compute .
Further , the entire map is in a very small geographical area so there 's no need to worry about the globe -- flat-earth assumptions are good enough .Chris ' probably has the best pure answer to the question : However , I 'm curious about the root of the question .
If the user should always wrap the call in a try/catch block , should the user-called function really be throwing exceptions in the first place ?
This is a difficult question to answer without more context regarding the code-base in question .
Shooting from the hip , I think the best answer here is to wrap the function up such that the recommended ( if not only , depending on the overall exception style of the code ) public interface does the try/catch for the user .
If you 're just trying to ensure that there are no unhandled exceptions in your code , unit tests and code review are probably the best solution .You are going to get a lot of strings , that 's for sure ... Where , x and y is how you define them and r is the number of characters we are selecting from -- if I am understanding you correctly .
You should definitely generate these as needed and not get sloppy and say , generate a powerset and then filter the length of strings .
The following definitely is n't the best way to generate these , but it 's an interesting aside , none-the-less .
Knuth ( volume 4 , fascicle 2 , 7.2.1.3 ) tells us that ( s , t ) -combination is equivalent to s+1 things taken t at a time with repetition -- an ( s , t ) -combination is notation used by Knuth that is equal to .
We can figure this out by first generating each ( s , t ) -combination in binary form ( so , of length ( s+t ) ) and counting the number of 0 's to the left of each 1 .
10001000011101 -- > becomes the permutation : { 0 , 3 , 4 , 4 , 4 , 1 }Brad 's list is pretty good .
I also listen to : Sparkling Client ( Silverlight specific ) Jon Udell 's Perspectives series Herding Code ( shameless plug for a podcast I put on with Kevin Dente , Scott `` lazycoder '' Koon , and K. Scott Allen .
We recently interviewed Jeff Atwood about Stack Overflow , discussing both how the site is designed and the technology behind it .If you can guarantee that there are no commas in the data , then the simplest way would probably be to use String.split .
For example : String [ ] values = myString.Split ( ' , ' ) ; myObject.StringField = values [ 0 ] ; myObject.IntField = Int32.Parse ( values [ 1 ] ) ; There may be libraries you could use to help , but that 's probably as simple as you can get .
Just make sure you ca n't have commas in the data , otherwise you will need to parse it better .Slightly off topic , but in a similar vein .
Have you looked at Castle ActiveRecord it is written on top of NHibernate and removes the need to spend time creating XML mappings from code to the database .
Like NHibernate you can structure your domain objects as you want and later generate a database schema from this structure .
Using ActiveWriter , a contributed tool , you can easily map from your database to domain objects .I listen to the javaposse regularly , they cover mostly Java , but not solely .the problem is that the sphere can be distorted a number of ways , and having all those points known on the equator , lets say , wont help you map points further away .
You need better 'close ' points , then you can assume these three points are on a plane with the fourth and do the interpolation -- knowing that the distance of longitudes is a function , not a constant .Ummm .
Maybe I am missing something about the question here , but if you have long/lat info , you also have the direction of north ?
It seems you need to map geodesic coordinates to a projected coordinates system .
For example osgb to wgs84 .
The maths involved is non-trivial , but the code comes out a only a few lines .
If I had more time I 'd post more but I need a shower so I will be boring and link to the wikipedia entry which is pretty good .
Note : Post shower edited .There are two articles on CodeProject that provide code for a solution , one that uses StreamReader and one that imports CSV data using the Microsoft Text Driver .I 'm not clear on whether or not you 're wanting to add the asynchronous bits to the server in C # or the client in C++ .
If you 're talking about doing this in C++ , desktop Windows platforms can do socket I/O asynchronously through the API 's that use overlapped I/O .
For sockets , WSASend , WSARecv both allow async I/O ( read the documentation on their LPOVERLAPPED parameters , which you can populate with events that get set when the I/O completes ) .
I do n't know if Windows Mobile platforms support these functions , so you might have to do some additional digging .Using ASP.NET MVC there are situations ( such as form submission ) that may require a RedirectToAction .
One such situation is when you encounter validation errors after a form submission and need to redirect back to the form , but would like the URL to reflect the URL of the form , not the action page it submits to .
As I require the form to contain the originally POSTed data , for user convenience , as well as validation purposes , how can I pass the data through the RedirectToAction ( ) ?
If I use the viewData parameter , my POST parameters will be changed to GET parameters .The solution is to use the TempData property to store the desired Request components .
For instance : public ActionResult Send ( ) { TempData [ `` form '' ] = Request.Form ; return this.RedirectToAction ( a = > a.Form ( ) ) ; } Then in your `` Form '' action you can go : public ActionResult Form ( ) { /* Declare viewData etc .
*/ if ( TempData [ `` form '' ] ! = null ) { /* Cast TempData [ `` form '' ] to System.Collections.Specialized.NameValueCollection and use it */ } return View ( `` Form '' , viewData ) ; }Are there any more specific details on the kind of distortion ?
If , for example , your latitudes and longitudes are `` distorted '' onto your 2D map using a Mercator projection , the conversion math is readily available .
If the map is distorted truly arbitrarily , there are lots of things you could try , but the simplest would probably be to compute a weighted average from your existing point mappings .
Your weights could be the squared inverse of the x/y distance from your new point to each of your existing points .
Some pseudocode : @ code1 @ This code will give a relatively simple approximation .
If you can be more precise about the way the projection distorts the geographical coordinates , you can probably do much better .Here is some sample code .
I think this is what you are looking for .
The following displays exactly the same in Firefox 3 ( mac ) and IE7 .
@ code1 @ @ code2 @In order to fully use LinqToSql in an ASP.net 3.5 application , it is necessary to create DataContext classes ( which is usually done using the designer in VS 2008 ) .
From the UI perspective , the DataContext is a design of the sections of your database that you would like to expose to through LinqToSql and is integral in setting up the ORM features of LinqToSql .
My question is : I am setting up a project that uses a large database where all tables are interconnected in some way through Foreign Keys .
My first inclination is to make one huge DataContext class that models the entire database .
That way I could in theory ( though I do n't know if this would be needed in practice ) use the Foreign Key connections that are generated through LinqToSql to easily go between related objects in my code , insert related objects , etc .
However , after giving it some thought , I am now thinking that it may make more sense to create multiple DataContext classes , each one relating to a specific namespace or logical interrelated section within my database .
My main concern is that instantiating and disposing one huge DataContext class all the time for individual operations that relate to specific areas of the Database would be impose an unnecessary imposition on application resources .
Additionally , it is easier to create and manage smaller DataContext files than one big one .
The thing that I would lose is that there would be some distant sections of the database that would not be navigable through LinqToSql ( even though a chain of relationships connects them in the actual database ) .
Additionally , there would be some table classes that would exist in more than one DataContext .
Any thoughts or experience on whether multiple DataContexts ( corresponding to DB namespaces ) are appropriate in place of ( or in addition to ) one very large DataContext class ( corresponding to the whole DB ) ?recognizing text inside an image is indeed a hot topic for researchers in that field , but only begun to grow out of control when captcha 's became the `` norm '' in terms of defense against spam bots .
Why use captcha 's as protection ?
well because it is/was very hard to locate ( and read ) text inside an image !
The reason why I mention captcha 's is because the most advancement* is made within that tiny area , and I think that your solution could be best found there .
especially because captcha 's are indeed about locating text ( or something that resembles text ) inside a cluttered image and afterwards trying to read the letters correctly .
so if you can find yourself a good open source captcha breaking tool you probably have all you need to continue your quest ... You could probably even throw away the most dificult code that handles the character recognition itself , because those OCR 's are used to read distorted text , something you do n't have to do .
* : advancement in terms of visible , usable , and practical information for a `` non-researcher ''PostgreSQL is interesting in that it supports several languages for writing stored procedures .
Which one do you use , and why ?When I wrote my first trigger it was in Perl because that was the language used in the example I found but the last triggers I wrote where in Python because I 'm more comfortable with that .
I think programmer comfort is my main reason to choose python but I think Perl is still better supported .I 've provided a solution for Python ... please flesh this out with examples for other languages .Using the Python DB API , do n't do this : @ code1 @ instead , do this : @ code2 @I was just wondering if there is an elegant way to set the maximum CPU load for a particular thread doing intensive calculations .
Right now I have located the most time consuming loop in the thread ( it does only compression ) and use GetTickCount ( ) and Sleep ( ) with hardcoded values .
It makes sure that the loop continues for a certain period of time and than sleeps for a certain minimal time .
It more or less does the job i.e .
guarantees that the thread will not use more than 50 % of CPU .
However behavior is dependent on the number of CPU cores ( huge disadvantage ) and simply ugly ( smaller disadvantage : ) ) .
Any ideas ?In many places , ( 1,2,3 ) and [ 1,2,3 ] can be used interchangably .
When should I use one or the other , and why ?Languages often have functions to escape the string , such as php with functions like mysql_escape_string as such : $ query = sprintf ( `` update people set name = ' % s ' where id = ' % s ' '' , mysql_escape_string ( $ name ) , mysql_escape_string ( $ id ) ) ; $ result = mysql_query ( $ query ) ; Other languages and frameworks provide a mechanism similar to the one you mentioned .
In Java , you can use a PreparedStatement as such : PreparedStatement statement = connection.prepareStatement ( `` update people set name = ?
where id = ?
`` ) ; statement.setString ( 0 , name ) ; statement.setString ( 1 , id ) ; statement.execute ( ) ;I ca n't think of any cross platform way of what you want ( or any guaranteed way full stop ) but as you are using GetTickCount perhaps you are n't interested in cross platform : ) I 'd use interprocess communications and set the intensive processes nice levels to get what you require but I 'm not sure that 's appropriate for your situation .
EDIT : I agree with Bernard which is why I think a process rather than a thread might be more appropriate but it just might not suit your purposes .The list [ 1,2,3 ] is dynamic and flexible but that flexibility comes at a speed cost .
The tuple ( 1,2,3 ) is fixed ( immutable ) and therefore faster .I have read a lot that LISP can redefine syntax on the fly , presumably with macros .
I am curious how far does this actually go ?
Can you redefine the language structure so much that it borderline becomes a compiler for another language ?
For example , could you change the functional nature of LISP into a more object oriented syntax and semantics , maybe say having syntax closer to something like Ruby ?
Especially , is it possible to get rid of the parenthesis hell using macros ?
I have learned enough ( Emacs- ) LISP to customize Emacs with my own micro-features , but I am very curious how far macros can go in customizing the language .If you want lisp to look like Ruby use Ruby .
It 's possible to use Ruby ( and Python ) in a very lisp like way which is one of the main reasons they have gained acceptance so quickly .@ sparkes Sometimes LISP is the clear language choice , namely Emacs extensions .
I 'm sure I could use Ruby to extend Emacs if I wanted to , but Emacs was designed to be extended with LISP , so it seems to make sense to use it in that situation .When you data bind in C # , the thread that changes the data causes the control to change too .
But if this thread is not the one on which the control was created , you 'll get the above exception .
I surfed the net and found no good answer .
Anyone ?I have values stored as strings in a DataTable where each value could really represent an int , double , or string ( they were all converted to strings during an import process from an external data source ) .
I need to test and see what type each value really is .
What is more efficient for the application ( or is there no practical difference ) ?
Try to convert to int ( and then double ) .
If conversion works , the return true .
If an exception is thrown , return false .
Regular expressions designed to match the pattern of an int or double Some other method ?If the data modification is not too time consuming ( meaning , if the main purpose of the background thread is not the actual data modification ) , try moving the section that modifies the data to a delegate and Invoke'ing that delegate .
If the actual heavy work is on the data , you 'll probably have to create a deep copy of this data to pass to the background thread , which will send the processed data back to the UI thread via Invoke again .You should be able to do something like : if ( control.InvokeRequired ) { control.Invoke ( delegateWithMyCode ) ; } else { delegateWithMyCode ( ) ; } InvokeRequired is a property on Controls to see if you are on the correct thread , then Invoke will invoke the delegate on the correct thread .
UPDATE : Actually , at my last job we did something like this : private void SomeEventHandler ( Object someParam ) { if ( this.InvokeRequired ) { this.Invoke ( new SomeEventHandlerDelegate ( SomeEventHandler ) , someParam ) ; } // Regular handling code } which removes the need for the else block and kind of tightens up the code .